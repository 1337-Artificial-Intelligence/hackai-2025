{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Fine-tuning script for Stable Diffusion for text2image using LoRA.</h1>\n",
        "\n",
        "This notebook demonstrates how to fine-tune Stable Diffusion using LoRA (Low-Rank Adaptation) to generate images in a specific style or domain. The training process works by taking your custom dataset of images with captions, adding random noise to the images, and teaching the model to predict and remove that noise. By learning to \"denoise\" images from your dataset, the model learns the visual patterns and style of your data.\n",
        "\n",
        "\n",
        "The LoRA technique allows this learning to happen efficiently by adding small trainable adapter layers to the existing Stable Diffusion model, rather than retraining the entire model from scratch. After training, you'll have a lightweight LoRA adapter that can be loaded into any Stable Diffusion pipeline to generate new images that match the style of your training data.\n",
        "\n",
        "\n",
        "Let's get started with setting up our environment and fine-tuning process!\n"
      ],
      "metadata": {
        "id": "H6-Dj206lEo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this challenge, you will fine-tune Stable Difusion to generate Image with Studio Ghibli style using LoRA.\n",
        "We will use the dataset from AtlasIA, which contains images of moroccan culture with Studio Ghibli Style.\n",
        "\n",
        "First, let's get the training file:"
      ],
      "metadata": {
        "id": "ub7NrnZ5sPjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ybendou/hackai-2025/main/py/train_text_to_image_lora.py"
      ],
      "metadata": {
        "id": "G4RNkbXY3AgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Some installs\n",
        "\n",
        "!pip install -U datasets"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UoDMuDU1tTck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before fine-tuning, let's connect to HuggingFace and Wandb"
      ],
      "metadata": {
        "id": "TfNfcrx8tD7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "cJvxTs4_qISe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "H-esEeQoqPgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Running the fine-tuning </h1>\n",
        "\n",
        "You can check the fine-tuning as well as the generation images on your wandb profile."
      ],
      "metadata": {
        "id": "aBWIr9SfqCmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n",
        "DATASET_NAME=\"atlasia/Ghibli-style-morocco-dataset\"\n",
        "OUTPUT_DIR=\"sd-ghibli-model-lora\"\n",
        "!mkdir -p OUTPUT_DIR\n",
        "!mkdir -p \"logs\"\n",
        "\n",
        "!python train_text_to_image_lora.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n",
        "  --resolution=256 --random_flip \\\n",
        "  --train_batch_size=1 \\\n",
        "  --num_train_epochs=100 --checkpointing_steps=5000 \\\n",
        "  --learning_rate=1e-04 --lr_scheduler=\"constant\" --rank 128\\\n",
        "  --seed=42 \\\n",
        "  --output_dir=\"sd-ghibli-model-lora\" \\\n",
        "  --validation_prompt=\"Craftsman, Moroccan Ghibli studio style\" --report_to=\"wandb\"\n",
        "\n"
      ],
      "metadata": {
        "id": "jR9ln7uYrF_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Exercise</h1>\n",
        "\n",
        "Run the fine-tuning and check the generated images at different epochs.\n",
        "\n",
        "\n",
        "Go to train_text_to_image_lora.py, read the code and answer:\n",
        "\n",
        "- What is the model trying to predict as its target?\n",
        "- Why do you think training a model to 'denoise' images would help it generate new images?\n",
        "\n"
      ],
      "metadata": {
        "id": "zKCcQdPAKgvf"
      }
    }
  ]
}