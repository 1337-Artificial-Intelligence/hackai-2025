{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd-vX3cavOCt"
      },
      "source": [
        "# **Stable Diffusion** ðŸŽ¨\n",
        "*...using `ðŸ§¨diffusers`*\n",
        "\n",
        "Stable Diffusion is a text-to-image AI model that generates images from written descriptions. Unlike traditional diffusion models that work directly with pixels (which is computationally expensive), Stable Diffusion uses a clever approach called \"latent diffusion\" that works in a compressed representation space, making it much faster and more efficient.\n",
        "\n",
        "The model has three key components: a VAE (autoencoder) that compresses images into a smaller latent space and reconstructs them back to full images, a U-Net that learns to remove noise step-by-step in this compressed space, and a text encoder (CLIP) that converts your text prompt into numerical embeddings the model can understand.\n",
        "\n",
        "\n",
        "During inference, Stable Diffusion starts with random noise in the latent space, then uses the U-Net to gradually \"denoise\" this randomness over ~50 steps, guided by your text prompt. Each step refines the image until you get a final result that matches your description. The VAE decoder then converts this latent representation back into a viewable image. This process allows you to generate high-quality 512Ã—512 images quickly, even on consumer GPUs.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz5Ge_47jUaA"
      },
      "source": [
        "**Stable Diffusion during inference**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUBqX1sMsDR6"
      },
      "source": [
        "\n",
        "This diagram shows Stable Diffusion's inference pipeline. It starts with two inputs: random Gaussian noise (latent seed) and a user prompt. The text prompt gets converted into numerical embeddings by the frozen CLIP text encoder.\n",
        "\n",
        "The core process happens in the Text-conditioned latent UNet, which takes the noisy 64Ã—64 latents and the text embeddings as inputs. The UNet predicts what noise to remove, guided by the text description. A scheduler algorithm uses this prediction to \"reconstruct\" (denoise) the latents step by step.\n",
        "\n",
        "This denoising process repeats N times (typically ~50 steps), with each iteration producing cleaner latents that better match the text prompt. Finally, the Variational Autoencoder Decoder converts the denoised 64Ã—64 latents back into a full 512Ã—512 output image.\n",
        "\n",
        "The key insight is that by working in the compressed latent space (64Ã—64) instead of pixel space (512Ã—512), the process is 64 times more memory efficient while still producing high-quality results.\n",
        "<p align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\" alt=\"sd-pipeline\" width=\"500\"/>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Denoising Process\n",
        "\n",
        "Given random Gaussian noise, the Unet model predicts the added noise and progressively denoises the image until it reaches a good quality image."
      ],
      "metadata": {
        "id": "bW2S898XbL6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Alt text](https://i.redd.it/a84scuqybtfc1.gif)"
      ],
      "metadata": {
        "id": "0ysC-o_uRJW8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xMJ6LaET6dT"
      },
      "source": [
        "## 2. How to use `StableDiffusionPipeline`\n",
        "\n",
        "In this section, we show how you can run text to image inference in just a few lines of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOlvQ1nQL7c"
      },
      "source": [
        "### Setup\n",
        "\n",
        "First, please make sure you are using a GPU runtime to run this notebook, so inference is much faster. If the following command fails, use the `Runtime` menu above and select `Change runtime type`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHkHsdtnry57"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMULjStDeeru"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    # Shows the nVidia GPUs, if this system has any\n",
        "    !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paJt_cx5QgVz"
      },
      "source": [
        "Next, you should install `diffusers` as well `scipy`, `ftfy` and `transformers`. `accelerate` is used to achieve much faster loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.11.1\n",
        "!pip install transformers scipy ftfy accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TVzGebMeerv"
      },
      "outputs": [],
      "source": [
        "# This is added to get around some issues of Torch not loading models correctly (test on Mac OS X and Kubuntu Linux)\n",
        "!pip install --upgrade huggingface-hub==0.26.2 transformers==4.46.1 tokenizers==0.20.1 diffusers==0.31.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NnPOMAqAABv"
      },
      "source": [
        "### Stable Diffusion Pipeline\n",
        "\n",
        "\n",
        "First, we load the pre-trained weights of all components of the model. In this notebook we use Stable Diffusion version 1.4 ([CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSKWBKFPArKS"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MgNzTxwbASv"
      },
      "source": [
        "Next, let's move the pipeline to GPU to have faster inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA9myHTxbDhm"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device=torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device=torch.device(\"mps\")\n",
        "\n",
        "pipe = pipe.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSTsT6McuzWW"
      },
      "source": [
        "And we are ready to generate images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEErJFjlrSWS"
      },
      "outputs": [],
      "source": [
        "prompt = \"Craftsman, Moroccan Ghibli studio style\"\n",
        "image = pipe(prompt).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "\n",
        "# Now to display an image you can either save it such as:\n",
        "image.save(f\"image1.png\")\n",
        "\n",
        "# or if you're in a google colab you can directly display it with\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfZCwCecVJI3"
      },
      "source": [
        "Running the above cell multiple times will give you a different image every time. If you want deterministic output you can pass a random seed to the pipeline. Every time you use the same seed you'll have the same image result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaAW4sSdV7vZ"
      },
      "outputs": [],
      "source": [
        "generator = torch.Generator(device).manual_seed(1024)\n",
        "\n",
        "image = pipe(prompt, generator=generator).images[0]\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RrbYQVQXK6I"
      },
      "source": [
        "You can change the number of inference steps using the `num_inference_steps` argument. In general, results are better the more steps you use. Stable Diffusion, being one of the latest models, works great with a relatively small number of steps, so we recommend to use the default of `50`. If you want faster results you can use a smaller number.\n",
        "\n",
        "The following cell uses the same seed as before, but with fewer steps. Note how some details, such as the horse's head or the helmet, are less defin realistic and less defined than in the previous image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKiK67iTXQkt"
      },
      "outputs": [],
      "source": [
        "generator = torch.Generator(device).manual_seed(1024)\n",
        "\n",
        "image = pipe(prompt, num_inference_steps=15, generator=generator).images[0]\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUbR3IszB1CD"
      },
      "source": [
        "To generate multiple images for the same prompt, we simply use a list with the same prompt repeated several times. We'll send the list to the pipeline instead of the string we used before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZcgsflpBoEM"
      },
      "source": [
        "\n",
        "\n",
        "Let's first write a helper function to display a grid of images. Just run the following cell to create the `image_grid` function, or disclose the code if you are interested in how it's done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REF_yuHprSa1"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcHccTDWbQRU"
      },
      "source": [
        "Now, we can generate a grid image once having run the pipeline with a list of 3 prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YAFLvWWrSdM"
      },
      "outputs": [],
      "source": [
        "num_images = 3\n",
        "prompt = [\"Craftsman, Moroccan Ghibli studio style\"] * num_images\n",
        "\n",
        "images = pipe(prompt).images\n",
        "\n",
        "grid = image_grid(images, rows=1, cols=3)\n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj-3lCCWYtMn"
      },
      "source": [
        "And here's how to generate a grid of `n Ã— m` images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylscg48YYxfF"
      },
      "outputs": [],
      "source": [
        "num_cols = 3\n",
        "num_rows = 4\n",
        "\n",
        "prompt = [\"Craftsman, Moroccan Ghibli studio style\"] * num_cols\n",
        "\n",
        "all_images = []\n",
        "for i in range(num_rows):\n",
        "  images = pipe(prompt).images\n",
        "  all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may observe, the results are not perfect, which indicates that some fine-tuning (retraining) could help improve them.\n",
        "\n",
        "In the next challenge, you will see how to fine-tune stable diffusion using LoRA for fast and efficient fine-tuning."
      ],
      "metadata": {
        "id": "3kiH8bV8_NCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Exercice</h1>\n",
        "\n",
        "1. In the notebook, you used the parameter num_inference_steps=15 to generate faster results. Based on what you learned about the diffusion process, explain why reducing the number of steps makes generation faster but potentially affects image quality.\n",
        "\n",
        "2. The notebook mentions that Stable Diffusion works in 'latent space' rather than directly with pixels. In simple terms, explain why this approach makes the model faster and what component is responsible for converting between latent space and the final image you see."
      ],
      "metadata": {
        "id": "xGEesCBS_ATk"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}