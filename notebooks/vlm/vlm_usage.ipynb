{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmd1wvhAFek2"
   },
   "source": [
    "# üèÜ VLM: Usage\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/notebooks/vlm/vlm_usage.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbI5IYy84FCH"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGFa6jqPipRZ"
   },
   "source": [
    "### üìå What are Vision Language Models & How do they work?\n",
    "\n",
    "Vision language models are multimodal AI systems built by combining a large language model (LLM) with a vision encoder, giving the LLM the ability to **‚Äúsee.‚Äù**\n",
    "\n",
    "Most VLMs follow an architecture with three parts (as shown in the image below):\n",
    "\n",
    "- A vision encoder is a transformer architecture that has been trained on millions of image-text pairs, giving it the ability to associate images and text.\n",
    "- A projector is a set of layers that translates the output of the vision encoder into a form the LLM can understand, often interpreted as image tokens.\n",
    "- An LLM, which can be any LLM that has already been trained and released.\n",
    "\n",
    "<p align=\"center\" >\n",
    "  <img src=\"https://www.nvidia.com/en-us/glossary/vision-language-models/_jcr_content/root/responsivegrid/nv_container_copy_co_300503066/nv_image.coreimg.svg/1736168815674/vlm-architecture-diagram.svg\" width=\"500\" title=\"VLM Components\"/>\n",
    "</p>\n",
    "\n",
    "### üìå What Are the Capabilities of Vision-Language Models?\n",
    "VLM can perform various tasks such as chatting about images, image recognition via instructions, visual question answering, document understanding, and more ‚Äî some of which we'll explore in this challenge.\n",
    "The image below showcases the different tasks a VLM can perform.\n",
    "\n",
    "\n",
    "<p align=\"center\" >\n",
    "  <img src=\"https://cdn.prod.website-files.com/6479eab6eb2ed5e597810e9e/67ed59567624cf7b802b88a2_6687a8f8ecade4efd45a3cf6_Vision%2520Model_fig1.png\" width=\"700\" title=\"VLM Components\"/>\n",
    "</p>\n",
    "\n",
    "Alright, enough talk ‚Äî let‚Äôs mess around with these VLMs and see what they do! ü§ñüéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKGDL8o94baG"
   },
   "source": [
    "## Use Cases of VLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAzjmCsYCCDu"
   },
   "source": [
    "### Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IeGTYIR0KF1"
   },
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZ7PZtWV6P4H"
   },
   "outputs": [],
   "source": [
    "# After you finish testing each VLM, we will use the following function to free up GPU memory and remove variables from the global scope\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeYCwfNtkHp6"
   },
   "source": [
    "### 1. Visual Question Answering\n",
    "- Goal: Answer questions about an image or video.\n",
    "- Input: Image or video + a natural language question\n",
    "- Output: Short natural language answer\n",
    "\n",
    "We'll use `Qwen2-VL-2B-Instruct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470,
     "referenced_widgets": [
      "ffb8d417e87e43e6a8d2dc667d0103e7",
      "3553a187de454ebdbbc7ed5a0c338510",
      "c3fc4685d68b4f8b8ef1fae3bf65727b",
      "b66c6278bd6948108291de7a0a9da036",
      "f2972656b170469dba622a60eba7440c",
      "6a58974e5c634d2b8f32cfe33475806a",
      "e8e147ec44f14ddbbcc52f8cf5c260ca",
      "e7928b7777644ebf8b2ca9ab9ee0f99f",
      "7a4e73a3d8d54dd283dbeab13cda19a6",
      "f4c2bf860072466d9a06a0c8aa77a359",
      "f9debbe1aa6342cbb0d8dea45cc7d4fa",
      "0fb1d7a5bab64408bfbab2725aae25a8",
      "0dd7ae13ba6e473eaccbe21e664b234d",
      "7d85a4b0b51d44b7b97d91ddc58322f9",
      "3a2f14237448413f9d99fe1013815c19",
      "fbc401ec61ea4994a379b3983fdffb39",
      "bd2722fb1b2848ec873379aedfd32bbd",
      "9cddb880b2a848819cbe916f4dfd400b",
      "1e1021ea569241b6bfa8aa6aec337588",
      "511fc0848eec419a95a63afefff02715",
      "bd176467d554444fb06f13bb385a0f53",
      "f8b900e0119d4dbebd27450403ec3d9a",
      "e97c77dd25764a60a1019663fa6dfb08",
      "4cfa9db4c8b44ef598dffb7b984c27df",
      "e3d9998705834c6d8da6d33588bc209e",
      "8b7f946ce3874518bd9727d9d6c90ead",
      "283314140d834776bdaeb5f8a5ee1102",
      "3c56322e4ad348369776fc9e6f44cb74",
      "3dcaf7dd237b4479b381c8a039132852",
      "04a284ffa8c549babfe7688c78149b8b",
      "e9ac597fd4834d428ebbd572c8e0aade",
      "fb33bb367a284d7ba65a84c8ded69c33",
      "b256856429a64ee5bb4eeb56ac5e45ad",
      "f186f43820f2437b830d14cc50369926",
      "0223753573014e4da1e0f25d8dd4ed8b",
      "f519d04d82a44bba991bfde18ce2eeda",
      "eddf13324870414396fcd06b68cb9542",
      "8ea47976c73146eba7548245d28d1c0b",
      "ce2a26fbf5924e8a9d7ae36b2881f48d",
      "e1d72f81505646a09d48319503ae87bf",
      "e78cda79c1f446cb92d647eece0eb3fb",
      "71f33f6f90de4adbbef4f813e7ce0718",
      "0fb0a6bee7604bcfac1c9caeb88a65a6",
      "017f3db47bdf4571a4083be7f4cdb5dc",
      "dad1e3476861416b9a8dda529c75ee72",
      "de3b3e2d59094659969510cb4cfee15d",
      "ed6f63e2922841c4affa5284fad94fa3",
      "e412cf6d5a8c494386b92971cee9f7f6",
      "38cecf9d5be94fd2adee864e44481fe6",
      "0589d890bf074341a6c12675a3faae8f",
      "a5ebb0ac4db8425c841c41606bf2e829",
      "344684bdc1b94a6f90b19b0c0711e0a9",
      "cafdfd822cb544c1acb9c38dd1daf2e9",
      "5f7ac93f09a24e519084a15f1b9c757d",
      "382190f2e3f248fab0a6f5a77ab6aefd",
      "b817d99a4b824c99b9507c64c5e115b5",
      "db2d1c59de7046e19850bbf5f374b23d",
      "b68d465e4c60422c86d035d35fdad70b",
      "d3f5d85ce8074f4ea40dec10055a5352",
      "fe97008ca50c4beeb839364403c3edd2",
      "4a3b9a9e87804e9caba4e6e1da3d5acc",
      "64e05b9ef7394e6e8cedee78daf95414",
      "d148f3a3064d40c5a119a4fa1f54a4b6",
      "14e5501d05484057af35982d6a0301c1",
      "64d8e7e11eef4cb5bc6c6799d6d8de24",
      "cbd0ae3a8e3f43d888d71f26d955c9fb",
      "e2869e53bb694fe4b26bc50a640f9ebe",
      "020e2d65347b43dfa59fee30668d027e",
      "72506adb3da34ae8bed7fcf61c056f3a",
      "d3a7531a3b504deaa57755634052c52a",
      "bb997ad5249a4642b643c93c4f3fb114",
      "a5450a18bfd848ff8a1c5dbd95345690",
      "f05aac77686748ed990acc9e9a6e24f6",
      "7c2a0fd7facb4a91a47635b02eb37ef6",
      "4ef2c9f23b2c4a9486fd20ef53c6edc0",
      "87bedb19268e434493014063a267d82d",
      "78cb5bef8e45413ea0316741c44f6a44",
      "ea9d81349ee544bd86f85393948c8fdc",
      "4228aae68cdf4d44bade2ce9abb769b4",
      "269a36142275453f9436c43fe6ee33a7",
      "e5012ed20ce249f5a86921a231f67cbd",
      "08e8c3c0e0b84b52840fb622e1b8d96b",
      "f9d83058ce8045c1850df49678723fa7",
      "47ce4d16bed24fe5b0241f00226205d1",
      "645b7b1f4f3c4003b3e486816107163a",
      "e91ee713b90e414dac9469d77c619d8f",
      "f8b58493fd6d41279ecbc0a128aa90e1",
      "a193e7453efd489b9fb3585ae73f3237",
      "07142f8c07964c38ba7487b2b1fc78d7",
      "e3d467c6d0c247a5927a890f9959ad0a",
      "3931b52a1f5e47e6b5d870ca5c87b6e7",
      "3279a8bf3dd4482e830716ed7b43a83b",
      "c1f5d38ab09e4067999ec61577fdad89",
      "7f697da03fff4f0fbbd6b8c8cf2ebe69",
      "38f1a2164ea941d1ab50a60ca32e5494",
      "45287a8e3a004099bc46cb6a16e77a5a",
      "b5b21d1818bd4dfaaa717ca7e3fbd371",
      "fcfbd0b9ab9f419499487e33953d92db",
      "826e1d4dc108449cb6468851fffd8240",
      "6bd5af71af594707b68e6281c87604bb",
      "8bbb7662295440cf96f9efae07f7f98b",
      "fe8a886b8acd42c796b166474ae28311",
      "67529a0393504b0faa9becfb905f2dca",
      "75f9e5901a22403e9eb48c8723d2a2ff",
      "4bd0695b66ca4d838a168a4ce58f55f2",
      "c128e717b0224167a12ffb4df04fbd25",
      "c0b2361ef0e842ac964298a6c8121d8f",
      "5ea33f184e8c44019497ab457d1ddaec",
      "49e6cdca427b49a9be84b3608e55efd2",
      "608630add28b471f8d73ffdcf91476db",
      "39e31d73f2d64ff6a9d02afc711cfd2b",
      "d20ed9b17fba4a26b96313f5f8069603",
      "2e85b870b3c3400caa4a3bf9b1dccc85",
      "bf36032b301a458b818b96eed808934f",
      "634c702e7ad545279a1f11ad68bae126",
      "f56d6dc1d01b49ffa1d1656e49fb8bc5",
      "f9aef45fd14b47ee83d06fa76de788a6",
      "60172503f19e42e19b5ed07099192f15",
      "793f028249574ddd86e93e1c131b2c39",
      "5b34945e754648ec997315c38015d2dd",
      "1e028396edf84c9a87cfed6610bb4962",
      "c0df82ed929f4fb0b7bb14c1034e56b3",
      "e53b8ea2241e416eb1b68259390655f4",
      "682cf3de60d54d2c937b6cb065a9090e",
      "1c0dd1daacae43e8b4ba69b63ac0f7fe",
      "4fa28c8e580d44a9bc23274bff95749d",
      "9a9405a5821341aa9e9b4c4a2d4314d1",
      "3dff9cba7c934f7c90e4219ec0a6a8d5",
      "8a652564e0704e58a6cb90eefb6f3f90",
      "9b05f6485d4f4faeb55c87b8a9d9f304",
      "0e4b986404714835af1db563d8947b2f",
      "d7bb2f7923d34af287f9049f7e73f391",
      "031f5201327c41f8a82349fa3da3db89",
      "868118b780204b24a25f79951b62c894",
      "0aa925a5911e411c9d7c2edc1d2a1552",
      "6244891b940841dfb16ca4692945f01d",
      "7786dfe885bb4c6495bec6892ed7a651",
      "516f93ebcdef4d42a1bb0e979b2c98e0",
      "891f07f825cc4b28a67f9960c96aaabd",
      "3447be7c70c74b5b9e80f1654e147da5",
      "21d24eaa95584580b0f585486fde9a68",
      "a1178e63a25e415a84c4cc13ceac9b8b",
      "f047c927375d4ec692bb455efb34228f"
     ]
    },
    "id": "sgS1Ps3TmkkQ",
    "outputId": "75a23a2d-8f0a-4b2d-ec3a-3d1491b0c1c4"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import io\n",
    "from typing import Dict\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "\n",
    "# First, let's load the model and its processor on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KkKGxmc798C"
   },
   "source": [
    "#### Let's test with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "m2pBfKEI1g3W",
    "outputId": "41fb35f9-9283-4107-d3a4-81a8bb8d9567"
   },
   "outputs": [],
   "source": [
    "# Image (Try other images)\n",
    "url = \"https://legarconboucher.com/img/cms/Recette/tajine-maroc.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Prompt (Try with other questions; Darija maybe?)\n",
    "text_query = \"What do you see in the image?\"\n",
    "\n",
    "# Define model chat template\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": text_query},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "import textwrap\n",
    "print(textwrap.fill(output_text[0], width=100))\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrEye1RX8HVN"
   },
   "source": [
    "#### Let's test with videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMob6WVk8d6I",
    "outputId": "8f093644-b864-4b9e-f14e-eed3fb0d4a29"
   },
   "outputs": [],
   "source": [
    "!pip install pyav yt-dlp qwen-vl-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_IvggdFA24I"
   },
   "outputs": [],
   "source": [
    "def download_video(video_url: str):\n",
    "  import yt_dlp\n",
    "\n",
    "  download_folder = \"/content/\"\n",
    "\n",
    "  ydl_opts = {\n",
    "      'outtmpl': f'{download_folder}/video.mp4',\n",
    "      'format': 'best',\n",
    "  }\n",
    "\n",
    "  with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "      ydl.download([video_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lSzCPmuBVLP",
    "outputId": "4b6a16df-923d-4a69-d476-aaf5c0456677"
   },
   "outputs": [],
   "source": [
    "download_video(\"https://www.youtube.com/shorts/po8D2FUCtu0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "K9SaNwWH8Gmd",
    "outputId": "ced3f8e8-87fc-42cc-f5af-37f88f9cde37"
   },
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "# Video\n",
    "video = \"/content/video.mp4\"\n",
    "text_query = \"What do you see in the video?\"\n",
    "\n",
    "# Messages containing a video and a text query\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": video,\n",
    "                \"max_pixels\": 360 * 420,\n",
    "                \"fps\": 1.0,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": text_query},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "output_text[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiVD1lKF4KL0",
    "outputId": "b91260bb-ef77-4356-d229-9b70c990a2e4"
   },
   "outputs": [],
   "source": [
    "# Ensure that you have finished testing the VLM before calling this function.\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAdkEfW6molR"
   },
   "source": [
    "### 2. Image Captioning\n",
    "\n",
    "- Goal: Generate a sentence that describes the entire image.\n",
    "- Input: An image\n",
    "- Output: A natural language description\n",
    "\n",
    "Unlike VQA, which answers specific questions, image captioning gives a general overview of the image.\n",
    "We'll use `blip-image-captioning-base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310,
     "referenced_widgets": [
      "8010d35ad5394c1782268fa00ab8dcb6",
      "f4ff10546ef54aa6ac657439914d0bb4",
      "2a5ef293359e4db5bff46dc87bd57b25",
      "7a43abbeb6fa4a2ea3ec7be37387682b",
      "e1acd63b1ff34e5e81b7e2d07c881952",
      "5416ccd44acd45cfa2add9dc5267a0e1",
      "4a6c9988ddb94e7c9b07da5f45fe775e",
      "c73da208ed954ad086c295e0cfe552db",
      "b0519257601c40d0823a28d2b012733a",
      "193d3aebd18d46f68182e596eb10e7b3",
      "e69fd3da5ae8473480b276a9f852d740",
      "49d718251b334dbc95f63f54b9f5c5e3",
      "1d6d199de32b48fd95b28bb2b91703c0",
      "a29c187b2e1b4bd280a02dd3fbb76661",
      "83953c69843b415691a68058b2bb9011",
      "2bb066c3745249e7a0e77141ead9a579",
      "fbce67d4db8a4b38ac33b2c29609f825",
      "68b29d91fadb40c695a303aa9378ecb9",
      "bfb018946e51461cb91fa6ad94b17b8c",
      "418d32eb9f424aaeacacafb42abc8825",
      "729bb7e7cd2341a7bd76914b614077d1",
      "d836c4f51c3347479410a65323673a61",
      "537da598f4b14dba900ec07b0618dba3",
      "bd4ea49926914399804bb940ff348d10",
      "e1ace0fb1bd44ad7af00290ef82df58a",
      "ba11c7d1365f4febb234b6adb9bd7bfe",
      "96df429f2cd248afb7d1a3a7bbb8aabc",
      "0279508e4263481eaea93ec242f4e71f",
      "a9a4a5a37bd845c98d087922034a58b4",
      "77ba8e47802f4673a3370aa25dca30c2",
      "b98dcf3338f04afa856256dead987e7c",
      "3a1ec407e8d543fcb97f3674149b624f",
      "48418b64eefe48f39fb688fa5059081c",
      "84a4f6abc1444bc1a8734359545a5ebc",
      "47f82374f6bd479ba49d2ff13ed0e296",
      "4b05c496b2374f06aece2ae9ff18a78f",
      "c9482b28379c466c938e2f598fd49601",
      "dd4950c43b0b4a03badd69d4f90dd806",
      "3e25c873841d419696543e86d408be2d",
      "670f9c84bf384dc38ee3e2202554cd15",
      "3ef3c35c86bd4ec6975e36dae99dbbf5",
      "6f391d861e5d429383e00ed67468cec2",
      "7009041390b24ff4999607043420e7c4",
      "812e7b883619471fb12d2323578e19f3",
      "9c68a077571b488f9123fce854991ac2",
      "1b85c4afa4974559a1fb180756ad2f60",
      "106cf860bc404313b230815335d1dfae",
      "456f4b0792ad4fcdb808e1d3e06c3fd6",
      "735933768c1a4903973ed16ba640dbc9",
      "43ae46b89a914dbd8aacb524b5001821",
      "6e2b497bed3c45b18720b50a215e35c1",
      "485a6b5e7ee444209907bcbc496ff1b7",
      "c789a78e7a8741469f8b080af6ee1efd",
      "b8f23a33588d4b1d8e1d30ec927c9182",
      "6b72d733b0cf4b8885d135b8573c80c2",
      "4efce111f2ee4b46be6d2c86f9438491",
      "b1a743e646cd4f508f23419bf8632415",
      "d6e3cc3dc52c40bb981e419602e45c50",
      "772ff7a6fad9434fa4d2d52f6c4ac0de",
      "5a150518cd4d40d6918a693a8ffdea52",
      "f4c792e8a14a4523a70fcb4cac0ab5c1",
      "f00b3240305548ba82204db690c3050e",
      "16fccf7f9bdd43d688b143ff19be317c",
      "640c259b43e640408b0ea15c5120661e",
      "2614639a92294f3e9ef3fda8b6e753a0",
      "9940b0ac7b3d45bd8645f30da0e90f71",
      "c9b87dfe1ae44ee3866d7e40672fd8d2",
      "648c70fc01cb46b8986af4ad33f5eb22",
      "a94044910f7444919ece89c58542ac07",
      "95b1d95a82b649b3980d8091d683c5a5",
      "b85a06cd12b4403e9fac715873b31835",
      "d67dd392354c40eeabfd07eef83a898d",
      "b09977a572ba47a180b17dbb580d7ba3",
      "af597d8db0d74b5d84e933985a637c07",
      "eba56054d69947d49748dd30935edf1a",
      "cc9aeb6b163447928773f46e85a608ab",
      "7ec4387e3da84785b5c803031a7476bd",
      "162303b1408e476990f8017d21fa1097",
      "ea3d20a527fa40829a48da6df0636a90",
      "33b66676058344d082f56a84656a004e",
      "d979a8cae73242389dcc13e4ab4555e2",
      "f5453bf0cfce44b1bd7472429fc7f6e8",
      "bb2b4c7a3f93407b9d7c94e8a3a64341",
      "75b058c1da3a4347b2d4ad4557bbafc7",
      "9615377a15c74850972b7ac6419e7409",
      "7fc96aefef874c1db790ff3261db6725",
      "1400ce456aff4c2594a2ba15dba2d8ad",
      "484df2a685604ceaa8a8ca6c3532d1f5"
     ]
    },
    "id": "dxdbe_UuFlBq",
    "outputId": "807a155d-7dc4-4f00-870f-0a615f312d26"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# First, let's load the model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "z5LUPYAHGatO",
    "outputId": "0ed9b119-e6ff-4741-8fe3-7983b8d51eae"
   },
   "outputs": [],
   "source": [
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# conditional image captioning (generates a caption based on an additional condition or query that guides the captioning process)\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(\"Conditional image captioning: \", processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# unconditional image captioning (generates a caption for an image without any specific context other than the image itself.)\n",
    "inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(\"unconditional image captioning: \", processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7D4tTZrWKatJ",
    "outputId": "13b52f5e-543f-43bf-94cc-736b152ffd1f"
   },
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpcMjvxRl4DV"
   },
   "source": [
    "### 3. OCR (Optical Character Recognition)\n",
    "- Goal: Extract text from images (printed, handwritten, or scene text).\n",
    "- Input: An image with optional prompts like ‚ÄúExtract the invoice number.‚Äù\n",
    "- Output: Transcribed, machine-readable text.\n",
    "\n",
    "We'll use `Qwen2-VL-OCR-2B-Instruct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DU6pq5-LKlEi"
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# First, let's load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "id": "-D4OhXpEL6FG",
    "outputId": "245395d1-4804-43c6-8f4d-abc570873b4a"
   },
   "outputs": [],
   "source": [
    "# Image\n",
    "url = \"https://trulysmall.com/wp-content/uploads/2023/04/Simple-Invoice-Template.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "text_query = \"What is the name of the invoice' sender?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": text_query},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "import textwrap\n",
    "print(textwrap.fill(output_text[0], width=50))\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60JyzdG_Mo-X",
    "outputId": "7863d1a2-5970-42d3-b8fa-88819475d045"
   },
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExP6vLAop8pQ"
   },
   "source": [
    "### 4. Visual Grounding\n",
    "- Goal: Locate objects or regions in an image based on a text query.\n",
    "- Input: An image and a natural language instruction (e.g., ‚ÄúFind the red car‚Äù).\n",
    "- Output:\n",
    "\n",
    "  - Bounding Box (e.g., [x_min, y_min, x_max, y_max]), or\n",
    "\n",
    "  - Segmentation Mask, or\n",
    "\n",
    "  - Object Label (e.g., ‚Äúcat‚Äù, ‚Äútree‚Äù).\n",
    "\n",
    "We'll use `microsoft/kosmos-2-patch14-224` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zff7lYJOkGE9"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# First, let's load the model and its processor\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\", device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmEqF7sMxFie"
   },
   "outputs": [],
   "source": [
    "# let's define a function to run a prompt.\n",
    "\n",
    "def run_example(prompt, image):\n",
    "\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    generated_ids = model.generate(\n",
    "      pixel_values=inputs[\"pixel_values\"],\n",
    "      input_ids=inputs[\"input_ids\"],\n",
    "      attention_mask=inputs[\"attention_mask\"],\n",
    "      image_embeds=None,\n",
    "      image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "      use_cache=True,\n",
    "      max_new_tokens=128,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    _processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "    processed_text, entities = processor.post_process_generation(generated_text)\n",
    "\n",
    "    return processed_text, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pl1vraaH0CRW"
   },
   "outputs": [],
   "source": [
    "# Let's define a function to draw the bounding boxes returned by the VLM.\n",
    "# REMARK >> (IT'S A QUITE LONG FUNCTION NO NEED TO UNDERSTAND EVERY DETAIL, Haha!)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def is_overlapping(rect1, rect2):\n",
    "    x1, y1, x2, y2 = rect1\n",
    "    x3, y3, x4, y4 = rect2\n",
    "    return not (x2 < x3 or x1 > x4 or y2 < y3 or y1 > y4)\n",
    "\n",
    "\n",
    "def draw_entity_boxes_on_image(image, entities, save_path=None):\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        image (_type_): image or image path\n",
    "        collect_entity_location (_type_): _description_\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_h = image.height\n",
    "        image_w = image.width\n",
    "        image = np.array(image)[:, :, [2, 1, 0]]\n",
    "    elif isinstance(image, str):\n",
    "        if os.path.exists(image):\n",
    "            pil_img = Image.open(image).convert(\"RGB\")\n",
    "            image = np.array(pil_img)[:, :, [2, 1, 0]]\n",
    "            image_h = pil_img.height\n",
    "            image_w = pil_img.width\n",
    "        else:\n",
    "            raise ValueError(f\"invaild image path, {image}\")\n",
    "    elif isinstance(image, torch.Tensor):\n",
    "        image_tensor = image.cpu()\n",
    "        reverse_norm_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])[:, None, None]\n",
    "        reverse_norm_std = torch.tensor([0.26862954, 0.26130258, 0.27577711])[:, None, None]\n",
    "        image_tensor = image_tensor * reverse_norm_std + reverse_norm_mean\n",
    "        pil_img = T.ToPILImage()(image_tensor)\n",
    "        image_h = pil_img.height\n",
    "        image_w = pil_img.width\n",
    "        image = np.array(pil_img)[:, :, [2, 1, 0]]\n",
    "    else:\n",
    "        raise ValueError(f\"invaild image format, {type(image)} for {image}\")\n",
    "\n",
    "    if len(entities) == 0:\n",
    "        return image\n",
    "\n",
    "    new_image = image.copy()\n",
    "    previous_bboxes = []\n",
    "    # size of text\n",
    "    text_size = 1\n",
    "    # thickness of text\n",
    "    text_line = 1  # int(max(1 * min(image_h, image_w) / 512, 1))\n",
    "    box_line = 3\n",
    "    (c_width, text_height), _ = cv2.getTextSize(\"F\", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n",
    "    base_height = int(text_height * 0.675)\n",
    "    text_offset_original = text_height - base_height\n",
    "    text_spaces = 3\n",
    "\n",
    "    for entity_name, (start, end), bboxes in entities:\n",
    "        for (x1_norm, y1_norm, x2_norm, y2_norm) in bboxes:\n",
    "            orig_x1, orig_y1, orig_x2, orig_y2 = int(x1_norm * image_w), int(y1_norm * image_h), int(x2_norm * image_w), int(y2_norm * image_h)\n",
    "            # draw bbox\n",
    "            # random color\n",
    "            color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "            new_image = cv2.rectangle(new_image, (orig_x1, orig_y1), (orig_x2, orig_y2), color, box_line)\n",
    "\n",
    "            l_o, r_o = box_line // 2 + box_line % 2, box_line // 2 + box_line % 2 + 1\n",
    "\n",
    "            x1 = orig_x1 - l_o\n",
    "            y1 = orig_y1 - l_o\n",
    "\n",
    "            if y1 < text_height + text_offset_original + 2 * text_spaces:\n",
    "                y1 = orig_y1 + r_o + text_height + text_offset_original + 2 * text_spaces\n",
    "                x1 = orig_x1 + r_o\n",
    "\n",
    "            # add text background\n",
    "            (text_width, text_height), _ = cv2.getTextSize(f\"  {entity_name}\", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n",
    "            text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2 = x1, y1 - (text_height + text_offset_original + 2 * text_spaces), x1 + text_width, y1\n",
    "\n",
    "            for prev_bbox in previous_bboxes:\n",
    "                while is_overlapping((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), prev_bbox):\n",
    "                    text_bg_y1 += (text_height + text_offset_original + 2 * text_spaces)\n",
    "                    text_bg_y2 += (text_height + text_offset_original + 2 * text_spaces)\n",
    "                    y1 += (text_height + text_offset_original + 2 * text_spaces)\n",
    "\n",
    "                    if text_bg_y2 >= image_h:\n",
    "                        text_bg_y1 = max(0, image_h - (text_height + text_offset_original + 2 * text_spaces))\n",
    "                        text_bg_y2 = image_h\n",
    "                        y1 = image_h\n",
    "                        break\n",
    "\n",
    "            alpha = 0.5\n",
    "            for i in range(text_bg_y1, text_bg_y2):\n",
    "                for j in range(text_bg_x1, text_bg_x2):\n",
    "                    if i < image_h and j < image_w:\n",
    "                        if j < text_bg_x1 + 1.35 * c_width:\n",
    "                            # original color\n",
    "                            bg_color = color\n",
    "                        else:\n",
    "                            # white\n",
    "                            bg_color = [255, 255, 255]\n",
    "                        new_image[i, j] = (alpha * new_image[i, j] + (1 - alpha) * np.array(bg_color)).astype(np.uint8)\n",
    "\n",
    "            cv2.putText(\n",
    "                new_image, f\"  {entity_name}\", (x1, y1 - text_offset_original - 1 * text_spaces), cv2.FONT_HERSHEY_COMPLEX, text_size, (0, 0, 0), text_line, cv2.LINE_AA\n",
    "            )\n",
    "            # previous_locations.append((x1, y1))\n",
    "            previous_bboxes.append((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2))\n",
    "\n",
    "    pil_image = Image.fromarray(new_image[:, :, [2, 1, 0]])\n",
    "    if save_path:\n",
    "        pil_image.save(save_path)\n",
    "\n",
    "    return pil_image # new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQac6JdOvvaa"
   },
   "outputs": [],
   "source": [
    "# Test with other images\n",
    "\n",
    "# url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\n",
    "url = \"https://cdn.nba.com/manage/2021/12/USATSI_15452777-scaled-e1639236310885-784x462.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdT5S4kDxhxb"
   },
   "outputs": [],
   "source": [
    "# Define the prompt and call the model\n",
    "\n",
    "prompt = \"<grounding> Describe this image in detail:\"\n",
    "model_output, entities = run_example(prompt, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "mBfAcRwOxsMs",
    "outputId": "6a1285bf-7f81-4fe7-91d8-04ca4108b732"
   },
   "outputs": [],
   "source": [
    "# Draw the bounding boxes on the image and print the model output\n",
    "new_image = draw_entity_boxes_on_image(image, entities)\n",
    "print(model_output, entities, sep=\"\\n\")\n",
    "new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lNdXUM3vMY4"
   },
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywpn36ji5aGq"
   },
   "source": [
    "## Example of VLM Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_-EfhKo6GGv"
   },
   "source": [
    "Here are examples of datasets for each task above:\n",
    "\n",
    "- Visual Question Answering (VQA): [DOC-VQA](https://huggingface.co/datasets/cmarkea/doc-vqa) ‚Äì is a multilingual (English & French) visual question answering (VQA) dataset focused on document images.\n",
    "\n",
    "- Image Captioning: [MS COCO Captions](https://huggingface.co/datasets/clip-benchmark/wds_mscoco_captions) ‚Äì A dataset with images and captions for each image.\n",
    "\n",
    "- Optical Character Recognition (OCR): [OCR-VQA](https://huggingface.co/datasets/howard-hou/OCR-VQA/viewer/default/train?row=0) ‚Äì designed for OCR-based VQA. It focuses on understanding and answering questions about text present in images, particularly book covers.\n",
    "\n",
    "- Visual Grounding: [Flickr30k_Grounding_Som](https://huggingface.co/datasets/Rajarshi-Roy-research/Flickr30k_Grounding_Som/viewer/default/train?row=4&views%5B%5D=train) ‚Äì Links phrases in image captions to corresponding regions in the image with bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyoudFBy3t_B"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "- **What other vision-language tasks can you think of?**\n",
    "(Think about how VLMs could be applied in new or unusual ways.)\n",
    "\n",
    "**Good luck with the rest of the challenge‚Äîand HAVE FUN! üöÄ**\n",
    "____"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FKGDL8o94baG",
    "NAzjmCsYCCDu",
    "xeYCwfNtkHp6",
    "3KkKGxmc798C",
    "hrEye1RX8HVN",
    "PAdkEfW6molR",
    "OpcMjvxRl4DV",
    "ExP6vLAop8pQ",
    "ywpn36ji5aGq"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
