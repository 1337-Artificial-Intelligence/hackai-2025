{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-nuvfeMUAmD"
   },
   "source": [
    "# üèÜ RAG Pipeline Construction & Evaluation Mini-Challenge\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook guides you through building and evaluating Retrieval-Augmented Generation (RAG) pipelines. You will:\n",
    "\n",
    "1. **Process a PDF document** using `docling` to extract text and handle images\n",
    "2. **Experiment with various RAG components**: chunking strategies, embedding models, retrieval techniques\n",
    "3. **Evaluate pipeline performance** using the `ragas` framework\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "First, let's install all required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56VWD8f6UAmQ"
   },
   "outputs": [],
   "source": [
    "# Install required packages (uncomment and run if needed)\n",
    "# !pip install docling-core ollama langchain openai ragas datasets pandas pymupdf chromadb rank_bm25 tiktoken python-dotenv scikit-learn pillow rapidocr_onnxruntime\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Create necessary directories\n",
    "Path(\"output\").mkdir(exist_ok=True)\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete! Make sure Ollama is running if you plan to use local models.\")\n",
    "print(\"üí° Don't forget to create your .env file with API keys if using OpenAI/Cohere.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsuD2eeIUAmZ"
   },
   "source": [
    "## Part 1: Document Processing\n",
    "\n",
    "The first step is to process our PDF document using Docling to extract text and handle images.\n",
    "This creates a clean markdown file that we can use for our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2VgsJbqUAmb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processes a PDF using Docling for text extraction and image handling,\n",
    "preparing it for RAG pipeline ingestion.\n",
    "\"\"\"\n",
    "\n",
    "import base64\n",
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import ollama  # Make sure ollama server is running\n",
    "\n",
    "# --- Docling Imports ---\n",
    "try:\n",
    "    from docling.datamodel.base_models import InputFormat\n",
    "    from docling.datamodel.pipeline_options import (\n",
    "        PdfPipelineOptions,\n",
    "        RapidOcrOptions,\n",
    "    )\n",
    "    from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "    print(\"Docling imports successful.\")\n",
    "except ImportError:\n",
    "    print(\"Error: Failed to import Docling components.\")\n",
    "    print(\"Please ensure 'docling-core' and its dependencies are installed.\")\n",
    "\n",
    "# Configuration\n",
    "PDF_PATH = Path(\"API_FR.pdf\")  # Input PDF for the challenge\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_PROCESSED_TEXT_PATH = OUTPUT_DIR / \"processed_text.md\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Placeholders\n",
    "IMAGE_PLACEHOLDER = \"<!__ image __>\"\n",
    "PAGE_BREAK_PLACEHOLDER = \"\\n\\n--- Page Break ---\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOdRxzcuUAmb"
   },
   "outputs": [],
   "source": [
    "def replace_occurrences(text: str, target: str, replacements: list[str]) -> str:\n",
    "    \"\"\"Replaces sequential occurrences of a target string with replacements.\"\"\"\n",
    "    current_replacement_index = 0\n",
    "    while target in text and current_replacement_index < len(replacements):\n",
    "        text = text.replace(target, replacements[current_replacement_index], 1)\n",
    "        current_replacement_index += 1\n",
    "    return text\n",
    "\n",
    "def process_pdf(pdf_path: Path, output_dir: Path, output_filename: str):\n",
    "    \"\"\"Converts PDF to Markdown text using Docling.\"\"\"\n",
    "    print(\"--- Starting Document Processing ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configure Docling\n",
    "    pipeline_options = PdfPipelineOptions(\n",
    "        generate_page_images=True,\n",
    "        images_scale=0.5,\n",
    "        do_ocr=True,\n",
    "        do_picture_description=False,  # Simplified for this challenge\n",
    "        ocr_options=RapidOcrOptions(),\n",
    "    )\n",
    "\n",
    "    converter = DocumentConverter(\n",
    "        format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n",
    "    )\n",
    "\n",
    "    # Convert PDF\n",
    "    if not pdf_path.is_file():\n",
    "        print(f\"Error: PDF file not found at {pdf_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing PDF: {pdf_path}...\")\n",
    "    result = converter.convert(pdf_path)\n",
    "    doc = result.document\n",
    "\n",
    "    # Export to Markdown\n",
    "    markdown_text = doc.export_to_markdown(\n",
    "        page_break_placeholder=PAGE_BREAK_PLACEHOLDER,\n",
    "        image_placeholder=IMAGE_PLACEHOLDER,\n",
    "    )\n",
    "\n",
    "    # Basic cleaning\n",
    "    processed_text = re.sub(r'\\n{3,}', '\\n\\n', markdown_text).strip()\n",
    "\n",
    "    # Save processed text\n",
    "    output_path = output_dir / output_filename\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(processed_text)\n",
    "\n",
    "    print(f\"Processed text saved to: {output_path}\")\n",
    "    print(f\"--- Processing completed in {time.time() - start_time:.2f} seconds ---\")\n",
    "\n",
    "# Run the document processing\n",
    "if PDF_PATH.exists():\n",
    "    process_pdf(PDF_PATH, OUTPUT_DIR, OUTPUT_PROCESSED_TEXT_PATH.name)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  PDF file '{PDF_PATH}' not found. Please add your PDF file to continue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bW-fyG8nUAmd"
   },
   "source": [
    "## Part 2: RAG Pipeline Evaluation\n",
    "\n",
    "Now we'll build and evaluate different RAG configurations using various:\n",
    "- **Chunking strategies** (RecursiveCharacterTextSplitter with different parameters)\n",
    "- **Retrieval methods** (Basic vector search, Parent Document Retriever, Ensemble Retriever)\n",
    "- **Evaluation metrics** (faithfulness, answer relevancy, context precision, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gnlNDyZUAme"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAG Pipeline Evaluation Script for Mini-Challenge.\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- LangChain Imports ---\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryStore\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import ParentDocumentRetriever, EnsembleRetriever\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# --- Ragas Imports ---\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "load_dotenv()\n",
    "PROCESSED_TEXT_PATH = Path(\"output/processed_text.md\")\n",
    "EVAL_DATASET_PATH = Path(\"groundtruth_eval_dataset.csv\")\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# LLM Configuration\n",
    "USE_OLLAMA = True\n",
    "if USE_OLLAMA:\n",
    "    LLM_MODEL = \"gemma:2b\"\n",
    "    EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "    print(f\"Using Ollama: LLM='{LLM_MODEL}', Embeddings='{EMBEDDING_MODEL}'\")\n",
    "else:\n",
    "    from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "    LLM_MODEL = \"gpt-3.5-turbo\"\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    print(f\"Using OpenAI: LLM='{LLM_MODEL}', Embeddings='{EMBEDDING_MODEL}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW3GhMzIUAmg"
   },
   "outputs": [],
   "source": [
    "# Helper Functions for RAG Pipeline\n",
    "\n",
    "def load_processed_text(file_path: Path) -> str:\n",
    "    \"\"\"Loads the processed markdown text.\"\"\"\n",
    "    if not file_path.is_file():\n",
    "        raise FileNotFoundError(f\"Processed text file not found at {file_path}\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_evaluation_dataset(file_path: Path) -> Dataset:\n",
    "    \"\"\"Loads the evaluation dataset from CSV.\"\"\"\n",
    "    if not file_path.is_file():\n",
    "        raise FileNotFoundError(f\"Evaluation dataset not found at {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"ground_truths\"] = df[\"ground_truth\"].apply(lambda x: [str(x)] if pd.notna(x) else [])\n",
    "    df_ragas = df[[\"question\", \"ground_truths\"]]\n",
    "    return Dataset.from_pandas(df_ragas)\n",
    "\n",
    "def get_llm():\n",
    "    \"\"\"Initializes the LLM based on configuration.\"\"\"\n",
    "    if USE_OLLAMA:\n",
    "        return ChatOllama(model=LLM_MODEL, temperature=0.0)\n",
    "    else:\n",
    "        return ChatOpenAI(model_name=LLM_MODEL, temperature=0.0)\n",
    "\n",
    "def get_embeddings():\n",
    "    \"\"\"Initializes the embedding model based on configuration.\"\"\"\n",
    "    if USE_OLLAMA:\n",
    "        return OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "    else:\n",
    "        return OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "\n",
    "def format_docs(docs: list[Document]) -> str:\n",
    "    \"\"\"Formats retrieved documents into a single string for the prompt.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKdb8ST8UAmi"
   },
   "outputs": [],
   "source": [
    "# Chunking Strategies\n",
    "\n",
    "def chunk_text_recursive(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> list[Document]:\n",
    "    \"\"\"Chunks text using RecursiveCharacterTextSplitter.\"\"\"\n",
    "    print(f\"Chunking text: size={chunk_size}, overlap={chunk_overlap}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    docs = [Document(page_content=chunk) for chunk in text_splitter.split_text(text)]\n",
    "    print(f\"Created {len(docs)} chunks.\")\n",
    "    return docs\n",
    "\n",
    "# Test chunking on a sample\n",
    "sample_text = \"This is a sample text for testing chunking strategies. \" * 100\n",
    "sample_chunks = chunk_text_recursive(sample_text, chunk_size=200, chunk_overlap=50)\n",
    "print(f\"Sample created {len(sample_chunks)} chunks from {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDevWbmqUAmk"
   },
   "outputs": [],
   "source": [
    "# Retriever Setup Functions\n",
    "\n",
    "def setup_basic_retriever(docs: list[Document], embeddings, k: int = 3):\n",
    "    \"\"\"Sets up a basic Chroma vector store retriever.\"\"\"\n",
    "    print(f\"Setting up basic Chroma retriever (k={k})...\")\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "def setup_parent_document_retriever(text: str, embeddings,\n",
    "                                    parent_chunk_size: int = 2000,\n",
    "                                    child_chunk_size: int = 400,\n",
    "                                    k_parent: int = 3):\n",
    "    \"\"\"Sets up a Parent Document Retriever.\"\"\"\n",
    "    print(f\"Setting up Parent Document Retriever...\")\n",
    "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=parent_chunk_size)\n",
    "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=child_chunk_size)\n",
    "    vectorstore = Chroma(collection_name=\"parent_doc_retriever\", embedding_function=embeddings)\n",
    "    store = InMemoryStore()\n",
    "\n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "        parent_splitter=parent_splitter,\n",
    "        search_kwargs={\"k\": k_parent}\n",
    "    )\n",
    "\n",
    "    initial_docs = [Document(page_content=doc) for doc in parent_splitter.split_text(text)]\n",
    "    retriever.add_documents(initial_docs, ids=None)\n",
    "    return retriever\n",
    "\n",
    "def setup_ensemble_retriever(docs: list[Document], embeddings, k: int = 3, bm25_weight: float = 0.5):\n",
    "    \"\"\"Sets up an Ensemble Retriever (BM25 + Dense).\"\"\"\n",
    "    print(f\"Setting up Ensemble Retriever (k={k}, bm25_weight={bm25_weight})...\")\n",
    "\n",
    "    # BM25 retriever\n",
    "    bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "    bm25_retriever.k = k\n",
    "\n",
    "    # Dense retriever\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "    dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    # Ensemble retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, dense_retriever],\n",
    "        weights=[bm25_weight, 1.0 - bm25_weight]\n",
    "    )\n",
    "    return ensemble_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aLyMYxuUAmr"
   },
   "outputs": [],
   "source": [
    "# QA Chain Factory\n",
    "\n",
    "# RAG Prompt Template\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know'.\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\n",
    "### ANSWER\n",
    "\"\"\"\n",
    "RAG_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def create_qa_chain(retriever, llm, prompt):\n",
    "    \"\"\"Creates a LangChain QA Runnable.\"\"\"\n",
    "    print(\"Creating QA chain...\")\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    rag_chain_with_source = RunnableParallel(\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    ).assign(answer=rag_chain_from_docs)\n",
    "\n",
    "    def final_chain(question: str):\n",
    "        return rag_chain_with_source.invoke(question)\n",
    "\n",
    "    return final_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyLhv4qvUAmw"
   },
   "outputs": [],
   "source": [
    "# Evaluation Functions\n",
    "\n",
    "EVALUATION_METRICS = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    "]\n",
    "\n",
    "def generate_ragas_dataset(rag_chain, eval_dataset: Dataset) -> Dataset:\n",
    "    \"\"\"Runs the RAG chain on the evaluation dataset to prepare for Ragas.\"\"\"\n",
    "    print(\"Generating answers and contexts for Ragas evaluation...\")\n",
    "    results = []\n",
    "    for row in tqdm(eval_dataset):\n",
    "        question = row[\"question\"]\n",
    "        response = rag_chain(question)\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response[\"answer\"],\n",
    "            \"contexts\": [doc.page_content for doc in response[\"context\"]],\n",
    "            \"ground_truths\": row[\"ground_truths\"]\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return Dataset.from_pandas(results_df)\n",
    "\n",
    "def evaluate_with_ragas(ragas_dataset: Dataset, pipeline_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Evaluates the generated dataset using Ragas.\"\"\"\n",
    "    print(f\"Evaluating '{pipeline_name}' with Ragas...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = evaluate(ragas_dataset, metrics=EVALUATION_METRICS)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Ragas evaluation finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    result_df = result.to_pandas()\n",
    "    output_path = RESULTS_DIR / f\"{pipeline_name}_ragas_results.csv\"\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "\n",
    "    mean_scores = result_df[[m.name for m in EVALUATION_METRICS]].mean().to_dict()\n",
    "    print(f\"Mean scores for {pipeline_name}: {mean_scores}\")\n",
    "    return mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EREc6uU7UAmy"
   },
   "outputs": [],
   "source": [
    "# Main Experiment Runner\n",
    "\n",
    "def run_rag_experiments():\n",
    "    \"\"\"Run all RAG pipeline experiments.\"\"\"\n",
    "    print(\"--- Starting RAG Pipeline Evaluation ---\")\n",
    "\n",
    "    # Load data\n",
    "    try:\n",
    "        processed_text = load_processed_text(PROCESSED_TEXT_PATH)\n",
    "        eval_dataset = load_evaluation_dataset(EVAL_DATASET_PATH)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå {e}\")\n",
    "        print(\"Please make sure you have:\")\n",
    "        print(\"1. Run the document processing step\")\n",
    "        print(\"2. Created the evaluation dataset CSV\")\n",
    "        return\n",
    "\n",
    "    # Initialize components\n",
    "    llm = get_llm()\n",
    "    embeddings = get_embeddings()\n",
    "\n",
    "    # Define experiment configurations\n",
    "    configurations = [\n",
    "        {\n",
    "            \"name\": \"baseline_recursive_1000_200\",\n",
    "            \"chunking_func\": chunk_text_recursive,\n",
    "            \"chunking_params\": {\"chunk_size\": 1000, \"chunk_overlap\": 200},\n",
    "            \"retriever_func\": setup_basic_retriever,\n",
    "            \"retriever_params\": {\"k\": 3},\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"chunking_recursive_500_100\",\n",
    "            \"chunking_func\": chunk_text_recursive,\n",
    "            \"chunking_params\": {\"chunk_size\": 500, \"chunk_overlap\": 100},\n",
    "            \"retriever_func\": setup_basic_retriever,\n",
    "            \"retriever_params\": {\"k\": 3},\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"retriever_parent_doc_2000_400\",\n",
    "            \"chunking_func\": None,\n",
    "            \"chunking_params\": {},\n",
    "            \"retriever_func\": setup_parent_document_retriever,\n",
    "            \"retriever_params\": {\"parent_chunk_size\": 2000, \"child_chunk_size\": 400, \"k_parent\": 3},\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"retriever_ensemble_bm25_0.5\",\n",
    "            \"chunking_func\": chunk_text_recursive,\n",
    "            \"chunking_params\": {\"chunk_size\": 500, \"chunk_overlap\": 100},\n",
    "            \"retriever_func\": setup_ensemble_retriever,\n",
    "            \"retriever_params\": {\"k\": 3, \"bm25_weight\": 0.5},\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Run experiments\n",
    "    all_results = []\n",
    "    for config in configurations:\n",
    "        print(f\"\\n--- Running Configuration: {config['name']} ---\")\n",
    "\n",
    "        try:\n",
    "            # Setup retriever\n",
    "            if config[\"chunking_func\"]:\n",
    "                docs = config[\"chunking_func\"](processed_text, **config[\"chunking_params\"])\n",
    "                retriever_input = docs\n",
    "            else:\n",
    "                retriever_input = processed_text\n",
    "\n",
    "            retriever = config[\"retriever_func\"](retriever_input, embeddings, **config[\"retriever_params\"])\n",
    "\n",
    "            # Create QA chain\n",
    "            qa_chain = create_qa_chain(retriever, llm, RAG_PROMPT)\n",
    "\n",
    "            # Generate and evaluate\n",
    "            ragas_dataset = generate_ragas_dataset(qa_chain, eval_dataset)\n",
    "            mean_scores = evaluate_with_ragas(ragas_dataset, config['name'])\n",
    "            mean_scores['name'] = config['name']\n",
    "            all_results.append(mean_scores)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in configuration {config['name']}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Summarize results\n",
    "    if all_results:\n",
    "        print(\"\\n--- Overall Results Summary ---\")\n",
    "        summary_df = pd.DataFrame(all_results).set_index('name')\n",
    "        print(summary_df)\n",
    "\n",
    "        summary_output_path = RESULTS_DIR / \"all_pipelines_summary_results.csv\"\n",
    "        summary_df.to_csv(summary_output_path)\n",
    "        print(f\"\\nSaved summary results to: {summary_output_path}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Run experiments (uncomment to execute)\n",
    "# results = run_rag_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H4wdCsgUAmz"
   },
   "source": [
    "## Part 3: Results Analysis\n",
    "\n",
    "After running all experiments, analyze your results:\n",
    "\n",
    "1. **Compare metrics** across different configurations\n",
    "2. **Identify best-performing** setups for different use cases\n",
    "3. **Understand trade-offs** between complexity and performance\n",
    "4. **Document insights** for future RAG implementations\n",
    "\n",
    "### Quiz Questions\n",
    "\n",
    "1. What is the purpose of `docling` in the first part of this challenge?\n",
    "2. Explain the difference between `chunk_size` and `chunk_overlap` in `RecursiveCharacterTextSplitter`.\n",
    "3. What is the fundamental difference between dense retrieval (vector search) and sparse retrieval (BM25)?\n",
    "4. Describe the idea behind the `ParentDocumentRetriever`. When might it be useful?\n",
    "5. What does the `ragas` metric `faithfulness` measure? Why is it important?\n",
    "6. What does the `ragas` metric `context_recall` measure?\n",
    "7. What is the role of a reranker in a RAG pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLkr0vydUAm0"
   },
   "outputs": [],
   "source": [
    "# Create Sample Evaluation Dataset (if needed)\n",
    "\n",
    "def create_sample_evaluation_dataset():\n",
    "    \"\"\"Create a sample evaluation dataset for testing.\"\"\"\n",
    "    sample_data = {\n",
    "        \"question\": [\n",
    "            \"How do you activate the public REST APIs?\",\n",
    "            \"What authentication method is mentioned for the APIs?\",\n",
    "            \"Can you filter data using the reporting API?\",\n",
    "            \"What is the purpose of the status endpoint?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"To activate the public REST APIs, you need to enable the 'Enable Public Rest API' option in the application's general settings.\",\n",
    "            \"The document mentions using API keys for authentication with the public REST APIs.\",\n",
    "            \"Yes, the reporting API allows filtering data based on dimensions, scenarios, time periods, and other criteria using query parameters.\",\n",
    "            \"The status endpoint allows you to retrieve the current status of processes like Running, Completed, or Failed.\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df.to_csv(\"groundtruth_eval_dataset.csv\", index=False)\n",
    "    print(\"‚úÖ Sample evaluation dataset created as 'groundtruth_eval_dataset.csv'\")\n",
    "    print(\"üìù Please replace this with real Q&A pairs from your document!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XGAArN5cOmp"
   },
   "source": [
    "## Part 4: Bonus - Deep Dive into Retrieval Mechanisms\n",
    "\n",
    "1.  How do vector search algorithms like Nearest Neighbor (NN) and Approximate Nearest Neighbor (ANN) efficiently find similar documents in a high-dimensional vector space? What are the core principles behind algorithms like HNSW, LSH, and IVF?\n",
    "\n",
    "    #### Resources:\n",
    "\n",
    "      * **HNSW (Hierarchical Navigable Small World):**\n",
    "\n",
    "          * **Blog Post:** \"Hierarchical Navigable Small Worlds (HNSW)\"\n",
    "              * **Link:** [https://www.pinecone.io/learn/series/faiss/hnsw/](https://www.pinecone.io/learn/series/faiss/hnsw/)\n",
    "\n",
    "      * **LSH (Locality Sensitive Hashing):**\n",
    "\n",
    "          * **Lecture Notes:** Stanford CS246 Lecture Notes on LSH (Chapter 3)\n",
    "              * **Link:** [http://www.mmds.org/mmds/v2.1/ch03-lsh.pdf](http://www.mmds.org/mmds/v2.1/ch03-lsh.pdf)\n",
    "      \n",
    "      * **IVF (Inverted File Index):**\n",
    "\n",
    "          * **Faiss Documentation:** \"The Inverted File Index (IVF)\"\n",
    "              * **Link:** [https://github.com/facebookresearch/faiss/wiki/Faiss-index-types](https://github.com/facebookresearch/faiss/wiki/Faiss-index-types)\n",
    "\n",
    "      * **General Vector Search/ANN:**\n",
    "\n",
    "          * **Google Cloud Blog:** \"Understanding Nearest Neighbor Search\"\n",
    "              * **Link:** [https://cloud.google.com/blog/products/databases/spanner-now-supports-approximate-nearest-neighbor-search](https://cloud.google.com/blog/products/databases/spanner-now-supports-approximate-nearest-neighbor-search)\n",
    "\n",
    "2.  How Does Hybrid Search Work?\n",
    "\n",
    "    #### Resources:\n",
    "\n",
    "      * **Pinecone Blog:** \"Hybrid Search: Combining Keyword and Vector Search\"\n",
    "          * **Link:** [https://www.pinecone.io/learn/hybrid-search/](https://www.pinecone.io/learn/hybrid-search/)\n",
    "\n",
    "\n",
    "\n",
    "2.  How Do We Measure How Good a Retrieval System Is?\n",
    "\n",
    "    #### Resources:\n",
    "      * **Survey:** \"Evaluation of Retrieval-Augmented Generation: A Survey\"\n",
    "          * **Link:** [https://arxiv.org/pdf/2405.07437](https://arxiv.org/pdf/2405.07437)\n",
    "      * **Hugging Face:** \"RAG Evaluation\"\n",
    "          * **Link:** [https://huggingface.co/learn/cookbook/en/rag_evaluation](https://huggingface.co/learn/cookbook/en/rag_evaluation)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
