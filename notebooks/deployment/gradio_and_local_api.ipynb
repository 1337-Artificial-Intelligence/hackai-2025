{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_15uhOvvzB0S"
      },
      "source": [
        "<h1>Deploy Gradio Apps to Hugging Face Spaces</h1>\n",
        "\n",
        "This notebook will teach you how to create and deploy Gradio applications to Hugging Face Spaces in simple steps.\n",
        "\n",
        "## What you'll learn:\n",
        "- Create a simple Gradio app\n",
        "- Test it locally\n",
        "- Deploy to Hugging Face Spaces\n",
        "- Best practices and tips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVBM8eWBzB0S"
      },
      "source": [
        "## 📦 Step 1: Install Required Libraries\n",
        "\n",
        "First, let's install Gradio and other necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx4fw0mHzB0S",
        "outputId": "ff2d0380-d264-446f-acba-1fe8635b449a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.11 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install gradio\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc38wW0PzB0T"
      },
      "source": [
        "## 🎯 Step 2: Create Your First Gradio App\n",
        "\n",
        "Let's start with a simple text processing app that converts text to uppercase and counts words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5UZZ28MzB0T",
        "outputId": "fb5ade08-427c-4bb7-e756-5b7c86557867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uppercase: HELLO WORLD! THIS IS A TEST.\n",
            "Word count: 6\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def process_text(text):\n",
        "    \"\"\"\n",
        "    Simple function that processes text:\n",
        "    - Converts to uppercase\n",
        "    - Counts words\n",
        "    - Returns both results\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"Please enter some text!\", 0\n",
        "\n",
        "    uppercase_text = text.upper()\n",
        "    word_count = len(text.split())\n",
        "\n",
        "    return uppercase_text, word_count\n",
        "\n",
        "# Test the function\n",
        "test_result = process_text(\"Hello world! This is a test.\")\n",
        "print(f\"Uppercase: {test_result[0]}\")\n",
        "print(f\"Word count: {test_result[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX2-C_L_zB0T"
      },
      "source": [
        "## 🎨 Step 3: Create the Gradio Interface\n",
        "\n",
        "Now let's create a beautiful Gradio interface for our function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "zYBqoW2PzB0T",
        "outputId": "ffbfc3cb-cba4-4efd-cf0c-d37c927b1c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a3712d5e96c3bf6e1b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a3712d5e96c3bf6e1b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a3712d5e96c3bf6e1b.gradio.live\n"
          ]
        }
      ],
      "source": [
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=process_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"Enter your text\",\n",
        "            placeholder=\"Type something here...\",\n",
        "            lines=3\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Uppercase Text\", lines=3),\n",
        "        gr.Number(label=\"Word Count\")\n",
        "    ],\n",
        "    title=\"📝 Text Processor\",\n",
        "    description=\"Enter some text and I'll convert it to uppercase and count the words!\",\n",
        "    examples=[\n",
        "        [\"Hello world!\"],\n",
        "        [\"Gradio makes machine learning demos easy!\"],\n",
        "        [\"This is a sample text with multiple words to demonstrate the word counting feature.\"]\n",
        "    ],\n",
        "    theme=gr.themes.Soft()\n",
        ")\n",
        "\n",
        "# Launch the app locally\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIGgVj1ozB0T"
      },
      "source": [
        "## 📁 Step 4: Prepare Files for Deployment\n",
        "\n",
        "To deploy to Hugging Face Spaces, we need to create specific files. Let's create them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgQHmaCEzB0T",
        "outputId": "86ec0330-eeeb-45dc-b2e0-22755400b315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ app.py created successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Create the main app file\n",
        "app_code = '''import gradio as gr\n",
        "\n",
        "def process_text(text):\n",
        "    \"\"\"\n",
        "    Simple function that processes text:\n",
        "    - Converts to uppercase\n",
        "    - Counts words\n",
        "    - Returns both results\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"Please enter some text!\", 0\n",
        "\n",
        "    uppercase_text = text.upper()\n",
        "    word_count = len(text.split())\n",
        "\n",
        "    return uppercase_text, word_count\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=process_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"Enter your text\",\n",
        "            placeholder=\"Type something here...\",\n",
        "            lines=3\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Uppercase Text\", lines=3),\n",
        "        gr.Number(label=\"Word Count\")\n",
        "    ],\n",
        "    title=\"📝 Text Processor\",\n",
        "    description=\"Enter some text and I'll convert it to uppercase and count the words!\",\n",
        "    examples=[\n",
        "        [\"Hello world!\"],\n",
        "        [\"Gradio makes machine learning demos easy!\"],\n",
        "        [\"This is a sample text with multiple words to demonstrate the word counting feature.\"]\n",
        "    ],\n",
        "    theme=gr.themes.Soft()\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "'''\n",
        "\n",
        "# Save the app file\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"✅ app.py created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv66Fg8yzB0T",
        "outputId": "1b7c23f8-0396-4422-dcd1-936cd00f354e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ requirements.txt created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create requirements.txt file\n",
        "requirements = '''gradio>=4.0.0\n",
        "'''\n",
        "\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(requirements)\n",
        "\n",
        "print(\"✅ requirements.txt created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm56YyE9zB0T",
        "outputId": "ebf15b98-fae8-4a41-8aef-0b7beee2026c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ README.md created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create README.md file\n",
        "readme_content = '''---\n",
        "title: Text Processor\n",
        "emoji: 📝\n",
        "colorFrom: blue\n",
        "colorTo: purple\n",
        "sdk: gradio\n",
        "sdk_version: 4.44.0\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "license: mit\n",
        "---\n",
        "\n",
        "# Text Processor\n",
        "\n",
        "A simple Gradio app that:\n",
        "- Converts text to uppercase\n",
        "- Counts the number of words\n",
        "\n",
        "Built with ❤️ using Gradio!\n",
        "'''\n",
        "\n",
        "with open('README.md', 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(\"✅ README.md created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzIfQM0hzB0U"
      },
      "source": [
        "## 🔐 Step 5: Set Up Hugging Face Authentication\n",
        "\n",
        "You'll need a Hugging Face account and token to deploy. Here's how to set it up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8W4W0c8zB0U",
        "outputId": "f830f2a2-7aca-4179-ca5e-7d03b03ac9e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔑 Please enter your Hugging Face token:\n",
            "Get it from: https://huggingface.co/settings/tokens\n",
            "Enter your HF token: ··········\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, login\n",
        "import getpass\n",
        "\n",
        "# Login to Hugging Face\n",
        "# You can get your token from: https://huggingface.co/settings/tokens\n",
        "print(\"🔑 Please enter your Hugging Face token:\")\n",
        "print(\"Get it from: https://huggingface.co/settings/tokens\")\n",
        "\n",
        "token = getpass.getpass(\"Enter your HF token: \")\n",
        "login(token=token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diEaB8kjzB0U"
      },
      "source": [
        "## 🚀 Step 6: Deploy to Hugging Face Spaces\n",
        "\n",
        "Now let's deploy our app to Hugging Face Spaces!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOQEuooSzB0U",
        "outputId": "a6a4bcba-9cc4-4c27-bcb1-dc19a328cd08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Space 'medmekk/my-text-processor-demo' created successfully!\n",
            "🎉 App deployed successfully!\n",
            "🌐 Visit your app at: https://huggingface.co/spaces/medmekk/my-text-processor-demo\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "def deploy_to_spaces(space_name, username):\n",
        "    \"\"\"\n",
        "    Deploy the current directory to Hugging Face Spaces\n",
        "\n",
        "    Args:\n",
        "        space_name: Name for your space (e.g., 'my-text-processor')\n",
        "        username: Your Hugging Face username\n",
        "    \"\"\"\n",
        "    api = HfApi()\n",
        "\n",
        "    repo_id = f\"{username}/{space_name}\"\n",
        "\n",
        "    try:\n",
        "        # Create the space\n",
        "        api.create_repo(\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"space\",\n",
        "            space_sdk=\"gradio\",\n",
        "            private=False\n",
        "        )\n",
        "        print(f\"✅ Space '{repo_id}' created successfully!\")\n",
        "\n",
        "        # Upload files\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=\"app.py\",\n",
        "            path_in_repo=\"app.py\",\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"space\"\n",
        "        )\n",
        "\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=\"requirements.txt\",\n",
        "            path_in_repo=\"requirements.txt\",\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"space\"\n",
        "        )\n",
        "\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=\"README.md\",\n",
        "            path_in_repo=\"README.md\",\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"space\"\n",
        "        )\n",
        "\n",
        "        print(f\"🎉 App deployed successfully!\")\n",
        "        print(f\"🌐 Visit your app at: https://huggingface.co/spaces/{repo_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        print(\"Make sure you're logged in and the space name doesn't already exist.\")\n",
        "\n",
        "# Example usage (uncomment and modify with your details):\n",
        "deploy_to_spaces(\"my-text-processor-demo\", \"medmekk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFJEzsx1zB0U"
      },
      "source": [
        "## 📋 Step 7: Alternative Deployment Methods\n",
        "\n",
        "If you prefer, you can also deploy manually through the Hugging Face website:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uysYsD9ZzB0U"
      },
      "source": [
        "### Method 1: Web Interface\n",
        "1. Go to [huggingface.co/new-space](https://huggingface.co/new-space)\n",
        "2. Choose \"Gradio\" as the SDK\n",
        "3. Upload your files (`app.py`, `requirements.txt`, `README.md`)\n",
        "4. Your app will build and deploy automatically!\n",
        "\n",
        "### Method 2: Git Clone Method\n",
        "```bash\n",
        "# Create the repo and clone it to work locally (best option)\n",
        "git clone https://huggingface.co/spaces/your-username/your-space-name\n",
        "cd your-space-name\n",
        "\n",
        "# Copy your files\n",
        "cp app.py requirements.txt README.md ./\n",
        "\n",
        "# Commit and push\n",
        "git add .\n",
        "git commit -m \"Add Gradio app\"\n",
        "git push\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq10hbYfzB0U"
      },
      "source": [
        "## 💡 Best Practices & Tips\n",
        "\n",
        "### 1. File Structure\n",
        "Your Hugging Face Space should have:\n",
        "```\n",
        "your-space/\n",
        "├── app.py          # Main application file\n",
        "├── requirements.txt # Python dependencies\n",
        "└── README.md       # Space configuration and description\n",
        "```\n",
        "\n",
        "### 2. README.md Configuration\n",
        "The header in README.md is crucial:\n",
        "- `sdk: gradio` tells HF to use Gradio\n",
        "- `app_file: app.py` specifies the main file\n",
        "- `sdk_version` should match your Gradio version\n",
        "\n",
        "### 3. Requirements.txt Tips\n",
        "- Pin versions for stability: `gradio==4.44.0`\n",
        "- Include all dependencies your app needs\n",
        "- Keep it minimal to reduce build time\n",
        "\n",
        "### 4. App.py Best Practices\n",
        "- Always include error handling\n",
        "- Use descriptive labels and placeholders\n",
        "- Add examples to showcase functionality\n",
        "- Test locally before deploying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAJmDNdbzB0U"
      },
      "source": [
        "## 🔧 Troubleshooting Common Issues\n",
        "\n",
        "### Build Failures\n",
        "- Check your `requirements.txt` for typos\n",
        "- Ensure all imports in `app.py` are available\n",
        "- Check the build logs in your Space's \"Logs\" tab\n",
        "\n",
        "### App Not Loading\n",
        "- Make sure `app.py` has `if __name__ == \"__main__\": demo.launch()`\n",
        "- Check that your main interface is named `demo`\n",
        "- Verify the `app_file` in README.md matches your filename\n",
        "\n",
        "### Performance Issues\n",
        "- Optimize your functions for speed\n",
        "- Consider using Gradio's built-in caching\n",
        "- For heavy models, consider using GPU spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ilVTspm_-E3"
      },
      "source": [
        "# Local API\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgUToJGEJfVX"
      },
      "source": [
        "You can deploy your model on custom resources such as Google Colab, making it accessible via a public API. This is especially useful when you want to demonstrate your model's capabilities.\n",
        "\n",
        "Here is an example of how to deploy a text translation model (FLAN-T5) using FastAPI and Ngrok.\n",
        "\n",
        "You can deploy any other model in the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkqDZ_98IlJE",
        "outputId": "8b8ea550-63fa-4d40-f3ab-6d58d7cc1728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/95.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn transformers accelerate pyngrok nest-asyncio -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyIF-lcBLGVE"
      },
      "source": [
        "This sets up the translation API and loads the pretrained **flan-t5-base** model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR9Da1YLI2Oj",
        "outputId": "77c7e476-e1ae-4707-e5de-8f44e4229d3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Initialize app\n",
        "app = FastAPI(title=\"T5 Translation API\", description=\"Translate English to other languages using FLAN-T5\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define request schema\n",
        "class InputText(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# Endpoint for text translation\n",
        "@app.post(\"/translate\")\n",
        "def generate_text(data: InputText):\n",
        "    input_ids = tokenizer(data.text, return_tensors=\"pt\").input_ids.to(device)\n",
        "    output_ids = model.generate(input_ids)\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return {\"output\": output_text}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx4EhbSKLCOc"
      },
      "source": [
        "Create an Ngrok Account :\n",
        "- Sign up (or log in) to Ngrok:\n",
        "👉 https://dashboard.ngrok.com/signup\n",
        "\n",
        "- Once logged in, go to:\n",
        "👉 https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n",
        "- Copy your authtoken (it looks like 1pJsd0X....)\n",
        "\n",
        "- Run this in your Colab notebook (replace <YOUR_AUTHTOKEN>):\n",
        "```!ngrok config add-authtoken <YOUR_AUTHTOKEN>```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2LHSlyBMI9g",
        "outputId": "b695c865-271f-49be-e90b-b5332a6ee2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken <YOUR_AUTHTOKEN>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShUhiDFOKF-J",
        "outputId": "989e2376-f00e-4d7e-87d8-1d48175a8aae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌐 Public URL: NgrokTunnel: \"https://ba76-34-150-220-201.ngrok-free.app\" -> \"http://localhost:8000\"/docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [8522]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     197.230.240.146:0 - \"POST /translate HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# Enable nested event loops for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Create a public URL\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"🌐 Public URL: {public_url}/docs\")\n",
        "\n",
        "# Run the server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lfvFJKSM9ed"
      },
      "source": [
        "In another file in your local computer or another colab notebook, execute the code below to get a response from your endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzTspJwXM8fN"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Replace this with your actual public ngrok URL\n",
        "url = \"https://ba76-34-150-220-201.ngrok-free.app/translate\"\n",
        "\n",
        "# Input text for the translation model\n",
        "payload = {\n",
        "    \"text\": \"translate English to French: What time is the meeting?\"\n",
        "}\n",
        "\n",
        "# Make the POST request\n",
        "response = requests.post(url, json=payload)\n",
        "\n",
        "print(\"Model Output:\", response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFmNMVziyHVz"
      },
      "source": [
        "\n",
        "# HF inference API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnWDLcC4PIjW"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# Upload entire folder\n",
        "api.upload_folder(\n",
        "    folder_path=\"path/to/your/local/model\",\n",
        "    repo_id=\"your-username/your-model-name\",\n",
        "    repo_type=\"model\"\n",
        ")_pretrained(\"path/to/your/local/model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"path/to/your/local/model\")\n",
        "\n",
        "# Push to Hub\n",
        "model.push_to_hub(\"your-username/your-model-name\")\n",
        "tokenizer.push_to_hub(\"your-username/your-model-name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge"
      ],
      "metadata": {
        "id": "v7RJiZrC0_RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy a simple LLM inference gradio demo"
      ],
      "metadata": {
        "id": "K8mDKEZm1I8K"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}