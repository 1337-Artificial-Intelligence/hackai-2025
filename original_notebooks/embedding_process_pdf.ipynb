{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elbou\\miniconda3\\envs\\mlw\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Recreation of the document-processing-for-ai.ipynb notebook logic.\n",
    "Processes a PDF, chunks it using LLM suggestions, enriches chunks with context,\n",
    "and demonstrates a simple RAG setup.\n",
    "\"\"\"\n",
    "\n",
    "import base64\n",
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import ollama # Make sure ollama server is running\n",
    "\n",
    "# --- Docling Imports ---\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    RapidOcrOptions,\n",
    "    # smolvlm_picture_description, # Assuming this exists or define a placeholder\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "# --- Scikit-learn Imports ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using smolvlm_picture_description.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Placeholder for smolvlm if not directly available or substitute\n",
    "# For demonstration, we'll use a simple placeholder function/value if needed.\n",
    "# If smolvlm_picture_description is available in your docling setup, use it.\n",
    "# Otherwise, you might need to disable picture description or use a fallback.\n",
    "# Let's assume for now picture description is enabled but might yield basic results\n",
    "# without the exact smolvlm setup.\n",
    "try:\n",
    "    # Attempt to import if it's part of your docling install structure\n",
    "    from docling.datamodel.pipeline_options import smolvlm_picture_description\n",
    "    PICTURE_DESCRIPTION_OPTIONS = smolvlm_picture_description\n",
    "    print(\"Using smolvlm_picture_description.\")\n",
    "except ImportError:\n",
    "    print(\"Warning: smolvlm_picture_description not found. Picture description might be basic.\")\n",
    "    # Define a fallback or disable picture description if necessary\n",
    "    # Disabling for simplicity if not found:\n",
    "    # PICTURE_DESCRIPTION_OPTIONS = None\n",
    "    # Or use a placeholder that might exist:\n",
    "    PICTURE_DESCRIPTION_OPTIONS = None # Set to None or a valid option\n",
    "\n",
    "\n",
    "PDF_PATH = Path(\"API_FR.pdf\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True) # Create output dir if needed\n",
    "\n",
    "# LLM Configuration\n",
    "MODEL = \"hf.co/google/gemma-3-12b-it-qat-q4_0-gguf\" # Make sure this model is pulled in Ollama\n",
    "TEMPERATURE = 0.0\n",
    "MIN_P = 0.01\n",
    "REPEAT_PENALTY = 1.0\n",
    "TOP_K = 64\n",
    "TOP_P = 0.95\n",
    "OLLAMA_KEEP_ALIVE = \"-1h\" # Keep model loaded for a while\n",
    "\n",
    "# Placeholders\n",
    "IMAGE_PLACEHOLDER = \"<!__ image_placeholder __>\"\n",
    "PAGE_BREAK_PLACEHOLDER = \"<!__ page_break __>\"\n",
    "\n",
    "# Chunking Pattern (Initial basic split before LLM refinement)\n",
    "# Using Markdown H2 headers as split points initially\n",
    "INITIAL_SPLIT_PATTERN = \"\\n## \"\n",
    "\n",
    "# --- Prompts ---\n",
    "\n",
    "CHUNKING_PROMPT = \"\"\"\n",
    "You are an assistant specialized in splitting text into semantically consistent sections.\n",
    "\n",
    "<instructions>\n",
    "<instruction>The text has been divided into initial chunks, each marked with <|start_chunk_X|> and <|end_chunk_X|> tags, where X is the chunk number.</instruction>\n",
    "<instruction>Identify points where splits should occur, such that consecutive chunks of similar themes stay together.</instruction>\n",
    "<instruction>Each final combined section should ideally be between 200 and 1000 words (this is a guideline, semantic coherence is more important).</instruction>\n",
    "<instruction>If chunks 1 and 2 belong together but chunk 3 starts a new topic, suggest a split after chunk 2.</instruction>\n",
    "<instruction>The split points must be listed in ascending order.</instruction>\n",
    "<instruction>Provide your response ONLY in the form: 'split_after: 3, 5' (use the number of the chunk AFTER which the split should occur).</instruction>\n",
    "<instruction>If no splits are suitable other than the initial ones, you might return just the last chunk number, e.g., 'split_after: 15'.</instruction>\n",
    "</instructions>\n",
    "\n",
    "This is the document text with initial chunk markers:\n",
    "<document>\n",
    "{document_text}\n",
    "</document>\n",
    "\n",
    "Respond ONLY with the IDs of the chunks AFTER which a split should occur, in the specified format 'split_after: X, Y, Z'.\n",
    "YOU MUST RESPOND WITH AT LEAST ONE SPLIT POINT suggestion (even if it's just the last chunk).\n",
    "\"\"\".strip()\n",
    "\n",
    "CONTEXTUALIZER_PROMPT = \"\"\"\n",
    "You are an assistant specialized in analyzing document chunks and providing relevant context for search retrieval.\n",
    "\n",
    "<instructions>\n",
    "<instruction>You will be given a full document and a specific chunk from that document.</instruction>\n",
    "<instruction>Provide 2-3 concise sentences that situate this chunk within the broader document, focusing on information useful for retrieval.</instruction>\n",
    "<instruction>Identify the main topic or concept discussed in the chunk.</instruction>\n",
    "<instruction>Include relevant information or comparisons from the broader document context if they help clarify the chunk's meaning or significance (e.g., overall trends, specific product lines mentioned).</instruction>\n",
    "<instruction>Note how this information relates to the overall theme or purpose of the document (e.g., financial results, product announcements).</instruction>\n",
    "<instruction>Include key figures, dates, or percentages from the chunk or surrounding context if they provide important context for search.</instruction>\n",
    "<instruction>Avoid phrases like \"This chunk discusses...\" or \"In this chunk...\". Instead, directly state the context.</instruction>\n",
    "<instruction>Keep your response brief (target 50-100 tokens) and focused on improving search retrieval for this specific chunk.</instruction>\n",
    "</instructions>\n",
    "\n",
    "Here is the full document:\n",
    "<document>\n",
    "{document}\n",
    "</document>\n",
    "\n",
    "Here is the specific chunk to contextualize:\n",
    "<chunk>\n",
    "{chunk}\n",
    "</chunk>\n",
    "\n",
    "Respond ONLY with the succinct context for this chunk. Do not add any explanations or conversational text.\n",
    "\"\"\".strip()\n",
    "\n",
    "RAG_PROMPT = \"\"\"\n",
    "Use the following context pieces to answer the question. Each context piece contains a chunk from a larger document and its generated context summary.\n",
    "\n",
    "<contexts>\n",
    "{contexts}\n",
    "</contexts>\n",
    "\n",
    "Based *only* on the provided contexts, answer the following question:\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def replace_occurrences(text: str, target: str, replacements: list[str]) -> str:\n",
    "    \"\"\"Replaces sequential occurrences of a target string with replacements.\"\"\"\n",
    "    for replacement in replacements:\n",
    "        if target in text:\n",
    "            # Replace only the first occurrence found in each iteration\n",
    "            text = text.replace(target, replacement, 1)\n",
    "        else:\n",
    "            print(f\"Warning: No more occurrences of '{target}' found for replacement: '{replacement[:50]}...'\")\n",
    "            # Decide how to handle: break, continue, raise error?\n",
    "            # For robustness, let's just break and leave remaining placeholders if any\n",
    "            break\n",
    "            # Alternative: raise ValueError(f\"No more occurrences of {target} found in the text for replacement {replacement}\")\n",
    "    # Check if any placeholders remain\n",
    "    if target in text:\n",
    "        print(f\"Warning: Some occurrences of '{target}' remained unreplaced.\")\n",
    "    return text\n",
    "\n",
    "def call_model(prompt: str) -> str:\n",
    "    \"\"\"Calls the Ollama model with the specified prompt and parameters.\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            keep_alive=OLLAMA_KEEP_ALIVE,\n",
    "            options={\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"min_p\": MIN_P,\n",
    "                \"repeat_penalty\": REPEAT_PENALTY,\n",
    "                \"top_k\": TOP_K,\n",
    "                \"top_p\": TOP_P,\n",
    "                \"num_ctx\": 16384 # Set context window if needed, depends on model\n",
    "            }\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Ollama model: {e}\")\n",
    "        return f\"Error: Could not get response from model. {e}\"\n",
    "\n",
    "def split_text_by_llm_suggestions(tagged_text: str, llm_response: str) -> list[str]:\n",
    "    \"\"\"Splits the initially tagged text based on LLM's split_after suggestions.\"\"\"\n",
    "    split_after_indices = set()\n",
    "    if \"split_after:\" in llm_response.lower():\n",
    "        try:\n",
    "            split_points_str = llm_response.lower().split(\"split_after:\")[1].strip()\n",
    "            if split_points_str: # Ensure there are numbers after 'split_after:'\n",
    "                 split_after_indices = {int(x.strip()) for x in split_points_str.split(\",\") if x.strip().isdigit()}\n",
    "            else:\n",
    "                 print(\"Warning: 'split_after:' found but no numbers followed.\")\n",
    "        except (IndexError, ValueError) as e:\n",
    "            print(f\"Warning: Could not parse LLM split suggestions '{llm_response}'. Error: {e}. Returning text as single chunk.\")\n",
    "            # Fallback: Find all chunk content and return as one big chunk\n",
    "            chunk_pattern = r\"<\\|start_chunk_\\d+\\|>(.*?)<\\|end_chunk_\\d+\\|>\"\n",
    "            all_content = re.findall(chunk_pattern, tagged_text, re.DOTALL)\n",
    "            return [\"\\n\".join(all_content).strip()] if all_content else []\n",
    "\n",
    "    print(f\"LLM suggested splitting after chunk indices: {sorted(list(split_after_indices))}\")\n",
    "\n",
    "    # Find all initial chunks using the tags\n",
    "    chunk_pattern = r\"<\\|start_chunk_(\\d+)\\|>(.*?)<\\|end_chunk_\\1\\|>\"\n",
    "    # Use re.DOTALL to make '.' match newlines within chunks\n",
    "    initial_chunks = re.findall(chunk_pattern, tagged_text, re.DOTALL)\n",
    "\n",
    "    if not initial_chunks:\n",
    "        print(\"Error: Could not find any initial chunks in the tagged text.\")\n",
    "        return []\n",
    "\n",
    "    final_sections = []\n",
    "    current_section_content = []\n",
    "\n",
    "    for chunk_id_str, chunk_content in initial_chunks:\n",
    "        chunk_id = int(chunk_id_str)\n",
    "        current_section_content.append(chunk_content.strip())\n",
    "\n",
    "        if chunk_id in split_after_indices:\n",
    "            final_sections.append(\"\\n\".join(current_section_content).strip())\n",
    "            current_section_content = [] # Start a new section\n",
    "\n",
    "    # Add the last section if it has content\n",
    "    if current_section_content:\n",
    "        final_sections.append(\"\\n\".join(current_section_content).strip())\n",
    "\n",
    "    # Filter out empty sections just in case\n",
    "    final_sections = [section for section in final_sections if section]\n",
    "\n",
    "    print(f\"Split into {len(final_sections)} sections based on LLM suggestions.\")\n",
    "    return final_sections\n",
    "\n",
    "\n",
    "# --- RAG Classes/Functions ---\n",
    "\n",
    "class SimpleRetriever:\n",
    "    \"\"\"A simple TF-IDF based retriever.\"\"\"\n",
    "    def __init__(self, texts: list[str]):\n",
    "        if not texts:\n",
    "            raise ValueError(\"Cannot initialize SimpleRetriever with empty text list.\")\n",
    "        self.texts = texts\n",
    "        print(f\"Initializing SimpleRetriever with {len(texts)} text chunks.\")\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        try:\n",
    "            self.text_vectors = self.vectorizer.fit_transform(self.texts)\n",
    "            print(\"TF-IDF vectors created successfully.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error during TF-IDF vectorization: {e}\")\n",
    "            print(\"This might happen if the input text is empty or contains only stop words.\")\n",
    "            # Handle error, maybe by setting vectors to None or raising exception\n",
    "            self.text_vectors = None\n",
    "\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> tuple[list[str], list[float]]:\n",
    "        \"\"\"Retrieves top k relevant texts for a given query.\"\"\"\n",
    "        if self.text_vectors is None:\n",
    "            print(\"Error: TF-IDF vectors not available.\")\n",
    "            return [], []\n",
    "        if k <= 0:\n",
    "            return [], []\n",
    "\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        # similarities will be a 2D array, get the first row\n",
    "        similarities = cosine_similarity(query_vector, self.text_vectors)[0]\n",
    "\n",
    "        # Get indices of top k similarities (descending order)\n",
    "        # Handle case where k is larger than number of documents\n",
    "        num_docs = len(self.texts)\n",
    "        actual_k = min(k, num_docs)\n",
    "        if actual_k == 0:\n",
    "            return [], []\n",
    "\n",
    "        # Argsort gives indices of smallest to largest, so we take the last 'actual_k' and reverse them\n",
    "        top_k_indices = np.argsort(similarities)[-actual_k:][::-1]\n",
    "\n",
    "        retrieved_texts = [self.texts[i] for i in top_k_indices]\n",
    "        retrieved_scores = [similarities[i] for i in top_k_indices]\n",
    "\n",
    "        return retrieved_texts, retrieved_scores\n",
    "\n",
    "def ask_question(query: str, retriever: SimpleRetriever, k: int = 3) -> str:\n",
    "    \"\"\"Retrieves context and asks the LLM to answer a question based on it.\"\"\"\n",
    "    print(f\"\\n--- Answering Question (k={k}): {query} ---\")\n",
    "    retrieved_texts, retrieved_scores = retriever.retrieve(query, k=k)\n",
    "\n",
    "    if not retrieved_texts:\n",
    "        return \"Could not retrieve any relevant context for the question.\"\n",
    "\n",
    "    # Format context for the prompt\n",
    "    contexts_for_prompt = []\n",
    "    for i, (text, score) in enumerate(zip(retrieved_texts, retrieved_scores)):\n",
    "         contexts_for_prompt.append(f\"<context index=\\\"{i}\\\" score=\\\"{score:.4f}\\\">\\n{text}\\n</context>\")\n",
    "\n",
    "    context_string = \"\\n\\n\".join(contexts_for_prompt)\n",
    "\n",
    "    prompt = RAG_PROMPT.format(contexts=context_string, question=query)\n",
    "\n",
    "    # print(\"\\n--- RAG Prompt ---\")\n",
    "    # print(textwrap.fill(prompt, width=120)) # Optional: print the prompt sent to LLM\n",
    "    # print(\"--- End RAG Prompt ---\")\n",
    "\n",
    "    answer = call_model(prompt)\n",
    "    print(\"--- LLM Answer ---\")\n",
    "    print(textwrap.fill(answer, width=120))\n",
    "    print(\"------\\n\")\n",
    "    return answer\n",
    "\n",
    "\n",
    "# --- Main Processing Pipeline --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Environment Variables ---\n",
    "load_dotenv()\n",
    "print(\"Environment variables loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentConverter configured.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Configure Docling ---\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    generate_page_images=False, # Don't need images for script processing\n",
    "    # images_scale=1.0, # Not needed if generate_page_images is False\n",
    "    do_ocr=True,\n",
    "    do_picture_description=True if PICTURE_DESCRIPTION_OPTIONS else False,\n",
    "    ocr_options=RapidOcrOptions(),\n",
    "    picture_description_options=PICTURE_DESCRIPTION_OPTIONS,\n",
    ")\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n",
    ")\n",
    "print(\"DocumentConverter configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: API_FR.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 18:02:45,314 - OrtInferSession - WARNING: DmlExecutionProvider is not in available providers (['AzureExecutionProvider', 'CPUExecutionProvider']). Use AzureExecutionProvider inference by default.\n",
      "2025-04-23 18:02:45,314 - OrtInferSession - INFO: If you want to use DirectML acceleration, you must do:\n",
      "2025-04-23 18:02:45,315 - OrtInferSession - INFO: First, uninstall all onnxruntime pakcages in current environment.\n",
      "2025-04-23 18:02:45,315 - OrtInferSession - INFO: Second, install onnxruntime-directml by `pip install onnxruntime-directml`\n",
      "2025-04-23 18:02:45,315 - OrtInferSession - INFO: Third, ensure DmlExecutionProvider is in available providers list. e.g. ['DmlExecutionProvider', 'CPUExecutionProvider']\n",
      "2025-04-23 18:02:45,369 - OrtInferSession - WARNING: DmlExecutionProvider is not in available providers (['AzureExecutionProvider', 'CPUExecutionProvider']). Use AzureExecutionProvider inference by default.\n",
      "2025-04-23 18:02:45,369 - OrtInferSession - INFO: If you want to use DirectML acceleration, you must do:\n",
      "2025-04-23 18:02:45,369 - OrtInferSession - INFO: First, uninstall all onnxruntime pakcages in current environment.\n",
      "2025-04-23 18:02:45,370 - OrtInferSession - INFO: Second, install onnxruntime-directml by `pip install onnxruntime-directml`\n",
      "2025-04-23 18:02:45,370 - OrtInferSession - INFO: Third, ensure DmlExecutionProvider is in available providers list. e.g. ['DmlExecutionProvider', 'CPUExecutionProvider']\n",
      "2025-04-23 18:02:45,397 - OrtInferSession - WARNING: DmlExecutionProvider is not in available providers (['AzureExecutionProvider', 'CPUExecutionProvider']). Use AzureExecutionProvider inference by default.\n",
      "2025-04-23 18:02:45,397 - OrtInferSession - INFO: If you want to use DirectML acceleration, you must do:\n",
      "2025-04-23 18:02:45,398 - OrtInferSession - INFO: First, uninstall all onnxruntime pakcages in current environment.\n",
      "2025-04-23 18:02:45,398 - OrtInferSession - INFO: Second, install onnxruntime-directml by `pip install onnxruntime-directml`\n",
      "2025-04-23 18:02:45,399 - OrtInferSession - INFO: Third, ensure DmlExecutionProvider is in available providers list. e.g. ['DmlExecutionProvider', 'CPUExecutionProvider']\n",
      "c:\\Users\\elbou\\miniconda3\\envs\\mlw\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\elbou\\.cache\\huggingface\\hub\\models--HuggingFaceTB--SmolVLM-256M-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF converted in 458.24 seconds.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Convert PDF to Docling Document ---\n",
    "if not PDF_PATH.is_file():\n",
    "    print(f\"Error: PDF file not found at {PDF_PATH}\")\n",
    "    # return\n",
    "\n",
    "print(f\"Processing PDF: {PDF_PATH}...\")\n",
    "convert_start = time.time()\n",
    "try:\n",
    "    result = converter.convert(PDF_PATH)\n",
    "    doc = result.document\n",
    "except Exception as e:\n",
    "    print(f\"Error during PDF conversion: {e}\")\n",
    "    # return\n",
    "print(f\"PDF converted in {time.time() - convert_start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rapidocr_onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 16 image annotations.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. Extract Image Annotations (if generated) ---\n",
    "image_annotations = []\n",
    "if pipeline_options.do_picture_description:\n",
    "    for picture in doc.pictures:\n",
    "        if picture.annotations:\n",
    "            # Take the first annotation if multiple exist\n",
    "            image_annotations.append(picture.annotations[0].text)\n",
    "        else:\n",
    "            # Add a default placeholder if no annotation was generated\n",
    "            image_annotations.append(\"Image detected, no description generated.\")\n",
    "    print(f\"Extracted {len(image_annotations)} image annotations.\")\n",
    "else:\n",
    "    print(\"Image description was disabled.\")\n",
    "    # Need to know how many images were potentially detected to replace placeholders\n",
    "    # This info might be in doc.pictures even if description is off.\n",
    "    num_image_placeholders = len(doc.pictures) # Assuming this count is still valid\n",
    "    image_annotations = [\"Image detected.\"] * num_image_placeholders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting document to Markdown...\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Export to Markdown with Placeholders ---\n",
    "print(\"Exporting document to Markdown...\")\n",
    "markdown_text = doc.export_to_markdown(\n",
    "    page_break_placeholder=PAGE_BREAK_PLACEHOLDER,\n",
    "    image_placeholder=IMAGE_PLACEHOLDER,\n",
    ")\n",
    "# Basic cleaning - remove potential extra newlines\n",
    "markdown_text = re.sub(r'\\n{3,}', '\\n\\n', markdown_text).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing image placeholders with annotations...\n",
      "Initial processed text word count: 8435\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Replace Image Placeholders with Annotations ---\n",
    "print(\"Replacing image placeholders with annotations...\")\n",
    "processed_text = replace_occurrences(markdown_text, IMAGE_PLACEHOLDER, image_annotations)\n",
    "# Optional: Write intermediate text to file\n",
    "# with open(OUTPUT_DIR / \"processed_text_intermediate.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(processed_text)\n",
    "\n",
    "word_count = len(processed_text.split())\n",
    "print(f\"Initial processed text word count: {word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting LLM-based Chunking ---\n",
      "Initially split into 56 chunks based on '\n",
      "## '.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. LLM-based Chunking ---\n",
    "print(\"\\n--- Starting LLM-based Chunking ---\")\n",
    "# 7a. Initial Split (using simple pattern)\n",
    "initial_split_chunks = re.split(f\"({INITIAL_SPLIT_PATTERN})\", processed_text)\n",
    "# The split includes the delimiter, need to recombine\n",
    "combined_initial_chunks = []\n",
    "if initial_split_chunks:\n",
    "    # Add the first part if it doesn't start with the delimiter\n",
    "    if not initial_split_chunks[0].strip().startswith(\"##\") and initial_split_chunks[0].strip():\n",
    "            combined_initial_chunks.append(initial_split_chunks[0].strip())\n",
    "    # Combine delimiter with the following text\n",
    "    for i in range(1, len(initial_split_chunks), 2):\n",
    "        if i + 1 < len(initial_split_chunks):\n",
    "                combined_initial_chunks.append(\n",
    "                    (initial_split_chunks[i] + initial_split_chunks[i+1]).strip()\n",
    "                )\n",
    "        else: # Handle potential last delimiter\n",
    "                combined_initial_chunks.append(initial_split_chunks[i].strip())\n",
    "\n",
    "print(f\"Initially split into {len(combined_initial_chunks)} chunks based on '{INITIAL_SPLIT_PATTERN}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking LLM for chunking suggestions...\n",
      "LLM chunking suggestion response: 'split_after: 41, 50, 53, 55'\n",
      "LLM suggested splitting after chunk indices: [41, 50, 53, 55]\n",
      "Split into 4 sections based on LLM suggestions.\n"
     ]
    }
   ],
   "source": [
    "# 7b. Add Start/End Tags for LLM\n",
    "tagged_text_parts = []\n",
    "for i, chunk in enumerate(combined_initial_chunks):\n",
    "    tagged_text_parts.append(f\"<|start_chunk_{i}|>\\n{chunk}\\n<|end_chunk_{i}|>\")\n",
    "tagged_text = \"\\n\\n\".join(tagged_text_parts)\n",
    "# Optional: Write tagged text\n",
    "# with open(OUTPUT_DIR / \"tagged_text_for_chunking.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "#      f.write(tagged_text)\n",
    "\n",
    "# 7c. Call LLM for Chunking Suggestions\n",
    "print(\"Asking LLM for chunking suggestions...\")\n",
    "chunking_prompt_filled = CHUNKING_PROMPT.format(document_text=tagged_text)\n",
    "llm_chunking_response = call_model(chunking_prompt_filled)\n",
    "print(f\"LLM chunking suggestion response: '{llm_chunking_response}'\")\n",
    "\n",
    "# 7d. Apply LLM Suggestions to Create Final Chunks\n",
    "llm_chunks = split_text_by_llm_suggestions(tagged_text, llm_chunking_response)\n",
    "\n",
    "if not llm_chunks:\n",
    "    print(\"Error: LLM-based chunking resulted in no chunks. Exiting.\")\n",
    "    # return\n",
    "\n",
    "# Optional: Print chunk examples\n",
    "# print(\"\\n--- Example LLM Chunks ---\")\n",
    "# for i, chunk in enumerate(llm_chunks[:2]):\n",
    "#     print(f\"--- Chunk {i} ---\")\n",
    "#     print(textwrap.fill(chunk, width=100))\n",
    "#     print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Contextual Enrichment ---\n",
      "Generating context for chunk 1/4...\n",
      "Generating context for chunk 2/4...\n",
      "Generating context for chunk 3/4...\n",
      "Generating context for chunk 4/4...\n",
      "Contextual enrichment finished in 58.48 seconds.\n",
      "Enriched chunks saved to output\\enriched_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Contextual Enrichment ---\n",
    "print(\"\\n--- Starting Contextual Enrichment ---\")\n",
    "enriched_chunks = []\n",
    "contexts_generated = []\n",
    "enrich_start = time.time()\n",
    "for i, chunk in enumerate(llm_chunks):\n",
    "    print(f\"Generating context for chunk {i+1}/{len(llm_chunks)}...\")\n",
    "    context_prompt = CONTEXTUALIZER_PROMPT.format(document=processed_text, chunk=chunk)\n",
    "    context = call_model(context_prompt)\n",
    "\n",
    "    if context.startswith(\"Error:\"):\n",
    "        print(f\"Warning: Failed to generate context for chunk {i}. Using original chunk only.\")\n",
    "        context = \"Context generation failed.\" # Placeholder context\n",
    "\n",
    "    contexts_generated.append(context)\n",
    "    enriched_chunks.append(f\"<chunk_context>\\n{context}\\n</chunk_context>\\n\\n<chunk>\\n{chunk}\\n</chunk>\")\n",
    "    # Optional: Add a small delay if hitting API rate limits\n",
    "    # time.sleep(0.5)\n",
    "\n",
    "print(f\"Contextual enrichment finished in {time.time() - enrich_start:.2f} seconds.\")\n",
    "\n",
    "# Optional: Print example enriched chunk\n",
    "# if enriched_chunks:\n",
    "#     print(\"\\n--- Example Enriched Chunk (Chunk 0) ---\")\n",
    "#     print(textwrap.fill(enriched_chunks[0], width=120))\n",
    "#     print(\"-\" * 20)\n",
    "\n",
    "# Save enriched chunks to file\n",
    "with open(OUTPUT_DIR / \"enriched_chunks.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, chunk in enumerate(enriched_chunks):\n",
    "        f.write(f\"--- Chunk {i} ---\\n\")\n",
    "        f.write(chunk)\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "print(f\"Enriched chunks saved to {OUTPUT_DIR / 'enriched_chunks.txt'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up RAG ---\n",
      "Initializing SimpleRetriever with 4 text chunks.\n",
      "TF-IDF vectors created successfully.\n",
      "\n",
      "--- Asking Questions via RAG ---\n",
      "\n",
      "--- Answering Question (k=3): How do you activate the public REST APIs in CCH Tagetik? Are they enabled by default? ---\n",
      "--- LLM Answer ---\n",
      "The public REST APIs are not enabled by default. You activate them by going to the administration and enabling them\n",
      "there.\n",
      "------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The public REST APIs are not enabled by default. You activate them by going to the administration and enabling them there.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 9. Setup RAG ---\n",
    "print(\"\\n--- Setting up RAG ---\")\n",
    "if not enriched_chunks:\n",
    "    print(\"Error: No enriched chunks available for RAG setup.\")\n",
    "    # return\n",
    "\n",
    "try:\n",
    "    retriever = SimpleRetriever(enriched_chunks)\n",
    "except ValueError as e:\n",
    "    print(f\"Error initializing retriever: {e}\")\n",
    "    # return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 10. Ask Questions using RAG ---\n",
    "print(\"\\n--- Asking Questions via RAG ---\")\n",
    "ask_question(\"How do you activate the public REST APIs in CCH Tagetik? Are they enabled by default?\", retriever, k=3)\n",
    "# ask_question(\"What is the gaming revenue for the fourth quarter?\", retriever, k=3)\n",
    "# ask_question(\"Summarize the financial highlights for Fiscal 2025.\", retriever, k=4)\n",
    "# ask_question(\"What are the key points about the Data Center business?\", retriever, k=3)\n",
    "\n",
    "# total_time = time.time() - start_time\n",
    "# print(f\"\\n--- Pipeline Finished ---\")\n",
    "# print(f\"Total execution time: {total_time:.2f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
