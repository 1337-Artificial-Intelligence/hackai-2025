{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34bd2363",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/train_pretrain_smollm_goudma.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb02762",
   "metadata": {},
   "source": [
    "# Training Your First Language Model üöÄ\n",
    "\n",
    "In this notebook, you'll learn how to train a small language model from scratch! We'll use the SmolLM2 model, which is perfect for learning because it's small but powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5c1ea",
   "metadata": {},
   "source": [
    "## What is Language Model Training? ü§î\n",
    "\n",
    "> A language model is like a student learning to read and write. It learns by:\n",
    "> 1. Reading lots of text\n",
    "> 2. Trying to predict the next word in a sentence\n",
    "> 3. Learning from its mistakes\n",
    "\n",
    "TODO: Add image showing how a language model predicts next words\n",
    "\n",
    "```\n",
    "Example:\n",
    "Input: \"The cat sat on the\"\n",
    "Model predicts: \"mat\" (or \"chair\", \"table\", etc.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccd5ea",
   "metadata": {},
   "source": [
    "## Let's Get Started! üõ†Ô∏è\n",
    "\n",
    "First, we need to install some tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U torch datasets transformers wandb -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef4aed8",
   "metadata": {},
   "source": [
    "## Check Your GPU üéÆ\n",
    "\n",
    "We need a GPU to train our model faster. Let's check if you have one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656973d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Model:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc7152",
   "metadata": {},
   "source": [
    "## Load Your Dataset üìö\n",
    "\n",
    "We'll use a dataset from Hugging Face. Think of it as a big book for our model to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3350486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name_id = \"atlasia/good25\"\n",
    "ds = load_dataset(dataset_name_id, split=\"train\")\n",
    "ds = ds.select_columns([\"content\"])  # We only need the text content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1496cb",
   "metadata": {},
   "source": [
    "## Prepare the Text üìù\n",
    "\n",
    "Before training, we need to:\n",
    "1. Split our text into small pieces (tokens)\n",
    "2. Make all pieces the same length\n",
    "\n",
    "TODO: Add image showing tokenization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252616c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = \"HuggingFaceTB/SmolLM2-138M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set how long each piece of text should be\n",
    "context_length = 128\n",
    "\n",
    "def tokenize(examples):\n",
    "    results = tokenizer(\n",
    "        examples[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True\n",
    "    )\n",
    "    input_batch = []\n",
    "    for l, in_ids in zip(results[\"length\"], results[\"input_ids\"]):\n",
    "        if l == context_length:\n",
    "            input_batch.append(in_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "# Split data into train and test\n",
    "ds_spliter = ds.train_test_split(test_size=0.2, seed=42)\n",
    "tokenized_ds = ds_spliter.map(tokenize, batched=True, remove_columns=ds_spliter[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48152c8b",
   "metadata": {},
   "source": [
    "## Set Up Training üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Now we'll:\n",
    "1. Create our model\n",
    "2. Set up how it should learn\n",
    "3. Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63830040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Create model\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# Set up training settings\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"test_dir\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    logging_steps=2,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "# Set up data preparation\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7cbb0",
   "metadata": {},
   "source": [
    "## Start Training! üöÄ\n",
    "\n",
    "This might take a while. While it's training, you can:\n",
    "1. Watch the loss go down (that's good!)\n",
    "2. Learn about what's happening in the background\n",
    "3. Think about how you could use this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88451d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e148b7d",
   "metadata": {},
   "source": [
    "## What Did We Learn? üìö\n",
    "\n",
    "In this notebook, you learned:\n",
    "1. How to prepare text for a language model\n",
    "2. How to set up and train a small language model\n",
    "3. How to monitor the training process\n",
    "\n",
    "TODO: Add image showing the training process and results\n",
    "\n",
    "## Next Steps üéØ\n",
    "\n",
    "Want to learn more? Check out:\n",
    "1. How to make your model follow instructions\n",
    "2. How to make your model smaller and faster\n",
    "3. How to use your trained model for cool projects"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
