{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c82b854",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/train_quantization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531aed29",
   "metadata": {},
   "source": [
    "# Quantization Challenge: Making AI Models Smaller and Faster\n",
    "\n",
    "In this notebook, you'll learn how to make AI models smaller and faster using quantization. This is super important for running AI on phones and other small devices!\n",
    "\n",
    "## What you'll learn:\n",
    "1. What quantization is and why it's useful\n",
    "2. How to make models smaller using different number formats\n",
    "3. How to use quantization in real AI models\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb2109a",
   "metadata": {},
   "source": [
    "## 1. What is Quantization?\n",
    "\n",
    "Quantization is like compressing a photo to make it smaller. Instead of using big numbers (32 bits), we use smaller numbers (8 bits or 4 bits) to store our AI model.\n",
    "\n",
    "### Why is this useful?\n",
    "- **Smaller models**: Takes less space on your phone\n",
    "- **Faster**: Runs quicker on your device\n",
    "- **Less power**: Uses less battery\n",
    "- **Works on more devices**: Can run on phones and small computers\n",
    "\n",
    "### Key Terms (Don't worry, we'll explain these as we go!):\n",
    "- **Bits**: The smallest unit of computer data (like a light switch: on/off)\n",
    "- **Float32**: The normal way computers store numbers (32 bits)\n",
    "- **Int8**: A smaller way to store numbers (8 bits)\n",
    "- **Scale**: A number that helps us convert between big and small numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53f3c4",
   "metadata": {},
   "source": [
    "## 2. How Computers Store Numbers\n",
    "\n",
    "Let's look at how computers store different types of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5708d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup for displaying figures\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d702c9",
   "metadata": {},
   "source": [
    "### 2.1 Integer Numbers\n",
    "\n",
    "Integers are whole numbers (like 1, 2, 3). They can be:\n",
    "- **Unsigned**: Only positive numbers (0 to 255 for 8 bits)\n",
    "- **Signed**: Both positive and negative numbers (-128 to 127 for 8 bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18401d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show different integer ranges\n",
    "int_types = {\n",
    "    'int8': (np.iinfo(np.int8).min, np.iinfo(np.int8).max, 8),\n",
    "    'uint8': (np.iinfo(np.uint8).min, np.iinfo(np.uint8).max, 8),\n",
    "    'int16': (np.iinfo(np.int16).min, np.iinfo(np.int16).max, 16),\n",
    "    'uint16': (np.iinfo(np.uint16).min, np.iinfo(np.uint16).max, 16)\n",
    "}\n",
    "\n",
    "for dtype, (min_val, max_val, bits) in int_types.items():\n",
    "    print(f\"{dtype}: Range [{min_val}, {max_val}], Bits: {bits}, Values: {2**bits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be66258",
   "metadata": {},
   "source": [
    "### 2.2 Floating-Point Numbers\n",
    "\n",
    "Floating-point numbers can store decimals (like 3.14). They use:\n",
    "- **Sign bit**: Is the number positive or negative?\n",
    "- **Exponent bits**: How big is the number?\n",
    "- **Mantissa bits**: What are the decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d25e85",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Show floating-point ranges\n",
    "float_types = {\n",
    "    'float16': (np.finfo(np.float16).min, np.finfo(np.float16).max, 16),\n",
    "    'float32': (np.finfo(np.float32).min, np.finfo(np.float32).max, 32)\n",
    "}\n",
    "\n",
    "for dtype, (min_val, max_val, bits) in float_types.items():\n",
    "    print(f\"{dtype}: Range [{min_val}, {max_val}], Bits: {bits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3611b2",
   "metadata": {},
   "source": [
    "### 2.3 Visual Comparison\n",
    "\n",
    "Let's see how different number formats look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78040f9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_number_line(dtype, num_points=1000):\n",
    "    if dtype == 'int8':\n",
    "        values = np.linspace(-128, 127, num_points)\n",
    "        representable = np.arange(-128, 128)\n",
    "    elif dtype == 'uint8':\n",
    "        values = np.linspace(0, 255, num_points)\n",
    "        representable = np.arange(0, 256)\n",
    "    elif dtype == 'float16':\n",
    "        values = np.linspace(-10, 10, num_points)\n",
    "        dense_near_zero = np.array([np.float16(x) for x in np.linspace(-0.1, 0.1, 40)])\n",
    "        medium_range = np.array([np.float16(x) for x in np.linspace(-1, 1, 30)])\n",
    "        sparse_range = np.array([np.float16(x) for x in np.linspace(-10, 10, 30)])\n",
    "        representable = np.concatenate([dense_near_zero, medium_range, sparse_range])\n",
    "        representable = np.unique(representable)\n",
    "\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    \n",
    "    if dtype == 'float16':\n",
    "        plt.scatter(representable, np.zeros_like(representable), color='blue', s=20,\n",
    "                   label=f'Representable {dtype} values')\n",
    "        point_density = np.ones_like(representable) * 0.1\n",
    "        plt.scatter(representable, point_density, color='red', s=5, alpha=0.5,\n",
    "                   label='Density visualization')\n",
    "    else:\n",
    "        plt.scatter(representable, np.zeros_like(representable), color='blue', s=20,\n",
    "                   label=f'Representable {dtype} values')\n",
    "\n",
    "    plt.xlim([min(values), max(values)])\n",
    "    plt.title(f'Distribution of representable {dtype} values')\n",
    "    plt.yticks([])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize different number formats\n",
    "visualize_number_line('int8')\n",
    "visualize_number_line('uint8')\n",
    "visualize_number_line('float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a283b08",
   "metadata": {},
   "source": [
    "Notice how:\n",
    "- Integers are evenly spaced (like steps on a ladder)\n",
    "- Floating-point numbers are closer together near zero (like a zoom lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb5be2",
   "metadata": {},
   "source": [
    "## 3. Types of Quantization\n",
    "\n",
    "There are two main ways to quantize:\n",
    "\n",
    "1. **Symmetric Quantization**: Like a mirror image around zero\n",
    "   - Simpler but less accurate\n",
    "   - Good for numbers that are balanced around zero\n",
    "\n",
    "2. **Affine Quantization**: Like a sliding scale\n",
    "   - More accurate but more complex\n",
    "   - Good for numbers that are mostly positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8b0c7",
   "metadata": {},
   "source": [
    "## 4. Let's Try Quantization!\n",
    "\n",
    "We'll create a simple quantizer that can make numbers smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450f1a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SymmetricQuantizer:\n",
    "    \"\"\"A simple quantizer that makes numbers smaller\"\"\"\n",
    "\n",
    "    def __init__(self, num_bits=8):\n",
    "        self.num_bits = num_bits\n",
    "        self.qmin = -(2**(num_bits-1) - 1)\n",
    "        self.qmax = 2**(num_bits-1) - 1\n",
    "        self.scale = None\n",
    "\n",
    "    def get_scale(self, x):\n",
    "        \"\"\"Find the right scale to make numbers fit\"\"\"\n",
    "        x_abs_max = torch.max(torch.abs(x))\n",
    "        scale = x_abs_max / self.qmax\n",
    "        scale = torch.max(scale, torch.tensor(1e-8))\n",
    "        return scale\n",
    "\n",
    "    def quantize(self, x):\n",
    "        \"\"\"Make numbers smaller\"\"\"\n",
    "        self.scale = self.get_scale(x)\n",
    "        x_q = torch.round(x / self.scale)\n",
    "        return torch.clamp(x_q, self.qmin, self.qmax)\n",
    "\n",
    "    def dequantize(self, x_q):\n",
    "        \"\"\"Make numbers bigger again\"\"\"\n",
    "        if self.scale is None:\n",
    "            raise ValueError(\"Scale is not set. Quantize first!\")\n",
    "        return x_q * self.scale\n",
    "\n",
    "    def quantize_dequantize(self, x):\n",
    "        \"\"\"Make numbers smaller and bigger again (to see how much we lost)\"\"\"\n",
    "        x_q = self.quantize(x)\n",
    "        x_dq = self.dequantize(x_q)\n",
    "        return x_dq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456901cb",
   "metadata": {},
   "source": [
    "Let's test our quantizer with different bit sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random numbers\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1000) * 5\n",
    "\n",
    "# Test different bit sizes\n",
    "bit_widths = [8, 4, 2]\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, bits in enumerate(bit_widths):\n",
    "    quantizer = SymmetricQuantizer(num_bits=bits)\n",
    "    x_dq = quantizer.quantize_dequantize(x)\n",
    "\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(x.numpy(), x_dq.numpy(), alpha=0.5, s=5)\n",
    "    plt.plot([-15, 15], [-15, 15], 'k--', alpha=0.5)  # perfect line\n",
    "\n",
    "    error = torch.abs(x - x_dq).mean().item()\n",
    "    plt.title(f'{bits}-bit Quantization\\nMAE: {error:.4f}')\n",
    "    plt.xlabel('Original Value')\n",
    "    plt.ylabel('Reconstructed Value')\n",
    "\n",
    "    print(f\"{bits}-bit: Scale = {quantizer.scale.item():.6f}, Mean Abs Error = {error:.6f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2a911",
   "metadata": {},
   "source": [
    "Notice how:\n",
    "- 8 bits: Almost perfect!\n",
    "- 4 bits: Some loss but still good\n",
    "- 2 bits: Not so good (only 4 values to work with!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e213ef0",
   "metadata": {},
   "source": [
    "## 5. Using Quantization in Real AI Models\n",
    "\n",
    "Let's see how to use quantization with a real AI model. We'll use a small language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb43d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip3 install accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Set up 8-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# Load a small model with quantization\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"unsloth/Llama-3.2-1B\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Try it out!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B\")\n",
    "input_text = \"What are we having for dinner?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = quantized_model.generate(**input_ids, max_new_tokens=10)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf84fb9",
   "metadata": {},
   "source": [
    "## 6. Challenge Time!\n",
    "\n",
    "Your challenge is to:\n",
    "1. Try different bit sizes (8, 4, 2) with our quantizer\n",
    "2. Compare the results and explain what you see\n",
    "3. Try quantizing a different model or dataset\n",
    "\n",
    "Remember:\n",
    "- More bits = better quality but bigger size\n",
    "- Fewer bits = smaller size but lower quality\n",
    "- Choose the right balance for your needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c05225",
   "metadata": {},
   "source": [
    "## 7. What's Next?\n",
    "\n",
    "- Try different quantization methods\n",
    "- Learn about quantization-aware training\n",
    "- Explore more advanced techniques\n",
    "\n",
    "Happy Hacking! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
