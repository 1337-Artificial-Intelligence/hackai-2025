{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d3a5e2",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/vlm_usage.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2612d5",
   "metadata": {},
   "source": [
    "## üèÜ Vision Language Models (VLMs)\n",
    "### üìå Description\n",
    "\n",
    "In this challenge, you'll explore Vision-Language Models (VLMs) - AI models that can understand both images and text! You'll learn how to:\n",
    "- Ask questions about images (Visual Question Answering)\n",
    "- Generate descriptions of images (Image Captioning)\n",
    "- Extract text from images (OCR)\n",
    "\n",
    "We'll use pre-trained models so you can focus on understanding how they work. Feel free to experiment with your own images and try prompts in **Darija** or **Arabic**!\n",
    "\n",
    "**Time**: ~45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aabad19",
   "metadata": {},
   "source": [
    "## üîß Setup\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc00f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q\n",
    "!pip install pyav yt-dlp qwen-vl-utils -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536d20e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import (\n",
    "    Qwen2VLForConditionalGeneration, \n",
    "    AutoProcessor,\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration\n",
    ")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Free up GPU memory after using each model\"\"\"\n",
    "    if \"inputs\" in globals(): del globals()[\"inputs\"]\n",
    "    if \"model\" in globals(): del globals()[\"model\"]\n",
    "    if \"processor\" in globals(): del globals()[\"processor\"]\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory cleared: {torch.cuda.memory_allocated() / 1024**3:.2f} GB used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d3d5ae",
   "metadata": {},
   "source": [
    "## 1. Visual Question Answering (VQA)\n",
    "VQA lets you ask questions about images and get answers in natural language.\n",
    "\n",
    "For example:\n",
    "- \"What food is shown in this image?\"\n",
    "- \"How many people are in the photo?\"\n",
    "- \"What color is the car?\"\n",
    "\n",
    "We'll use Qwen2-VL, a powerful multilingual VLM that can understand both English and Arabic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33760d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", \n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it with a Moroccan tajine image!\n",
    "url = \"https://legarconboucher.com/img/cms/Recette/tajine-maroc.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Try these questions in English or Darija:\n",
    "# - \"What food is this?\"\n",
    "# - \"What ingredients can you see?\"\n",
    "# - \"Is this a traditional Moroccan dish?\"\n",
    "text_query = \"What food is this?\"\n",
    "\n",
    "# Prepare the model input\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": text_query},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the model's response\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "output_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Model's response:\", output_text)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82579a",
   "metadata": {},
   "source": [
    "## 2. Image Captioning\n",
    "Image captioning generates a natural language description of an image. Unlike VQA, it doesn't need a specific question - it just describes what it sees!\n",
    "\n",
    "We'll use BLIP, a model specifically trained for image captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\", \n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89493bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it with a sample image\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# Generate a caption\n",
    "inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "out = model.generate(**inputs)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Image caption:\", caption)\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0647734a",
   "metadata": {},
   "source": [
    "## 3. Optical Character Recognition (OCR)\n",
    "OCR helps extract text from images. This is useful for:\n",
    "- Reading text from photos\n",
    "- Digitizing documents\n",
    "- Extracting information from receipts or ID cards\n",
    "\n",
    "We'll use Qwen2-VL-OCR, which is great at reading text in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb40221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\", \n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f45ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it with a sample invoice\n",
    "url = \"https://trulysmall.com/wp-content/uploads/2023/04/Simple-Invoice-Template.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Try these questions:\n",
    "# - \"What is the invoice number?\"\n",
    "# - \"What is the total amount?\"\n",
    "# - \"What is the date?\"\n",
    "text_query = \"What is the invoice number?\"\n",
    "\n",
    "# Get the model's response\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": text_query},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "output_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Model's response:\", output_text)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83700e0c",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "You've learned how to use Vision-Language Models for three important tasks:\n",
    "1. Visual Question Answering (VQA)\n",
    "2. Image Captioning\n",
    "3. Optical Character Recognition (OCR)\n",
    "\n",
    "### ü§î What's Next?\n",
    "- Try the models with your own images\n",
    "- Experiment with prompts in Darija or Arabic\n",
    "- Think about how these models could help solve real-world problems in Morocco\n",
    "\n",
    "Share your results and ideas with the mentors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bbe583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "clear_memory()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
