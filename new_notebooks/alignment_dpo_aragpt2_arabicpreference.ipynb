{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151dd850",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization (DPO) for Language Model Alignment\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/alignment_dpo_aragpt2_arabicpreference.ipynb)\n",
    "\n",
    "This notebook demonstrates how to align language models with human preferences using Direct Preference Optimization (DPO), a powerful technique that improves upon traditional Reinforcement Learning from Human Feedback (RLHF) methods.\n",
    "\n",
    "Estimated time needed: **1** hour\n",
    "\n",
    "## Lab Objective\n",
    "\n",
    "The goal of this lab is to give you practical experience with:\n",
    "- Preparing a dataset specially formatted for DPO,\n",
    "- Fine-tuning a model using the DPO method,\n",
    "- Evaluating how much the model's behavior improves after training\n",
    "\n",
    "\n",
    "### How DPO Works (Simple Explanation)\n",
    "\n",
    "- You show the model two answers for the same question: one that humans prefer (the **chosen** one) and one that's less preferred (the **rejected** one).\n",
    "- The model learns **directly** from this comparison by adjusting itself to favor the \"chosen\" answers over the \"rejected\" ones.\n",
    "- It does this **without** needing a complex reward model like in traditional reinforcement learning.\n",
    "\n",
    "Think of it like training a dog: you show it two actions (e.g., sit nicely vs jump on people) and **reward** it for the one you like better, over and over, until it consistently chooses the good one.\n",
    "\n",
    "\n",
    "## DPO vs PPO: What's the Difference?\n",
    "\n",
    "| Aspect | DPO (Direct Preference Optimization) | PPO (Proximal Policy Optimization) |\n",
    "|:------|:--------------------------------------|:----------------------------------|\n",
    "| How it works | Directly trains the model from comparisons (chosen vs rejected) | Needs a reward model first, then trains the model using rewards |\n",
    "| Complexity | Simpler (no reward model needed) | More complex (2 steps: train reward model + policy optimization) |\n",
    "| Stability | Very stable and efficient | Stable but more sensitive to hyperparameters |\n",
    "| Training Type | Preference-based fine-tuning | Reinforcement learning fine-tuning |\n",
    "\n",
    "![image](https://cdn.labellerr.com/1%201%201%20DPO/dpo-ppo-diagram.webp)\n",
    "\n",
    "**In short:**\n",
    "- **DPO** is **easier and faster** because it skips the \"build a reward model\" step.\n",
    "- **PPO** is a **full reinforcement learning method**, needing more setup but offering more flexibility when rewards are tricky.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7c43c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57ae9b11",
   "metadata": {},
   "source": [
    "![texte](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*adNPXsn8v1qXiy98.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756adae0",
   "metadata": {},
   "source": [
    "In the DPO’s paper, the authors apply the Bradley and Terry model, which is a preference model in the loss function. Through some algebraic wor, they demonstrate that the second step can be skipped because language models inherently act as reward models themselves. Surprisingly, once the second step is removed, the problem is significantly simplified to an optimization problem with a cross-entropy objective, as shown in Figure below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f16016",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*zE6I3BBUDMN9lfwV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c436ae",
   "metadata": {},
   "source": [
    "<img href=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/0*adNPXsn8v1qXiy98.png)\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16af66",
   "metadata": {},
   "source": [
    "#### Setup and Installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3d4a8",
   "metadata": {},
   "source": [
    "- Installing required libraries\n",
    "\n",
    "**Note**: These versions are specifically selected for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfe42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --q torch==2.3.1 trl==0.11.4 peft==0.14.0 pandas numpy==1.26.0 datasets==3.2.0 transformers==4.45.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5947f3",
   "metadata": {},
   "source": [
    "- Importing required libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    GenerationConfig\n",
    ")\n",
    "from trl import DPOConfig, DPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd301da4",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer Setup\n",
    "\n",
    "For this workshop, we'll use the OPT model, a decoder-only language model from Meta AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905d106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability and set device accordingly\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model selection:  We're using AraGPT2, an Arabic language model based on the GPT-2 architecture\n",
    "MODEL_NAME = \"aubmindlab/aragpt2-base\" # \"unsloth/Qwen2.5-1.5B\" \n",
    "\n",
    "# The model name for the fine-tuned version\n",
    "FINETUNED_MODEL_NAME = \"aragpt2-base-dpo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597bc96",
   "metadata": {},
   "source": [
    "- Set the Hugging face token found [here](https://huggingface.co/settings/tokens)\n",
    "In order to interact and use the hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hugging Face token for accessing models \n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_HF_API_TOKEN\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fafe1",
   "metadata": {},
   "source": [
    "- Get your wandb API Key found [here](https://wandb.ai/authorize) and set it as an environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201aea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"YOUR_API_TOKEN\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f152eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for training\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Load reference model (used during training)\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Load and configure tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Configure padding token and padding side\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Padding on the right side preserves the beginning of sequences\n",
    "\n",
    "# Disable cache during forward pass to save memory\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bf1e6",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6262ff",
   "metadata": {},
   "source": [
    "-  Load the Arabic preference [dataset](https://huggingface.co/datasets/FreedomIntelligence/Arabic-preference-data-RLHF) for RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debe8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset contains pairs of responses where one is preferred over the other\n",
    "# We use only 10% for this demo to keep training time reasonable\n",
    "print(\"Loading preference dataset...\")\n",
    "ds = load_dataset(\"FreedomIntelligence/Arabic-preference-data-RLHF\", split=\"train[:10%]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503566f",
   "metadata": {},
   "source": [
    "Examine the dataset structure to understand its format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf1dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample features: dict_keys(['id', 'instruction', 'chosen', 'rejected'])\n",
      "\n",
      "Sample entry:\n",
      "{'id': '10001_q', 'instruction': 'هل يمكنك تحقيق ارباح من تطبيق او لعبة فقط من خلال الاعلان داخل التطبيق؟', 'chosen': 'نعم، يمكن تحقيق أرباح من تطبيق أو لعبة من خلال الإعلانات داخل التطبيق. هذا يتم عن طريق استخدام شبكات الإعلانات مثل Google AdMob أو Facebook Audience Network، حيث تُظهر الإعلانات في التطبيق أو اللعبة ويتم تحقيق العائد بناءً على عدد الأشخاص الذين ينقرون على تلك الإعلانات. العائد يمكن أن يتراوح من صفر إلى عشرات الألاف من الدولارات بناءً على شعبية التطبيق أو اللعبة وكيفية تفاعل المستخدمين مع الإعلانات.', 'rejected': 'نعم، يمكن تحقيق أرباح من تطبيق أو لعبة من خلال الإعلانات الموجودة داخل التطبيق. يعتمد هذا على مجموعة من العوامل بما في ذلك شعبية التطبيق وعدد المستخدمين، ونوع ومحتوى الإعلانات، والاستراتيجيات التسويقية المستخدمة. يمكن للشركات أن تحقق أرباح أيضًا من الإعلانات التابعة أو الإعلانات المدفوعة المستندة إلى النقر أو الإعلانات التي تظهر عند توقف المستخدمين عن استخدام التطبيق.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset sample features:\", ds[0].keys())\n",
    "print(\"\\nExample entry from dataset:\")\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4ce61",
   "metadata": {},
   "source": [
    "- Transform the dataset into the format required by DPO:\n",
    "    - `prompt`: The input query\n",
    "    - `chosen`: The preferred response\n",
    "    - `rejected`: The less preferred response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPreparing dataset for DPO training...\")\n",
    "ds = ds.rename_column(\"instruction\", \"prompt\").remove_columns([\"id\"])\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "# We use a 90/10 split with a fixed random seed for reproducibility\n",
    "ds = ds.train_test_split(0.1, shuffle=True, seed=42)\n",
    "train_dataset, eval_dataset = ds[\"train\"], ds[\"test\"]\n",
    "print(f\"Training set size: {len(train_dataset)}, Evaluation set size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ea01e",
   "metadata": {},
   "source": [
    "| Chosen | Rejected | Prompt |\n",
    "| --- | --- | --- |\n",
    " | نعم، يمكن تحقيق أرباح من تطبيق أو لعبة من خلال الإعلانات داخل التطبيق. هذا يتم عن طريق استخدام شبكات الإعلانات مثل Google AdMob أو Facebook Audience Network، حيث تُظهر الإعلانات في التطبيق أو اللعبة ويتم تحقيق العائد بناءً على عدد الأشخاص الذين ينقرون على تلك الإعلانات. العائد يمكن أن يتراوح من صفر إلى عشرات الألاف من الدولارات بناءً على شعبية التطبيق أو اللعبة وكيفية تفاعل المستخدمين مع الإعلانات.|نعم، يمكن تحقيق أرباح من تطبيق أو لعبة من خلال الإعلانات الموجودة داخل التطبيق. يعتمد هذا على مجموعة من العوامل بما في ذلك شعبية التطبيق وعدد المستخدمين، ونوع ومحتوى الإعلانات، والاستراتيجيات التسويقية المستخدمة. يمكن للشركات أن تحقق أرباح أيضًا من الإعلانات التابعة أو الإعلانات المدفوعة المستندة إلى النقر أو الإعلانات التي تظهر عند توقف المستخدمين عن استخدام التطبيق.| هل يمكنك تحقيق ارباح من تطبيق او لعبة فقط من خلال الاعلان داخل التطبيق\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fb29e",
   "metadata": {},
   "source": [
    "### Optional: Quantized Model Configuration (for GPUs)\n",
    "For r environments with GPU support, you can use quantization to reduce memory usage: Uncomment the following blocks if working with limited GPU memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e5745",
   "metadata": {},
   "source": [
    "![lora](https://pytorch.org/torchtune/0.4/_images/lora_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes # this package is required for quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d18d2a",
   "metadata": {},
   "source": [
    "**_Note:_**  _You can run the installed package by restarting a Kernel._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes  # Required for quantization\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# # Configure quantization parameters\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,                    # Load model in 4-bit precision instead of 16/32-bit\n",
    "#     bnb_4bit_use_double_quant=True,       # Use double quantization for better accuracy\n",
    "#     bnb_4bit_quant_type=\"nf4\",            # Use normalized float 4-bit quantization\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for calculations\n",
    "# )\n",
    "\n",
    "# # Load models with quantization config\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME, \n",
    "#     quantization_config=quantization_config,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# model_ref = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME, \n",
    "#     quantization_config=quantization_config,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "# model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb0a23",
   "metadata": {},
   "source": [
    "#### LoRA Configuration for Efficient Fine-tuning\n",
    "#LoRA allows us to train only a small number of adapter parameters instead of the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56115e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT (Parameter-Efficient Finetuning) configuration\n",
    "print(\"Setting up LoRA configuration...\")\n",
    "peft_config = LoraConfig(\n",
    "    r=4,                    # Rank of the low-rank decomposition matrices\n",
    "    target_modules=[        # Which modules to apply LoRA to\n",
    "        'c_proj',           # Projection layers in the transformer\n",
    "        'c_attn'            # Attention layers in the transformer\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",  # The type of task we're performing\n",
    "    lora_alpha=8,           # Scaling factor for the LoRA parameters (typically 2x rank)\n",
    "    lora_dropout=0.1,       # Dropout probability for LoRA layers\n",
    "    bias=\"none\",           # Whether to train bias parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56766ec2",
   "metadata": {},
   "source": [
    "####  DPO Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9ae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure DPO training parameters\n",
    "print(\"Setting up DPO training configuration...\")\n",
    "training_args = DPOConfig(\n",
    "    beta=0.1,                      # Temperature parameter for the DPO loss (typically 0.1-0.5)\n",
    "                                   # Higher values make the model more conservative\n",
    "    output_dir=\"dpo\",              # Directory to save model checkpoints\n",
    "    num_train_epochs=5,            # Number of training passes through the data\n",
    "    per_device_train_batch_size=2, # Batch size for training (adjust based on GPU memory)\n",
    "    per_device_eval_batch_size=2,  # Batch size for evaluation\n",
    "    remove_unused_columns=False,   # Keep all columns in the dataset\n",
    "    logging_steps=10,              # Log training progress every 10 steps\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients over multiple batches\n",
    "                                   # Effectively increases batch size to 2 * 4 = 8\n",
    "    learning_rate=1e-4,            # Learning rate for the optimizer\n",
    "    evaluation_strategy=\"epoch\",   # Evaluate after each epoch\n",
    "    warmup_steps=2,                # Number of warmup steps for learning rate scheduler\n",
    "    save_steps=500,                # Save checkpoint every 500 steps\n",
    "    report_to='wandb'              # Report training metrics to Weights & Biases\n",
    "                                   # Use 'none' to disable reporting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a74d8",
   "metadata": {},
   "source": [
    "####  DPO Trainer Setup\n",
    "\n",
    "Next step is creating the actual trainer using DPOTrainer class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c80458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/trl/trainer/dpo_trainer.py:655: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/trl/trainer/dpo_trainer.py:673: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Tokenizing train dataset: 100%|██████████| 6236/6236 [00:11<00:00, 534.04 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 693/693 [00:01<00:00, 602.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the DPO trainer that will handle the training process\n",
    "print(\"Setting up DPO trainer...\")\n",
    "trainer = DPOTrainer(\n",
    "    model=model,              # The model to be fine-tuned\n",
    "    ref_model=None,           # Reference model (None because we're using LoRA)\n",
    "                              # When using LoRA, DPOTrainer will automatically handle the reference model\n",
    "    args=training_args,       # Training arguments defined above\n",
    "    train_dataset=train_dataset,  # Training data\n",
    "    eval_dataset=eval_dataset,    # Evaluation data\n",
    "    tokenizer=tokenizer,          # Tokenizer\n",
    "    peft_config=peft_config,      # LoRA configuration\n",
    "    max_length=512,               # Maximum sequence length for inputs and outputs\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37ff85",
   "metadata": {},
   "source": [
    "Please note that when using LoRA for the base model, it's efficient to leave the model_ref param null, in which case the DPOTrainer will unload the adapter for reference inference.\n",
    "\n",
    "\n",
    "Now, you're all set for training the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115bc8c",
   "metadata": {},
   "source": [
    "#### Training Process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e7bf3b",
   "metadata": {},
   "source": [
    "**Training can be time-consuming on CPU and may cause memory issues, If you encounter problems, skip to the next section to load a pre-trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff5262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mafaf\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/afafelwafi/HackAI/wandb/run-20250502_195410-45waqojn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/afaf/huggingface/runs/45waqojn' target=\"_blank\">dpo</a></strong> to <a href='https://wandb.ai/afaf/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/afaf/huggingface' target=\"_blank\">https://wandb.ai/afaf/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/afaf/huggingface/runs/45waqojn' target=\"_blank\">https://wandb.ai/afaf/huggingface/runs/45waqojn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3895' max='3895' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3895/3895 8:09:01, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.351199</td>\n",
       "      <td>7.322288</td>\n",
       "      <td>4.321124</td>\n",
       "      <td>0.860231</td>\n",
       "      <td>3.001164</td>\n",
       "      <td>-783.083191</td>\n",
       "      <td>-1090.636475</td>\n",
       "      <td>-3.003296</td>\n",
       "      <td>-3.291013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>0.357454</td>\n",
       "      <td>8.366143</td>\n",
       "      <td>4.700235</td>\n",
       "      <td>0.864553</td>\n",
       "      <td>3.665909</td>\n",
       "      <td>-779.292175</td>\n",
       "      <td>-1080.197876</td>\n",
       "      <td>-3.019666</td>\n",
       "      <td>-3.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.172100</td>\n",
       "      <td>0.377715</td>\n",
       "      <td>8.918534</td>\n",
       "      <td>4.731424</td>\n",
       "      <td>0.861671</td>\n",
       "      <td>4.187110</td>\n",
       "      <td>-778.980347</td>\n",
       "      <td>-1074.674072</td>\n",
       "      <td>-3.005437</td>\n",
       "      <td>-3.298560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3895, training_loss=0.2658155122525579, metrics={'train_runtime': 29344.701, 'train_samples_per_second': 1.063, 'train_steps_per_second': 0.133, 'total_flos': 0.0, 'train_loss': 0.2658155122525579, 'epoch': 4.996792815907633})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start the training process\n",
    "print(\"Starting DPO training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac63cde",
   "metadata": {},
   "source": [
    "!!!!You can skip the training !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to Hugging Face Hub\n",
    "# print(\"Pushing model to Hugging Face Hub...\")\n",
    "# trainer.push_to_hub(FINETUNED_MODEL_NAME, commit_message=\"DPO finetuning with LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from the local checkpoint\n",
    "# print(\"Loading trained model from checkpoint...\")\n",
    "# dpo_model = AutoModelForCausalLM.from_pretrained('./dpo/checkpoint-3895').to(device)\n",
    "# dpo_tokenizer = AutoTokenizer.from_pretrained('./dpo/checkpoint-3895')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640afd5",
   "metadata": {},
   "source": [
    "#### Loading Pre-trained Model (Alternative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f5a1f",
   "metadata": {},
   "source": [
    "If training is too resource-intensive, you can load a pre-trained model\n",
    "\n",
    "This section loads a model that's already been fine-tuned with DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc96fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained DPO model from Hub...\n"
     ]
    }
   ],
   "source": [
    "# Load the DPO-fine-tuned model from Hugging Face Hub\n",
    "print(\"Loading pre-trained DPO model from Hub...\")\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(f\"HackAI-2025/{FINETUNED_MODEL_NAME}\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"HackAI-2025/{FINETUNED_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference (baseline) model for comparison\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995e198",
   "metadata": {},
   "source": [
    "####  Text Generation and Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfe8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up text generation configuration...\n",
      "Generating response with DPO model...\n",
      "DPO response:\t كيف يمكنني التغلب على القلق والتوتر؟ القلق والاكتئاب والقلق هو أحد الاضطرابات النفسية التي تصيب الأشخاص الذين يعانون من التوتر ، ويمكن أن يكون الخوف والقلق هو أكثر ما يميز الشخص المصاب بالقلق عن الآخرين المصابين به .1 - الاكتئاب : وهو حالة نفسية تؤثر بشكل كبير في التفكير والتحليل والتصرف وفي الأحاسيس والمشاعر السلبية مثل الحزن والغضب والخوف والإحباط والقلق والخوف أو الغضب والحزن والخوف والقلق والخوف والإحباط وغيرها من المشاعر المختلفة .2\n",
      "\n",
      "Generating response with baseline model...\n",
      "Baseline response:\t كيف يمكنني التغلب على القلق والتوتر؟ ؟ ! ! ، من هو الشخص الذي يملك هذا الإحساس الرائع . . . هل هو شخص عادي أم مجنون ؟ . . . أم أنه إنسان فاشل ؟ . . . لا أدري . . لكن ما أعرفه أن كل منا يعاني من مشكلة . . . أما أنا فأعتقد أن الإنسان الناجح في حياته هو الإنسان الذي يمتلك هذا الشعور . . . فهو الذي يستطيع\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducible generation\n",
    "set_seed(40)\n",
    "\n",
    "# Configure generation parameters\n",
    "print(\"Setting up text generation configuration...\")\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=70,         # Maximum number of tokens to generate\n",
    "    do_sample=True,            # Use sampling instead of greedy decoding\n",
    "    top_k=50,                  # Consider top 50 tokens at each step\n",
    "    top_p=0.8,                 # Consider tokens with cumulative probability of 0.8\n",
    "    temperature=0.8,           # Controls randomness (higher = more random)\n",
    "    repetition_penalty=1.2,    # Penalize repetition of tokens\n",
    "    pad_token_id=tokenizer.eos_token_id  # Use EOS token for padding\n",
    ")\n",
    "\n",
    "# Define a test prompt in Arabic\n",
    "PROMPT = \"كيف يمكنني التغلب على القلق والتوتر؟\" # \"What are the benefits of healthy food?\"\n",
    "\n",
    "# Tokenize the prompt and move to the appropriate device\n",
    "inputs = tokenizer(PROMPT, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate text with the DPO-fine-tuned model\n",
    "print(\"Generating response with DPO model...\")\n",
    "outputs = dpo_model.generate(**inputs, generation_config=generation_config).to(device)\n",
    "print(\"DPO response:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Generate text with the baseline model for comparison\n",
    "print(\"\\nGenerating response with baseline model...\")\n",
    "outputs = model_ref.generate(**inputs, generation_config=generation_config).to(device)\n",
    "print(\"Baseline response:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda11b2f",
   "metadata": {},
   "source": [
    "Althought the model is trained on a small data for 5 epochs only, it can be seen that the response generated by the DPO-tuned model is more concise and straightforward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97cba2",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\"ما هي فوائد الغذاء الصحي؟\",\n",
    "\"كيف يمكنني التغلب على القلق والتوتر؟\",\n",
    "\"اشرح لي كيفية استخدام الذكاء الاصطناعي في التعليم.\",\n",
    "\"ما هي أفضل طريقة لتعلم لغة جديدة؟\",\n",
    "\"هل يجب علي الاستثمار في العملات المشفرة؟\",\n",
    "\"ما هي أخطر تهديدات البيئة في العالم اليوم؟\",\n",
    "\"كيف يمكنني تحسين مهارات التواصل لدي؟\",\n",
    "\"اقترح برنامجاً لتمارين رياضية لشخص مبتدئ.\",\n",
    "\"ما هي الخطوات اللازمة لبدء مشروع تجاري ناجح؟\",\n",
    "\"كيف يمكن للتكنولوجيا أن تساعد في حل مشكلة تغير المناخ؟\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557d38e",
   "metadata": {},
   "source": [
    "## Exercise 1: Experiment with Generation Parameters\n",
    "Try different generation parameters (temperature, top_p, top_k) and compare their effects on:\n",
    "1. The quality of the generated text\n",
    "2. The diversity of responses\n",
    "3. How closely they align with human preferences\n",
    "\n",
    "## Exercise 2: Test with Different Prompts\n",
    "Create 3-5 different prompts and compare the responses from:\n",
    "1. The base model (model_ref)\n",
    "2. The DPO fine-tuned model\n",
    "Analyze the differences and explain how the DPO training has affected the outputs.\n",
    "\n",
    "## Exercise 3: Error Analysis\n",
    "Identify cases where the DPO model still produces suboptimal responses and suggest:\n",
    "1. Possible reasons for these failures\n",
    "2. How you might improve the training data to address these issues\n",
    "3. Alternative training strategies that might help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e834e",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to better understand DPO:\n",
    "\n",
    "1. **Experiment with Generation Parameters**\n",
    "   - Try different values for temperature, top_p, and top_k\n",
    "   - How do they affect the responses?\n",
    "\n",
    "2. **Test Different Prompts**\n",
    "   - Try these Arabic prompts:\n",
    "   ```python\n",
    "   test_questions = [\n",
    "       \"ما هي فوائد الغذاء الصحي؟\",\n",
    "       \"كيف يمكنني التغلب على القلق والتوتر؟\",\n",
    "       \"اشرح لي كيفية استخدام الذكاء الاصطناعي في التعليم.\",\n",
    "       \"ما هي أفضل طريقة لتعلم لغة جديدة؟\",\n",
    "       \"هل يجب علي الاستثمار في العملات المشفرة؟\"\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. **Compare Responses**\n",
    "   - How do the DPO model's responses differ from the baseline?\n",
    "   - What makes the DPO responses better or worse?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackai",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
