{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60adc03b",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/alignment_dpo_aragpt2_arabicpreference.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e83c2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Direct Preference Optimization (DPO) Using Hugging Face**\n",
    "\n",
    "Estimated time needed: **1** hour\n",
    "\n",
    "## What is DPO?\n",
    "\n",
    "Direct Preference Optimization (DPO) is a technique that helps make AI language models better at giving helpful and safe responses. It's like teaching a student by showing them examples of good and bad answers.\n",
    "\n",
    "### How DPO Works (Simple Explanation)\n",
    "\n",
    "1. We show the model two answers for the same question:\n",
    "   - A good answer (chosen by humans)\n",
    "   - A less good answer (rejected by humans)\n",
    "2. The model learns to prefer the good answers over time\n",
    "3. No complex reward system needed - it learns directly from examples!\n",
    "\n",
    "Think of it like training a dog:\n",
    "- Show it two actions (sit nicely vs jump on people)\n",
    "- Reward it for the good action\n",
    "- Repeat until it consistently chooses the good action\n",
    "\n",
    "## DPO vs Traditional Methods\n",
    "\n",
    "| Method | How it Works | Complexity |\n",
    "|:------|:------------|:-----------|\n",
    "| DPO | Learns directly from good/bad examples | Simple |\n",
    "| Traditional RLHF | Needs a separate reward model first | Complex |\n",
    "\n",
    "![image](https://cdn.labellerr.com/1%201%201%20DPO/dpo-ppo-diagram.webp)\n",
    "\n",
    "## Lab Objective\n",
    "\n",
    "In this lab, you will:\n",
    "1. Prepare a dataset of good and bad answers\n",
    "2. Fine-tune an Arabic language model using DPO\n",
    "3. See how the model improves after training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c8f15",
   "metadata": {},
   "source": [
    "### Setup and Installation\n",
    "\n",
    "First, let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --q torch==2.3.1 trl==0.11.4 peft==0.14.0 pandas numpy==1.26.0 datasets==3.2.0 transformers==4.45.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b3824",
   "metadata": {},
   "source": [
    "Now, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf48d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    GenerationConfig\n",
    ")\n",
    "from trl import DPOConfig, DPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7713b",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "We'll use AraGPT2, an Arabic language model based on GPT-2. This model is smaller and faster to train, perfect for our 1-hour lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model selection\n",
    "MODEL_NAME = \"aubmindlab/aragpt2-base\"\n",
    "FINETUNED_MODEL_NAME = \"aragpt2-base-dpo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2234879d",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We'll use a dataset of Arabic text pairs, where each pair contains:\n",
    "- A question\n",
    "- A good answer (chosen by humans)\n",
    "- A less good answer (rejected by humans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (we use only 10% to keep training time reasonable)\n",
    "print(\"Loading preference dataset...\")\n",
    "ds = load_dataset(\"FreedomIntelligence/Arabic-preference-data-RLHF\", split=\"train[:10%]\")\n",
    "\n",
    "# Look at an example\n",
    "print(\"\\nExample from dataset:\")\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612fc5a",
   "metadata": {},
   "source": [
    "### Prepare Data for Training\n",
    "\n",
    "We need to format our data for DPO training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ef0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for DPO\n",
    "print(\"\\nPreparing dataset for DPO training...\")\n",
    "ds = ds.rename_column(\"instruction\", \"prompt\").remove_columns([\"id\"])\n",
    "\n",
    "# Split into train and test sets\n",
    "ds = ds.train_test_split(0.1, shuffle=True, seed=42)\n",
    "train_dataset, eval_dataset = ds[\"train\"], ds[\"test\"]\n",
    "print(f\"Training set size: {len(train_dataset)}, Evaluation set size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882e8da",
   "metadata": {},
   "source": [
    "### Training Setup\n",
    "\n",
    "We'll use LoRA (Low-Rank Adaptation) to make training faster and more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=4,                    # Rank of the low-rank decomposition\n",
    "    target_modules=[        # Which parts of the model to train\n",
    "        'c_proj',           # Projection layers\n",
    "        'c_attn'            # Attention layers\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",  # Type of task\n",
    "    lora_alpha=8,           # Scaling factor\n",
    "    lora_dropout=0.1,       # Dropout for regularization\n",
    "    bias=\"none\",           # Don't train bias parameters\n",
    ")\n",
    "\n",
    "# DPO training configuration\n",
    "training_args = DPOConfig(\n",
    "    beta=0.1,                      # How strongly to prefer good answers\n",
    "    output_dir=\"dpo\",              # Where to save the model\n",
    "    num_train_epochs=5,            # Number of training passes\n",
    "    per_device_train_batch_size=2, # Batch size for training\n",
    "    per_device_eval_batch_size=2,  # Batch size for evaluation\n",
    "    remove_unused_columns=False,   # Keep all columns\n",
    "    logging_steps=10,              # Log progress every 10 steps\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients\n",
    "    learning_rate=1e-4,            # Learning rate\n",
    "    evaluation_strategy=\"epoch\",   # Evaluate after each epoch\n",
    "    warmup_steps=2,                # Warmup steps\n",
    "    save_steps=500,                # Save checkpoint every 500 steps\n",
    "    report_to='none'              # Don't report to external services\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6837186e",
   "metadata": {},
   "source": [
    "### Training Process\n",
    "\n",
    "**Note**: Training can take a while. You can skip to the next section to use a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "print(\"Setting up DPO trainer...\")\n",
    "trainer = DPOTrainer(\n",
    "    model=model,              # Model to train\n",
    "    ref_model=None,           # Reference model (handled automatically with LoRA)\n",
    "    args=training_args,       # Training arguments\n",
    "    train_dataset=train_dataset,  # Training data\n",
    "    eval_dataset=eval_dataset,    # Evaluation data\n",
    "    tokenizer=tokenizer,          # Tokenizer\n",
    "    peft_config=peft_config,      # LoRA configuration\n",
    "    max_length=512,               # Maximum sequence length\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting DPO training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c1e12",
   "metadata": {},
   "source": [
    "### Using a Pre-trained Model\n",
    "\n",
    "If you skipped training, you can load a pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d448ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "print(\"Loading pre-trained DPO model...\")\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(f\"HackAI-2025/{FINETUNED_MODEL_NAME}\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"HackAI-2025/{FINETUNED_MODEL_NAME}\")\n",
    "\n",
    "# Load baseline model for comparison\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dddc4c",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "\n",
    "Let's see how our model performs compared to the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798222f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up generation parameters\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=70,         # Maximum length of response\n",
    "    do_sample=True,            # Use sampling\n",
    "    top_k=50,                  # Consider top 50 tokens\n",
    "    top_p=0.8,                 # Consider tokens with 80% probability mass\n",
    "    temperature=0.8,           # Control randomness\n",
    "    repetition_penalty=1.2,    # Avoid repetition\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Test prompt\n",
    "PROMPT = \"كيف يمكنني التغلب على القلق والتوتر؟\"\n",
    "\n",
    "# Generate responses\n",
    "inputs = tokenizer(PROMPT, return_tensors='pt').to(device)\n",
    "\n",
    "print(\"Generating response with DPO model...\")\n",
    "outputs = dpo_model.generate(**inputs, generation_config=generation_config)\n",
    "print(\"DPO response:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nGenerating response with baseline model...\")\n",
    "outputs = model_ref.generate(**inputs, generation_config=generation_config)\n",
    "print(\"Baseline response:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e834e",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to better understand DPO:\n",
    "\n",
    "1. **Experiment with Generation Parameters**\n",
    "   - Try different values for temperature, top_p, and top_k\n",
    "   - How do they affect the responses?\n",
    "\n",
    "2. **Test Different Prompts**\n",
    "   - Try these Arabic prompts:\n",
    "   ```python\n",
    "   test_questions = [\n",
    "       \"ما هي فوائد الغذاء الصحي؟\",\n",
    "       \"كيف يمكنني التغلب على القلق والتوتر؟\",\n",
    "       \"اشرح لي كيفية استخدام الذكاء الاصطناعي في التعليم.\",\n",
    "       \"ما هي أفضل طريقة لتعلم لغة جديدة؟\",\n",
    "       \"هل يجب علي الاستثمار في العملات المشفرة؟\"\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. **Compare Responses**\n",
    "   - How do the DPO model's responses differ from the baseline?\n",
    "   - What makes the DPO responses better or worse?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackai",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
