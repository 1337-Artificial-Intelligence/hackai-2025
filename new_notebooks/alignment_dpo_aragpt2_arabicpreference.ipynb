{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151dd850",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization (DPO) for Language Model Alignment\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/alignment_dpo_aragpt2_arabicpreference.ipynb)\n",
    "\n",
    "This notebook demonstrates how to align language models with human preferences using Direct Preference Optimization (DPO), a powerful technique that improves upon traditional Reinforcement Learning from Human Feedback (RLHF) methods.\n",
    "\n",
    "Estimated time needed: **1** hour\n",
    "\n",
    "## Lab Objective\n",
    "\n",
    "The goal of this lab is to give you practical experience with:\n",
    "- Preparing a dataset specially formatted for DPO,\n",
    "- Fine-tuning a model using the DPO method,\n",
    "- Evaluating how much the model's behavior improves after training\n",
    "\n",
    "\n",
    "### How DPO Works (Simple Explanation)\n",
    "\n",
    "- You show the model two answers for the same question: one that humans prefer (the **chosen** one) and one that's less preferred (the **rejected** one).\n",
    "- The model learns **directly** from this comparison by adjusting itself to favor the \"chosen\" answers over the \"rejected\" ones.\n",
    "- It does this **without** needing a complex reward model like in traditional reinforcement learning.\n",
    "\n",
    "Think of it like training a dog: you show it two actions (e.g., sit nicely vs jump on people) and **reward** it for the one you like better, over and over, until it consistently chooses the good one.\n",
    "\n",
    "\n",
    "## DPO vs PPO: What's the Difference?\n",
    "\n",
    "| Aspect | DPO (Direct Preference Optimization) | PPO (Proximal Policy Optimization) |\n",
    "|:------|:--------------------------------------|:----------------------------------|\n",
    "| How it works | Directly trains the model from comparisons (chosen vs rejected) | Needs a reward model first, then trains the model using rewards |\n",
    "| Complexity | Simpler (no reward model needed) | More complex (2 steps: train reward model + policy optimization) |\n",
    "| Stability | Very stable and efficient | Stable but more sensitive to hyperparameters |\n",
    "| Training Type | Preference-based fine-tuning | Reinforcement learning fine-tuning |\n",
    "\n",
    "![image](https://cdn.labellerr.com/1%201%201%20DPO/dpo-ppo-diagram.webp)\n",
    "\n",
    "**In short:**\n",
    "- **DPO** is **easier and faster** because it skips the \"build a reward model\" step.\n",
    "- **PPO** is a **full reinforcement learning method**, needing more setup but offering more flexibility when rewards are tricky.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7c43c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57ae9b11",
   "metadata": {},
   "source": [
    "![texte](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*adNPXsn8v1qXiy98.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756adae0",
   "metadata": {},
   "source": [
    "In the DPOâ€™s paper, the authors apply the Bradley and Terry model, which is a preference model in the loss function. Through some algebraic wor, they demonstrate that the second step can be skipped because language models inherently act as reward models themselves. Surprisingly, once the second step is removed, the problem is significantly simplified to an optimization problem with a cross-entropy objective, as shown in Figure below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f16016",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*zE6I3BBUDMN9lfwV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c436ae",
   "metadata": {},
   "source": [
    "<img href=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/0*adNPXsn8v1qXiy98.png)\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16af66",
   "metadata": {},
   "source": [
    "#### Setup and Installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3d4a8",
   "metadata": {},
   "source": [
    "- Installing required libraries\n",
    "\n",
    "**Note**: These versions are specifically selected for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfe42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --q torch==2.3.1 trl==0.11.4 peft==0.14.0 pandas numpy==1.26.0 datasets==3.2.0 transformers==4.45.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5947f3",
   "metadata": {},
   "source": [
    "- Importing required libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    GenerationConfig\n",
    ")\n",
    "from trl import DPOConfig, DPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd301da4",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer Setup\n",
    "\n",
    "For this workshop, we'll use the OPT model, a decoder-only language model from Meta AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905d106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability and set device accordingly\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model selection:  We're using AraGPT2, an Arabic language model based on the GPT-2 architecture\n",
    "MODEL_NAME = \"aubmindlab/aragpt2-base\" # \"unsloth/Qwen2.5-1.5B\" \n",
    "\n",
    "# The model name for the fine-tuned version\n",
    "FINETUNED_MODEL_NAME = \"aragpt2-base-dpo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597bc96",
   "metadata": {},
   "source": [
    "- Set the Hugging face token found [here](https://huggingface.co/settings/tokens)\n",
    "In order to interact and use the hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hugging Face token for accessing models \n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_HF_API_TOKEN\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fafe1",
   "metadata": {},
   "source": [
    "- Get your wandb API Key found [here](https://wandb.ai/authorize) and set it as an environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201aea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"YOUR_API_TOKEN\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f152eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for training\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Load reference model (used during training)\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Load and configure tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Configure padding token and padding side\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Padding on the right side preserves the beginning of sequences\n",
    "\n",
    "# Disable cache during forward pass to save memory\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bf1e6",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6262ff",
   "metadata": {},
   "source": [
    "-  Load the Arabic preference [dataset](https://huggingface.co/datasets/FreedomIntelligence/Arabic-preference-data-RLHF) for RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debe8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset contains pairs of responses where one is preferred over the other\n",
    "# We use only 10% for this demo to keep training time reasonable\n",
    "print(\"Loading preference dataset...\")\n",
    "ds = load_dataset(\"FreedomIntelligence/Arabic-preference-data-RLHF\", split=\"train[:10%]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503566f",
   "metadata": {},
   "source": [
    "Examine the dataset structure to understand its format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf1dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample features: dict_keys(['id', 'instruction', 'chosen', 'rejected'])\n",
      "\n",
      "Sample entry:\n",
      "{'id': '10001_q', 'instruction': 'Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù‚ÙŠÙ‚ Ø§Ø±Ø¨Ø§Ø­ Ù…Ù† ØªØ·Ø¨ÙŠÙ‚ Ø§Ùˆ Ù„Ø¹Ø¨Ø© ÙÙ‚Ø· Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø§Ø¹Ù„Ø§Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ØŸ', 'chosen': 'Ù†Ø¹Ù…ØŒ ÙŠÙ…ÙƒÙ† ØªØ­Ù‚ÙŠÙ‚ Ø£Ø±Ø¨Ø§Ø­ Ù…Ù† ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ù„Ø¹Ø¨Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚. Ù‡Ø°Ø§ ÙŠØªÙ… Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ù…Ø«Ù„ Google AdMob Ø£Ùˆ Facebook Audience NetworkØŒ Ø­ÙŠØ« ØªÙØ¸Ù‡Ø± Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ø§Ù„Ù„Ø¹Ø¨Ø© ÙˆÙŠØªÙ… ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠÙ† ÙŠÙ†Ù‚Ø±ÙˆÙ† Ø¹Ù„Ù‰ ØªÙ„Ùƒ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª. Ø§Ù„Ø¹Ø§Ø¦Ø¯ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØªØ±Ø§ÙˆØ­ Ù…Ù† ØµÙØ± Ø¥Ù„Ù‰ Ø¹Ø´Ø±Ø§Øª Ø§Ù„Ø£Ù„Ø§Ù Ù…Ù† Ø§Ù„Ø¯ÙˆÙ„Ø§Ø±Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø´Ø¹Ø¨ÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ø§Ù„Ù„Ø¹Ø¨Ø© ÙˆÙƒÙŠÙÙŠØ© ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ø¹ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª.', 'rejected': 'Ù†Ø¹Ù…ØŒ ÙŠÙ…ÙƒÙ† ØªØ­Ù‚ÙŠÙ‚ Ø£Ø±Ø¨Ø§Ø­ Ù…Ù† ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ù„Ø¹Ø¨Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚. ÙŠØ¹ØªÙ…Ø¯ Ù‡Ø°Ø§ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø´Ø¹Ø¨ÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†ØŒ ÙˆÙ†ÙˆØ¹ ÙˆÙ…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§ØªØŒ ÙˆØ§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ³ÙˆÙŠÙ‚ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©. ÙŠÙ…ÙƒÙ† Ù„Ù„Ø´Ø±ÙƒØ§Øª Ø£Ù† ØªØ­Ù‚Ù‚ Ø£Ø±Ø¨Ø§Ø­ Ø£ÙŠØ¶Ù‹Ø§ Ù…Ù† Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„ØªØ§Ø¨Ø¹Ø© Ø£Ùˆ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø± Ø£Ùˆ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªØ¸Ù‡Ø± Ø¹Ù†Ø¯ ØªÙˆÙ‚Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¹Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset sample features:\", ds[0].keys())\n",
    "print(\"\\nExample entry from dataset:\")\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4ce61",
   "metadata": {},
   "source": [
    "- Transform the dataset into the format required by DPO:\n",
    "    - `prompt`: The input query\n",
    "    - `chosen`: The preferred response\n",
    "    - `rejected`: The less preferred response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPreparing dataset for DPO training...\")\n",
    "ds = ds.rename_column(\"instruction\", \"prompt\").remove_columns([\"id\"])\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "# We use a 90/10 split with a fixed random seed for reproducibility\n",
    "ds = ds.train_test_split(0.1, shuffle=True, seed=42)\n",
    "train_dataset, eval_dataset = ds[\"train\"], ds[\"test\"]\n",
    "print(f\"Training set size: {len(train_dataset)}, Evaluation set size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ea01e",
   "metadata": {},
   "source": [
    "| Chosen | Rejected | Prompt |\n",
    "| --- | --- | --- |\n",
    " | Ù†Ø¹Ù…ØŒ ÙŠÙ…ÙƒÙ† ØªØ­Ù‚ÙŠÙ‚ Ø£Ø±Ø¨Ø§Ø­ Ù…Ù† ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ù„Ø¹Ø¨Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚. Ù‡Ø°Ø§ ÙŠØªÙ… Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ù…Ø«Ù„ Google AdMob Ø£Ùˆ Facebook Audience NetworkØŒ Ø­ÙŠØ« ØªÙØ¸Ù‡Ø± Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ø§Ù„Ù„Ø¹Ø¨Ø© ÙˆÙŠØªÙ… ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠÙ† ÙŠÙ†Ù‚Ø±ÙˆÙ† Ø¹Ù„Ù‰ ØªÙ„Ùƒ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª. Ø§Ù„Ø¹Ø§Ø¦Ø¯ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØªØ±Ø§ÙˆØ­ Ù…Ù† ØµÙØ± Ø¥Ù„Ù‰ Ø¹Ø´Ø±Ø§Øª Ø§Ù„Ø£Ù„Ø§Ù Ù…Ù† Ø§Ù„Ø¯ÙˆÙ„Ø§Ø±Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø´Ø¹Ø¨ÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ø§Ù„Ù„Ø¹Ø¨Ø© ÙˆÙƒÙŠÙÙŠØ© ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ø¹ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª.|Ù†Ø¹Ù…ØŒ ÙŠÙ…ÙƒÙ† ØªØ­Ù‚ÙŠÙ‚ Ø£Ø±Ø¨Ø§Ø­ Ù…Ù† ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ù„Ø¹Ø¨Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚. ÙŠØ¹ØªÙ…Ø¯ Ù‡Ø°Ø§ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø´Ø¹Ø¨ÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†ØŒ ÙˆÙ†ÙˆØ¹ ÙˆÙ…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§ØªØŒ ÙˆØ§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ³ÙˆÙŠÙ‚ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©. ÙŠÙ…ÙƒÙ† Ù„Ù„Ø´Ø±ÙƒØ§Øª Ø£Ù† ØªØ­Ù‚Ù‚ Ø£Ø±Ø¨Ø§Ø­ Ø£ÙŠØ¶Ù‹Ø§ Ù…Ù† Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„ØªØ§Ø¨Ø¹Ø© Ø£Ùˆ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø± Ø£Ùˆ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªØ¸Ù‡Ø± Ø¹Ù†Ø¯ ØªÙˆÙ‚Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¹Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.| Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù‚ÙŠÙ‚ Ø§Ø±Ø¨Ø§Ø­ Ù…Ù† ØªØ·Ø¨ÙŠÙ‚ Ø§Ùˆ Ù„Ø¹Ø¨Ø© ÙÙ‚Ø· Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø§Ø¹Ù„Ø§Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fb29e",
   "metadata": {},
   "source": [
    "### Optional: Quantized Model Configuration (for GPUs)\n",
    "For r environments with GPU support, you can use quantization to reduce memory usage: Uncomment the following blocks if working with limited GPU memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e5745",
   "metadata": {},
   "source": [
    "![lora](https://pytorch.org/torchtune/0.4/_images/lora_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes # this package is required for quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d18d2a",
   "metadata": {},
   "source": [
    "**_Note:_**  _You can run the installed package by restarting a Kernel._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes  # Required for quantization\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# # Configure quantization parameters\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,                    # Load model in 4-bit precision instead of 16/32-bit\n",
    "#     bnb_4bit_use_double_quant=True,       # Use double quantization for better accuracy\n",
    "#     bnb_4bit_quant_type=\"nf4\",            # Use normalized float 4-bit quantization\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for calculations\n",
    "# )\n",
    "\n",
    "# # Load models with quantization config\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME, \n",
    "#     quantization_config=quantization_config,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# model_ref = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME, \n",
    "#     quantization_config=quantization_config,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "# model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb0a23",
   "metadata": {},
   "source": [
    "#### LoRA Configuration for Efficient Fine-tuning\n",
    "#LoRA allows us to train only a small number of adapter parameters instead of the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56115e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT (Parameter-Efficient Finetuning) configuration\n",
    "print(\"Setting up LoRA configuration...\")\n",
    "peft_config = LoraConfig(\n",
    "    r=4,                    # Rank of the low-rank decomposition matrices\n",
    "    target_modules=[        # Which modules to apply LoRA to\n",
    "        'c_proj',           # Projection layers in the transformer\n",
    "        'c_attn'            # Attention layers in the transformer\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",  # The type of task we're performing\n",
    "    lora_alpha=8,           # Scaling factor for the LoRA parameters (typically 2x rank)\n",
    "    lora_dropout=0.1,       # Dropout probability for LoRA layers\n",
    "    bias=\"none\",           # Whether to train bias parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56766ec2",
   "metadata": {},
   "source": [
    "####  DPO Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9ae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure DPO training parameters\n",
    "print(\"Setting up DPO training configuration...\")\n",
    "training_args = DPOConfig(\n",
    "    beta=0.1,                      # Temperature parameter for the DPO loss (typically 0.1-0.5)\n",
    "                                   # Higher values make the model more conservative\n",
    "    output_dir=\"dpo\",              # Directory to save model checkpoints\n",
    "    num_train_epochs=5,            # Number of training passes through the data\n",
    "    per_device_train_batch_size=2, # Batch size for training (adjust based on GPU memory)\n",
    "    per_device_eval_batch_size=2,  # Batch size for evaluation\n",
    "    remove_unused_columns=False,   # Keep all columns in the dataset\n",
    "    logging_steps=10,              # Log training progress every 10 steps\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients over multiple batches\n",
    "                                   # Effectively increases batch size to 2 * 4 = 8\n",
    "    learning_rate=1e-4,            # Learning rate for the optimizer\n",
    "    evaluation_strategy=\"epoch\",   # Evaluate after each epoch\n",
    "    warmup_steps=2,                # Number of warmup steps for learning rate scheduler\n",
    "    save_steps=500,                # Save checkpoint every 500 steps\n",
    "    report_to='wandb'              # Report training metrics to Weights & Biases\n",
    "                                   # Use 'none' to disable reporting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a74d8",
   "metadata": {},
   "source": [
    "####  DPO Trainer Setup\n",
    "\n",
    "Next step is creating the actual trainer using DPOTrainer class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c80458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/trl/trainer/dpo_trainer.py:655: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/trl/trainer/dpo_trainer.py:673: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6236/6236 [00:11<00:00, 534.04 examples/s]\n",
      "Tokenizing eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 693/693 [00:01<00:00, 602.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the DPO trainer that will handle the training process\n",
    "print(\"Setting up DPO trainer...\")\n",
    "trainer = DPOTrainer(\n",
    "    model=model,              # The model to be fine-tuned\n",
    "    ref_model=None,           # Reference model (None because we're using LoRA)\n",
    "                              # When using LoRA, DPOTrainer will automatically handle the reference model\n",
    "    args=training_args,       # Training arguments defined above\n",
    "    train_dataset=train_dataset,  # Training data\n",
    "    eval_dataset=eval_dataset,    # Evaluation data\n",
    "    tokenizer=tokenizer,          # Tokenizer\n",
    "    peft_config=peft_config,      # LoRA configuration\n",
    "    max_length=512,               # Maximum sequence length for inputs and outputs\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37ff85",
   "metadata": {},
   "source": [
    "Please note that when using LoRA for the base model, it's efficient to leave the model_ref param null, in which case the DPOTrainer will unload the adapter for reference inference.\n",
    "\n",
    "\n",
    "Now, you're all set for training the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115bc8c",
   "metadata": {},
   "source": [
    "#### Training Process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e7bf3b",
   "metadata": {},
   "source": [
    "**Training can be time-consuming on CPU and may cause memory issues, If you encounter problems, skip to the next section to load a pre-trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff5262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mafaf\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/afafelwafi/HackAI/wandb/run-20250502_195410-45waqojn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/afaf/huggingface/runs/45waqojn' target=\"_blank\">dpo</a></strong> to <a href='https://wandb.ai/afaf/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/afaf/huggingface' target=\"_blank\">https://wandb.ai/afaf/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/afaf/huggingface/runs/45waqojn' target=\"_blank\">https://wandb.ai/afaf/huggingface/runs/45waqojn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3895' max='3895' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3895/3895 8:09:01, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.351199</td>\n",
       "      <td>7.322288</td>\n",
       "      <td>4.321124</td>\n",
       "      <td>0.860231</td>\n",
       "      <td>3.001164</td>\n",
       "      <td>-783.083191</td>\n",
       "      <td>-1090.636475</td>\n",
       "      <td>-3.003296</td>\n",
       "      <td>-3.291013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>0.357454</td>\n",
       "      <td>8.366143</td>\n",
       "      <td>4.700235</td>\n",
       "      <td>0.864553</td>\n",
       "      <td>3.665909</td>\n",
       "      <td>-779.292175</td>\n",
       "      <td>-1080.197876</td>\n",
       "      <td>-3.019666</td>\n",
       "      <td>-3.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.172100</td>\n",
       "      <td>0.377715</td>\n",
       "      <td>8.918534</td>\n",
       "      <td>4.731424</td>\n",
       "      <td>0.861671</td>\n",
       "      <td>4.187110</td>\n",
       "      <td>-778.980347</td>\n",
       "      <td>-1074.674072</td>\n",
       "      <td>-3.005437</td>\n",
       "      <td>-3.298560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3895, training_loss=0.2658155122525579, metrics={'train_runtime': 29344.701, 'train_samples_per_second': 1.063, 'train_steps_per_second': 0.133, 'total_flos': 0.0, 'train_loss': 0.2658155122525579, 'epoch': 4.996792815907633})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start the training process\n",
    "print(\"Starting DPO training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac63cde",
   "metadata": {},
   "source": [
    "!!!!You can skip the training !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to Hugging Face Hub\n",
    "# print(\"Pushing model to Hugging Face Hub...\")\n",
    "# trainer.push_to_hub(FINETUNED_MODEL_NAME, commit_message=\"DPO finetuning with LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from the local checkpoint\n",
    "# print(\"Loading trained model from checkpoint...\")\n",
    "# dpo_model = AutoModelForCausalLM.from_pretrained('./dpo/checkpoint-3895').to(device)\n",
    "# dpo_tokenizer = AutoTokenizer.from_pretrained('./dpo/checkpoint-3895')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640afd5",
   "metadata": {},
   "source": [
    "#### Loading Pre-trained Model (Alternative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f5a1f",
   "metadata": {},
   "source": [
    "If training is too resource-intensive, you can load a pre-trained model\n",
    "\n",
    "This section loads a model that's already been fine-tuned with DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc96fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained DPO model from Hub...\n"
     ]
    }
   ],
   "source": [
    "# Load the DPO-fine-tuned model from Hugging Face Hub\n",
    "print(\"Loading pre-trained DPO model from Hub...\")\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(f\"HackAI-2025/{FINETUNED_MODEL_NAME}\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"HackAI-2025/{FINETUNED_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference (baseline) model for comparison\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995e198",
   "metadata": {},
   "source": [
    "####  Text Generation and Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfe8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up text generation configuration...\n",
      "Generating response with DPO model...\n",
      "DPO response:\t ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„ØªØºÙ„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„ØªÙˆØªØ±ØŸ Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„Ø§ÙƒØªØ¦Ø§Ø¨ ÙˆØ§Ù„Ù‚Ù„Ù‚ Ù‡Ùˆ Ø£Ø­Ø¯ Ø§Ù„Ø§Ø¶Ø·Ø±Ø§Ø¨Ø§Øª Ø§Ù„Ù†ÙØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªØµÙŠØ¨ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ† Ù…Ù† Ø§Ù„ØªÙˆØªØ± ØŒ ÙˆÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ù‚Ù„Ù‚ Ù‡Ùˆ Ø£ÙƒØ«Ø± Ù…Ø§ ÙŠÙ…ÙŠØ² Ø§Ù„Ø´Ø®Øµ Ø§Ù„Ù…ØµØ§Ø¨ Ø¨Ø§Ù„Ù‚Ù„Ù‚ Ø¹Ù† Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø§Ù„Ù…ØµØ§Ø¨ÙŠÙ† Ø¨Ù‡ .1 - Ø§Ù„Ø§ÙƒØªØ¦Ø§Ø¨ : ÙˆÙ‡Ùˆ Ø­Ø§Ù„Ø© Ù†ÙØ³ÙŠØ© ØªØ¤Ø«Ø± Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„ØªÙÙƒÙŠØ± ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªØµØ±Ù ÙˆÙÙŠ Ø§Ù„Ø£Ø­Ø§Ø³ÙŠØ³ ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø³Ù„Ø¨ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ø­Ø²Ù† ÙˆØ§Ù„ØºØ¶Ø¨ ÙˆØ§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¥Ø­Ø¨Ø§Ø· ÙˆØ§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„Ø®ÙˆÙ Ø£Ùˆ Ø§Ù„ØºØ¶Ø¨ ÙˆØ§Ù„Ø­Ø²Ù† ÙˆØ§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¥Ø­Ø¨Ø§Ø· ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø®ØªÙ„ÙØ© .2\n",
      "\n",
      "Generating response with baseline model...\n",
      "Baseline response:\t ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„ØªØºÙ„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„ØªÙˆØªØ±ØŸ ØŸ ! ! ØŒ Ù…Ù† Ù‡Ùˆ Ø§Ù„Ø´Ø®Øµ Ø§Ù„Ø°ÙŠ ÙŠÙ…Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø­Ø³Ø§Ø³ Ø§Ù„Ø±Ø§Ø¦Ø¹ . . . Ù‡Ù„ Ù‡Ùˆ Ø´Ø®Øµ Ø¹Ø§Ø¯ÙŠ Ø£Ù… Ù…Ø¬Ù†ÙˆÙ† ØŸ . . . Ø£Ù… Ø£Ù†Ù‡ Ø¥Ù†Ø³Ø§Ù† ÙØ§Ø´Ù„ ØŸ . . . Ù„Ø§ Ø£Ø¯Ø±ÙŠ . . Ù„ÙƒÙ† Ù…Ø§ Ø£Ø¹Ø±ÙÙ‡ Ø£Ù† ÙƒÙ„ Ù…Ù†Ø§ ÙŠØ¹Ø§Ù†ÙŠ Ù…Ù† Ù…Ø´ÙƒÙ„Ø© . . . Ø£Ù…Ø§ Ø£Ù†Ø§ ÙØ£Ø¹ØªÙ‚Ø¯ Ø£Ù† Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ø§Ù„Ù†Ø§Ø¬Ø­ ÙÙŠ Ø­ÙŠØ§ØªÙ‡ Ù‡Ùˆ Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ø§Ù„Ø°ÙŠ ÙŠÙ…ØªÙ„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø´Ø¹ÙˆØ± . . . ÙÙ‡Ùˆ Ø§Ù„Ø°ÙŠ ÙŠØ³ØªØ·ÙŠØ¹\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducible generation\n",
    "set_seed(40)\n",
    "\n",
    "# Configure generation parameters\n",
    "print(\"Setting up text generation configuration...\")\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=70,         # Maximum number of tokens to generate\n",
    "    do_sample=True,            # Use sampling instead of greedy decoding\n",
    "    top_k=50,                  # Consider top 50 tokens at each step\n",
    "    top_p=0.8,                 # Consider tokens with cumulative probability of 0.8\n",
    "    temperature=0.8,           # Controls randomness (higher = more random)\n",
    "    repetition_penalty=1.2,    # Penalize repetition of tokens\n",
    "    pad_token_id=tokenizer.eos_token_id  # Use EOS token for padding\n",
    ")\n",
    "\n",
    "# Define a test prompt in Arabic\n",
    "PROMPT = \"ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„ØªØºÙ„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„ØªÙˆØªØ±ØŸ\" # \"What are the benefits of healthy food?\"\n",
    "\n",
    "# Tokenize the prompt and move to the appropriate device\n",
    "inputs = tokenizer(PROMPT, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate text with the DPO-fine-tuned model\n",
    "print(\"Generating response with DPO model...\")\n",
    "outputs = dpo_model.generate(**inputs, generation_config=generation_config).to(device)\n",
    "print(\"DPO response:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Generate text with the baseline model for comparison\n",
    "print(\"\\nGenerating response with baseline model...\")\n",
    "outputs = model_ref.generate(**inputs, generation_config=generation_config).to(device)\n",
    "print(\"Baseline response:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda11b2f",
   "metadata": {},
   "source": [
    "Althought the model is trained on a small data for 5 epochs only, it can be seen that the response generated by the DPO-tuned model is more concise and straightforward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97cba2",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\"Ù…Ø§ Ù‡ÙŠ ÙÙˆØ§Ø¦Ø¯ Ø§Ù„ØºØ°Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØŸ\",\n",
    "\"ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„ØªØºÙ„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„ØªÙˆØªØ±ØŸ\",\n",
    "\"Ø§Ø´Ø±Ø­ Ù„ÙŠ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ….\",\n",
    "\"Ù…Ø§ Ù‡ÙŠ Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¹Ù„Ù… Ù„ØºØ© Ø¬Ø¯ÙŠØ¯Ø©ØŸ\",\n",
    "\"Ù‡Ù„ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠ Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± ÙÙŠ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø´ÙØ±Ø©ØŸ\",\n",
    "\"Ù…Ø§ Ù‡ÙŠ Ø£Ø®Ø·Ø± ØªÙ‡Ø¯ÙŠØ¯Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„ÙŠÙˆÙ…ØŸ\",\n",
    "\"ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØªØ­Ø³ÙŠÙ† Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ù„Ø¯ÙŠØŸ\",\n",
    "\"Ø§Ù‚ØªØ±Ø­ Ø¨Ø±Ù†Ø§Ù…Ø¬Ø§Ù‹ Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ø±ÙŠØ§Ø¶ÙŠØ© Ù„Ø´Ø®Øµ Ù…Ø¨ØªØ¯Ø¦.\",\n",
    "\"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ø¨Ø¯Ø¡ Ù…Ø´Ø±ÙˆØ¹ ØªØ¬Ø§Ø±ÙŠ Ù†Ø§Ø¬Ø­ØŸ\",\n",
    "\"ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù„Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø£Ù† ØªØ³Ø§Ø¹Ø¯ ÙÙŠ Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© ØªØºÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø®ØŸ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557d38e",
   "metadata": {},
   "source": [
    "## Exercise 1: Experiment with Generation Parameters\n",
    "Try different generation parameters (temperature, top_p, top_k) and compare their effects on:\n",
    "1. The quality of the generated text\n",
    "2. The diversity of responses\n",
    "3. How closely they align with human preferences\n",
    "\n",
    "## Exercise 2: Test with Different Prompts\n",
    "Create 3-5 different prompts and compare the responses from:\n",
    "1. The base model (model_ref)\n",
    "2. The DPO fine-tuned model\n",
    "Analyze the differences and explain how the DPO training has affected the outputs.\n",
    "\n",
    "## Exercise 3: Error Analysis\n",
    "Identify cases where the DPO model still produces suboptimal responses and suggest:\n",
    "1. Possible reasons for these failures\n",
    "2. How you might improve the training data to address these issues\n",
    "3. Alternative training strategies that might help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e834e",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to better understand DPO:\n",
    "\n",
    "1. **Experiment with Generation Parameters**\n",
    "   - Try different values for temperature, top_p, and top_k\n",
    "   - How do they affect the responses?\n",
    "\n",
    "2. **Test Different Prompts**\n",
    "   - Try these Arabic prompts:\n",
    "   ```python\n",
    "   test_questions = [\n",
    "       \"Ù…Ø§ Ù‡ÙŠ ÙÙˆØ§Ø¦Ø¯ Ø§Ù„ØºØ°Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØŸ\",\n",
    "       \"ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„ØªØºÙ„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„ØªÙˆØªØ±ØŸ\",\n",
    "       \"Ø§Ø´Ø±Ø­ Ù„ÙŠ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ….\",\n",
    "       \"Ù…Ø§ Ù‡ÙŠ Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¹Ù„Ù… Ù„ØºØ© Ø¬Ø¯ÙŠØ¯Ø©ØŸ\",\n",
    "       \"Ù‡Ù„ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠ Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± ÙÙŠ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø´ÙØ±Ø©ØŸ\"\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. **Compare Responses**\n",
    "   - How do the DPO model's responses differ from the baseline?\n",
    "   - What makes the DPO responses better or worse?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackai",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
