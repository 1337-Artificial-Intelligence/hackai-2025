{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d97454a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/embedding_mlm_train_bert_goudma.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728416f5",
   "metadata": {},
   "source": [
    "# Training a BERT Model for Moroccan Arabic (Darija)\n",
    "\n",
    "In this notebook, we'll learn how to train a BERT model specifically for Moroccan Arabic (Darija). This is important because:\n",
    "- Pre-trained models often don't work well with Darija\n",
    "- We can create a model that better understands our local language\n",
    "- It's a great way to learn about how language models work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76637d4e",
   "metadata": {},
   "source": [
    "## What is Masked Language Modeling (MLM)?\n",
    "\n",
    "MLM is like a fill-in-the-blank game for computers:\n",
    "- We hide some words in a sentence\n",
    "- The model tries to guess what those hidden words are\n",
    "- This helps the model learn how words relate to each other\n",
    "\n",
    "Example:\n",
    "- Original: \"I love eating couscous on Fridays\"\n",
    "- Masked: \"I [MASK] eating couscous on [MASK]\"\n",
    "- Model predicts: \"love\" and \"Fridays\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc30f5ab",
   "metadata": {},
   "source": [
    "## Why BERT?\n",
    "\n",
    "BERT is a powerful language model that:\n",
    "- Can understand context from both directions (left and right)\n",
    "- Works well for many languages\n",
    "- Can be trained on our own data\n",
    "\n",
    "TODO: Add image showing BERT's bidirectional attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0ce49",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0705410",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers huggingface_hub wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab6820",
   "metadata": {},
   "source": [
    "## 1. Load Our Dataset\n",
    "We'll use a dataset of Moroccan Arabic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54929418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face (you'll need to create an account)\n",
    "login()\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"atlasia/good25\")\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3552329",
   "metadata": {},
   "source": [
    "## 2. Prepare Our Data\n",
    "We need to:\n",
    "1. Select only the text content\n",
    "2. Split into train/test sets\n",
    "3. Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the content column\n",
    "dataset = dataset.select_columns([\"content\"])\n",
    "\n",
    "# Split into train/test\n",
    "dataset_splited = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f456ba",
   "metadata": {},
   "source": [
    "## 3. Load Our Model\n",
    "We'll use a pre-trained model and continue training it on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e541c80",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_id = \"atlasia/XLM-RoBERTa-Morocco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3339a",
   "metadata": {},
   "source": [
    "## 4. Process Our Data\n",
    "We need to:\n",
    "1. Tokenize our text\n",
    "2. Prepare it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877cfa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_tokenizer(examples):\n",
    "    return tokenizer(examples[\"content\"])\n",
    "\n",
    "# Tokenize our datasets\n",
    "train_tokenized = dataset_splited[\"train\"].map(ds_tokenizer).remove_columns(dataset_splited[\"train\"].column_names)\n",
    "eval_tokenized = dataset_splited[\"test\"].map(ds_tokenizer).remove_columns(dataset_splited[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181deab",
   "metadata": {},
   "source": [
    "## 5. Prepare for Training\n",
    "We need to:\n",
    "1. Split text into chunks\n",
    "2. Set up our training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d880d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks\n",
    "context_length = 256\n",
    "def concatenate_splite(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[\"input_ids\"])\n",
    "    if total_length >= context_length:\n",
    "        total_length = (total_length // context_length) * context_length\n",
    "    result = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    for k, v in concatenated_examples.items():\n",
    "        for i in range(0, len(v), context_length):\n",
    "            result[k].append(v[i:i+context_length])\n",
    "    return result\n",
    "\n",
    "# Apply the splitting\n",
    "train_ds = train_tokenized.map(concatenate_splite, batched=True)\n",
    "eval_ds = eval_tokenized.map(concatenate_splite, batched=True)\n",
    "\n",
    "# Set up data collator for MLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.2  # Mask 20% of words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d90a53",
   "metadata": {},
   "source": [
    "## 6. Train Our Model\n",
    "Now we'll set up and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Set up training arguments\n",
    "args = TrainingArguments(\n",
    "    \"model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    warmup_ratio=0.03,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"Bert CPT\"\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b67aa",
   "metadata": {},
   "source": [
    "## 7. Start Training\n",
    "This might take a while. For the workshop, we'll use a pre-trained checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15828f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train (takes about 1 hour)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1179c9",
   "metadata": {},
   "source": [
    "## 8. Save Our Model\n",
    "Once training is complete, we can save our model to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb08260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save (requires Hugging Face account)\n",
    "# trainer.push_to_hub(\"your-username/your-model-name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55dc881",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "- Try using your model for different tasks\n",
    "- Experiment with different training parameters\n",
    "- Share your model with the community\n",
    "\n",
    "TODO: Add image showing model usage examples"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
