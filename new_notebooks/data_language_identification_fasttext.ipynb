{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d34a892",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NouamaneTazi/hackai-challenges/blob/main/new_notebooks/data_language_identification_fasttext.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217d74a1",
   "metadata": {},
   "source": [
    "# Language Identification using FastText\n",
    "\n",
    "**Time Estimate**: 1 hour\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what language identification is and why it's important\n",
    "- Learn how to use FastText for text classification\n",
    "- Build a simple model to identify Moroccan Darija vs other Arabic dialects\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of text classification (briefly explained below)\n",
    "\n",
    "## What is Language Identification?\n",
    "Language identification is the task of determining which language a given text is written in. In this notebook, we'll focus on identifying Moroccan Darija (Moroccan Arabic) from other Arabic dialects.\n",
    "\n",
    "## What is FastText?\n",
    "FastText is a library for efficient text classification and word representation learning. It's particularly good at handling text in different languages and can work well even with limited training data.\n",
    "\n",
    "## What is Text Classification?\n",
    "Text classification is a machine learning task where we teach a computer to categorize text into predefined groups. In our case, we're teaching it to categorize text as either Moroccan Darija or another Arabic dialect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ab79e",
   "metadata": {},
   "source": [
    "# Setup (5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "! pip install fasttext datasets pandas scikit-learn seaborn arabic-reshaper wordcloud python-bidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4310a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8713b",
   "metadata": {},
   "source": [
    "# Load and Explore Data (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Darija-LID dataset\n",
    "data = load_dataset('atlasia/Darija-LID')\n",
    "train_data = data['train'].to_pandas()\n",
    "test_data = data['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b0b2f",
   "metadata": {},
   "source": [
    "Let's look at some examples from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bf486",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"Example texts from our dataset:\")\n",
    "print(\"\\nMoroccan Darija example:\")\n",
    "print(train_data[train_data['label'] == 'ary']['text'].iloc[0])\n",
    "print(\"\\nOther dialect example:\")\n",
    "print(train_data[train_data['label'] == 'other']['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e5ba2",
   "metadata": {},
   "source": [
    "# Data Preprocessing (10 minutes)\n",
    "\n",
    "Before training our model, we need to clean our text data. This involves:\n",
    "- Removing URLs and numbers\n",
    "- Removing special characters\n",
    "- Converting text to lowercase\n",
    "- Removing extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # remove Latin characters but keep Arabic text\n",
    "    text = re.sub(r'[a-zA-Z]', '', text)\n",
    "    # remove emojis but keep Arabic text\n",
    "    text = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "train_data['processed_text'] = train_data['text'].apply(preprocess_text)\n",
    "test_data['processed_text'] = test_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0d00a",
   "metadata": {},
   "source": [
    "# Data Visualization (10 minutes)\n",
    "\n",
    "Let's visualize our data to understand it better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of text lengths\n",
    "train_data['text_length'] = train_data['processed_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(\n",
    "    data=train_data.assign(text_length_clipped=train_data['text_length'].clip(upper=30)),\n",
    "    x='text_length_clipped',\n",
    "    hue='label',\n",
    "    bins=30,\n",
    "    palette={'ary': '#2ecc71', 'other': '#e74c3c'},\n",
    "    multiple=\"layer\",\n",
    "    stat='percent'\n",
    ")\n",
    "plt.title('Distribution of Text Lengths by Dialect')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0454a",
   "metadata": {},
   "source": [
    "# Train FastText Model (15 minutes)\n",
    "\n",
    "Now we'll train our FastText model. FastText requires data in a specific format:\n",
    "- Each line should start with `__label__` followed by the label\n",
    "- Then a space and the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0457222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data in FastText format\n",
    "data_train = train_data[['label', 'processed_text']].copy()\n",
    "data_train['label'] = '__label__' + data_train['label']\n",
    "\n",
    "data_test = test_data[['label', 'processed_text']].copy()\n",
    "data_test['label'] = '__label__' + data_test['label']\n",
    "\n",
    "# Save data\n",
    "data_train.to_csv('data_train.txt', header=None, index=None, sep=' ', mode='w')\n",
    "data_test.to_csv('data_test.txt', header=None, index=None, sep=' ', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a084a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = fasttext.train_supervised(\n",
    "    'data_train.txt',\n",
    "    lr=0.1,\n",
    "    epoch=5,\n",
    "    dim=100,\n",
    "    minCount=5,\n",
    "    wordNgrams=2,\n",
    "    bucket=200,\n",
    "    loss='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508874a",
   "metadata": {},
   "source": [
    "# Evaluate Model (10 minutes)\n",
    "\n",
    "Let's see how well our model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "with open('data_test.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "texts = [' '.join(x.split()[1:]) for x in lines]\n",
    "preds = model.predict(texts)\n",
    "\n",
    "# Get predictions and true labels\n",
    "y_hat = [x[0].split('__label__')[1] for x in preds[0]]\n",
    "y_true = [x.split('__label__')[1].split()[0] for x in lines]\n",
    "\n",
    "# Print results\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_hat))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_true, y_hat)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642073a",
   "metadata": {},
   "source": [
    "# Try It Yourself!\n",
    "\n",
    "Now you can try the model with your own text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "test_text = \"كيفاش حالك\"  # Replace with your own text\n",
    "processed_text = preprocess_text(test_text)\n",
    "prediction = model.predict(processed_text)\n",
    "print(f\"Predicted dialect: {prediction[0][0].split('__label__')[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa97f80",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "- Try different preprocessing steps\n",
    "- Experiment with different FastText parameters\n",
    "- Try the model on different Arabic dialects\n",
    "- Learn about other text classification methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
