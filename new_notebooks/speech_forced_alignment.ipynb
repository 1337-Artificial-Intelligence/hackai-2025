{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/speech_forced_alignment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b3bcc",
   "metadata": {},
   "source": [
    "# Word-Level Force Alignment Tutorial\n",
    "\n",
    "In this tutorial, you'll learn how to automatically align words in a transcript with their exact timestamps in an audio recording. This is called \"force alignment\" and it's super useful for creating subtitles, language learning apps, and more!\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what force alignment is and why it's useful\n",
    "- Learn how to use wav2vec to align Arabic speech with text\n",
    "- Create an interactive tool where you can click on words to hear them\n",
    "\n",
    "## What is Force Alignment?\n",
    "\n",
    "Force alignment is like creating a precise timeline of when each word is spoken in an audio recording. It takes:\n",
    "- An audio recording\n",
    "- A text transcript of what was said\n",
    "\n",
    "And tells you exactly when each word starts and ends in the audio.\n",
    "\n",
    "For example, if someone says \"مرحبا كيف حالك\" (Hello, how are you), force alignment would tell you:\n",
    "- \"مرحبا\" occurs from 0.2 to 0.5 seconds\n",
    "- \"كيف\" occurs from 0.6 to 0.8 seconds\n",
    "- \"حالك\" occurs from 0.9 to 1.2 seconds\n",
    "\n",
    "## Why is Force Alignment Useful?\n",
    "\n",
    "- **Subtitles**: Create perfectly timed subtitles for videos\n",
    "- **Language Learning**: Build apps where students can click words to hear pronunciation\n",
    "- **Video Editing**: Quickly find specific phrases in long recordings\n",
    "- **Dubbing**: Match new audio to original timing for natural-sounding dubs\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eed879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchaudio transformers matplotlib librosa ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613f03a",
   "metadata": {},
   "source": [
    "## Loading and Preparing Our Data\n",
    "\n",
    "We'll use a short Arabic audio sample with its transcript. The code below:\n",
    "1. Loads the audio file\n",
    "2. Reads the transcript\n",
    "3. Splits the text into words\n",
    "4. Shows you the audio waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54700d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# Set up matplotlib for Arabic text\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans', 'sans-serif']\n",
    "\n",
    "def split_arabic_text(text):\n",
    "    words = re.split(r'\\s+', text.strip())\n",
    "    return [word for word in words if word]\n",
    "\n",
    "# Load the audio and transcript\n",
    "audio_path = \"sample_data/arabic_sample.wav\"\n",
    "transcript_path = \"sample_data/arabic_transcript.txt\"\n",
    "\n",
    "# Read the transcript\n",
    "with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "    transcript = f.read().strip()\n",
    "\n",
    "words = split_arabic_text(transcript)\n",
    "print(f\"Transcript: {transcript}\")\n",
    "print(f\"Words ({len(words)}): {words}\")\n",
    "\n",
    "# Load and display the audio\n",
    "y, sr = librosa.load(audio_path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4ddf6",
   "metadata": {},
   "source": [
    "## Visualizing the Audio\n",
    "\n",
    "Let's look at the audio waveform to understand our data better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title('Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168a128",
   "metadata": {},
   "source": [
    "## Using Wav2Vec for Force Alignment\n",
    "\n",
    "We'll use wav2vec, a powerful AI model that can understand speech. Here's what we do:\n",
    "1. Load the pre-trained Arabic model\n",
    "2. Process our audio to the right format\n",
    "3. Get the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Arabic model\n",
    "model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "# Load and process audio\n",
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# Resample to 16kHz (required by wav2vec)\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "    waveform = resampler(waveform)\n",
    "    sample_rate = 16000\n",
    "\n",
    "# Get model predictions\n",
    "input_values = processor(waveform.squeeze().numpy(), sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# Convert to probabilities\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "probs = probs.squeeze().detach().cpu()\n",
    "print(f\"Shape of predictions: {probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35717a",
   "metadata": {},
   "source": [
    "## Understanding the Model's Output\n",
    "\n",
    "The model gives us a matrix of probabilities. Each row represents a tiny slice of time (20ms), and each column represents a possible Arabic character.\n",
    "\n",
    "Let's see what characters the model knows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = processor.tokenizer.get_vocab()\n",
    "id_to_char = {v: k for k, v in vocab.items()}\n",
    "print(\"First 10 characters in vocabulary:\", dict(list(id_to_char.items())[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1425bc91",
   "metadata": {},
   "source": [
    "## Finding Word Timestamps\n",
    "\n",
    "Now we'll use the model's predictions to find exactly when each word is spoken. We do this by:\n",
    "1. Creating a \"trellis\" (a fancy word for a probability map)\n",
    "2. Finding the most likely path through this map\n",
    "3. Converting this path into word timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e851c0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    token_index: int\n",
    "    time_index: int\n",
    "    score: float\n",
    "\n",
    "def get_trellis(emission, tokens, blank_id=0):\n",
    "    num_frame = emission.size(0)\n",
    "    num_tokens = len(tokens)\n",
    "    trellis = torch.zeros((num_frame, num_tokens))\n",
    "    trellis[1:, 0] = torch.cumsum(emission[1:, blank_id], 0)\n",
    "    trellis[0, 1:] = -float(\"inf\")\n",
    "    trellis[-num_tokens + 1 :, 0] = float(\"inf\")\n",
    "    \n",
    "    for t in range(num_frame - 1):\n",
    "        trellis[t + 1, 1:] = torch.maximum(\n",
    "            trellis[t, 1:] + emission[t, blank_id],\n",
    "            trellis[t, :-1] + emission[t, tokens[1:]])\n",
    "    return trellis\n",
    "\n",
    "# Get the alignment\n",
    "tokens = processor.tokenizer(transcript, add_special_tokens=False).input_ids\n",
    "trellis = get_trellis(probs, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711d302",
   "metadata": {},
   "source": [
    "## Creating Word Segments\n",
    "\n",
    "Now we'll convert our alignment into actual word timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f308a7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Segment:\n",
    "    label: str\n",
    "    start: int\n",
    "    end: int\n",
    "    score: float\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.label}\\t({self.score:4.2f}): [{self.start:5d}, {self.end:5d})\"\n",
    "\n",
    "def merge_repeats(path):\n",
    "    i1, i2 = 0, 0\n",
    "    segments = []\n",
    "    while i1 < len(path):\n",
    "        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
    "            i2 += 1\n",
    "        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
    "        segments.append(\n",
    "            Segment(\n",
    "                transcript[path[i1].token_index],\n",
    "                path[i1].time_index,\n",
    "                path[i2 - 1].time_index + 1,\n",
    "                score,\n",
    "            )\n",
    "        )\n",
    "        i1 = i2\n",
    "    return segments\n",
    "\n",
    "def merge_words(segments, separator=\" \"):\n",
    "    words = []\n",
    "    i1, i2 = 0, 0\n",
    "    while i1 < len(segments):\n",
    "        if i2 >= len(segments) or segments[i2].label == separator:\n",
    "            if i1 != i2:\n",
    "                segs = segments[i1:i2]\n",
    "                word = \"\".join([seg.label for seg in segs])\n",
    "                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n",
    "                words.append(Segment(word, segments[i1].start, segments[i2 - 1].end, score))\n",
    "            i1 = i2 + 1\n",
    "            i2 = i1\n",
    "        else:\n",
    "            i2 += 1\n",
    "    return words\n",
    "\n",
    "# Get word segments\n",
    "path = backtrack(trellis, probs, tokens)\n",
    "segments = merge_repeats(path)\n",
    "word_segments = merge_words(segments)\n",
    "\n",
    "# Print the results\n",
    "for word in word_segments:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c87a64",
   "metadata": {},
   "source": [
    "## Interactive Word Playback\n",
    "\n",
    "Now you can click on any word to hear it! Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_segment(i):\n",
    "    ratio = waveform.size(1) / trellis.size(0)\n",
    "    word = word_segments[i]\n",
    "    x0 = int(ratio * word.start)\n",
    "    x1 = int(ratio * word.end)\n",
    "    print(f\"{word.label} ({word.score:.2f}): {x0 / 16000:.3f} - {x1 / 16000:.3f} sec\")\n",
    "    segment = waveform[:, x0:x1]\n",
    "    return IPython.display.Audio(segment.numpy(), rate=16000)\n",
    "\n",
    "# Try clicking on different words to hear them!\n",
    "display_segment(0)  # First word\n",
    "display_segment(1)  # Second word\n",
    "display_segment(2)  # Third word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6c1b2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've learned how to:\n",
    "1. Use wav2vec to align Arabic speech with text\n",
    "2. Create word-level timestamps\n",
    "3. Build an interactive tool for word playback\n",
    "\n",
    "This technology is used in many real-world applications like:\n",
    "- Automatic subtitle generation\n",
    "- Language learning apps\n",
    "- Voice-controlled systems\n",
    "\n",
    "Try it with your own audio files and transcripts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea1561",
   "metadata": {},
   "source": [
    "## TODO: Add Images\n",
    "- Add a diagram showing how force alignment works\n",
    "- Add a visualization of the trellis matrix\n",
    "- Add a screenshot of the interactive word playback interface"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
