{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NouamaneTazi/hackai-challenges/blob/main/new_notebooks/embedding_evaluate_ragas.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad0f34",
   "metadata": {},
   "source": [
    "# Evaluating RAG Systems with RAGAS\n",
    "\n",
    "In this notebook, you'll learn how to evaluate the quality of Retrieval Augmented Generation (RAG) systems using RAGAS, a popular evaluation framework.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what RAG evaluation means and why it's important\n",
    "- Learn about different metrics used to evaluate RAG systems\n",
    "- Practice evaluating a simple RAG system using RAGAS\n",
    "\n",
    "## What is RAG Evaluation?\n",
    "When we build RAG systems, we need to know if they're working well. RAGAS helps us measure:\n",
    "- How relevant the retrieved information is\n",
    "- How accurate the generated answers are\n",
    "- How well the system uses the provided context\n",
    "\n",
    "Let's start by installing our required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q langchain openai ragas arxiv pymupdf chromadb wandb tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671eb5cd",
   "metadata": {},
   "source": [
    "## Setting Up OpenAI\n",
    "We'll need an OpenAI API key to use their models. You can get one from [OpenAI's website](https://platform.openai.com/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5d3a6",
   "metadata": {},
   "source": [
    "## Loading Sample Data\n",
    "We'll use some academic papers about RAG as our test data. This will help us evaluate our system with real-world content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "# Load 3 papers about RAG\n",
    "base_docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=3).load()\n",
    "print(f\"Loaded {len(base_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7df32",
   "metadata": {},
   "source": [
    "## Creating a Simple RAG System\n",
    "Let's build a basic RAG system that we can evaluate:\n",
    "1. Split documents into smaller chunks\n",
    "2. Create embeddings for these chunks\n",
    "3. Store them in a vector database\n",
    "4. Set up a retriever to find relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dadbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
    "docs = text_splitter.split_documents(base_docs)\n",
    "\n",
    "# Create vector store with embeddings\n",
    "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n",
    "\n",
    "# Create retriever\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9325d1",
   "metadata": {},
   "source": [
    "## Setting Up the QA Chain\n",
    "Now we'll create a simple question-answering chain that:\n",
    "1. Takes a question\n",
    "2. Retrieves relevant context\n",
    "3. Generates an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60df491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create QA chain\n",
    "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34a897",
   "metadata": {},
   "source": [
    "## Creating Test Questions\n",
    "We'll create some test questions to evaluate our RAG system. In a real scenario, you'd want more questions, but we'll keep it simple for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bbe5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What is Retrieval Augmented Generation?\",\n",
    "    \"How does RAG improve language models?\",\n",
    "    \"What are the main components of a RAG system?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18cc8b",
   "metadata": {},
   "source": [
    "## Evaluating with RAGAS\n",
    "Now we'll use RAGAS to evaluate our system. RAGAS provides several metrics:\n",
    "- Answer Relevancy: How relevant is the answer to the question?\n",
    "- Faithfulness: Does the answer stay true to the retrieved context?\n",
    "- Context Relevancy: How relevant is the retrieved context to the question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb575889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_relevancy\n",
    ")\n",
    "from ragas import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create evaluation dataset\n",
    "eval_dataset = []\n",
    "for question in tqdm(test_questions):\n",
    "    result = retrieval_augmented_qa_chain.invoke({\"question\": question})\n",
    "    eval_dataset.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": result[\"response\"].content,\n",
    "        \"contexts\": [context.page_content for context in result[\"context\"]],\n",
    "        \"ground_truths\": [\"TODO: Add ground truth answers\"]  # In a real scenario, you'd have human-verified answers\n",
    "    })\n",
    "\n",
    "# Evaluate\n",
    "result = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_relevancy\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d07e1e",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "Let's look at what our evaluation tells us about our RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9855dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation Results:\")\n",
    "for metric, score in result.items():\n",
    "    print(f\"{metric}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eee7e45",
   "metadata": {},
   "source": [
    "## What Do These Scores Mean?\n",
    "- Answer Relevancy (0-1): Higher is better. Shows how well the answer matches the question.\n",
    "- Faithfulness (0-1): Higher is better. Shows if the answer is based on the retrieved context.\n",
    "- Context Relevancy (0-1): Higher is better. Shows if we retrieved the right information.\n",
    "\n",
    "## Next Steps\n",
    "To improve your RAG system, you could:\n",
    "1. Try different chunk sizes\n",
    "2. Use different embedding models\n",
    "3. Adjust the number of retrieved documents\n",
    "4. Improve the prompt template\n",
    "\n",
    "## Additional Resources\n",
    "- [RAGAS Documentation](https://docs.ragas.io/)\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
