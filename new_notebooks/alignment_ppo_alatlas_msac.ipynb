{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9470347d",
   "metadata": {},
   "source": [
    "# Training a Happy/Positive LLM with PPO\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/alignment_ppo_alatlas_msac.ipynb)\n",
    "\n",
    "Estimated time needed: **1** hour on a free T4 (Google Colab)\n",
    "\n",
    "\n",
    "Imagine you're an AI engineer building LLM that is super cheerful (\"Happy LLM\") \n",
    "\n",
    "You don't tell them exactly what to say. Instead, you let them **learn by trial and error** â€” this is **Reinforcement Learning (RL)**.  \n",
    "The LLM acts (outputs text), a **reward model** scores it (positive/negative sentiment), and the LLM improves over time.\n",
    "\n",
    "#### What is Reinforcement Learning (RL)?\n",
    "\n",
    "Reinforcement Learning is a branch of machine learning where agents learn by interacting with an environment and receiving feedback in the form of rewards or penalties.  \n",
    "Unlike supervised learning (labeled examples), RL relies on **exploration** and **learning from consequences**.\n",
    "\n",
    "In this setup:  \n",
    "- **Agent** = the LLM (Large Language Model)  \n",
    "- **Environment** = the text generation task  \n",
    "- **Action** = the generated text  \n",
    "- **Reward** = score from a sentiment classifier\n",
    "\n",
    "<img src='https://superagi.com/wp-content/uploads/2024/03/Untitled-2.png.webp' width='600'>\n",
    "\n",
    "\n",
    "#### What is PPO?\n",
    "\n",
    "**Proximal Policy Optimization (PPO)** is an RL algorithm created by OpenAI that allows stable, efficient policy updates.  \n",
    "It keeps updates **gentle** (no big jumps) to avoid breaking the learning process.\n",
    "\n",
    "#### How the Reward Model Works?\n",
    "\n",
    "You use a **sentiment classifier** (trained on the IMDb movie review dataset) to score generated text:  \n",
    "- Positive text â†’ big reward for Happy LLM!  \n",
    "\n",
    "In other words, the classifier **judges** the LLM outputs and converts sentiment into a **numerical reward**.\n",
    "\n",
    "\n",
    "\n",
    "#### PPO Training Steps\n",
    "\n",
    "1. **Collect Rollouts:**  \n",
    "   Let the model generate text, record states, actions, rewards.\n",
    "\n",
    "2. **Compute Advantages:**  \n",
    "   How much better was an action compared to expected?\n",
    "\n",
    "3. **Policy Update:**  \n",
    "   Use loss to gently improve policy.\n",
    "\n",
    "4. **Value Update:**  \n",
    "   Improve the model's predictions of expected rewards.\n",
    "\n",
    "5. **Entropy Regularization:**  \n",
    "   Encourage exploration by rewarding randomness.\n",
    "\n",
    "6. **Repeat:**  \n",
    "   Across mini-batches and epochs.\n",
    "   \n",
    "<img src='https://superagi.com/wp-content/uploads/2024/03/Untitled-3.png.webp' width='600'>\n",
    "\n",
    "\n",
    "#### In This Lab\n",
    "\n",
    "You will fine-tune  Al-Atlas-0.5B to generate **positive things** using PPO, following the Hugging Face example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6542f04",
   "metadata": {},
   "source": [
    "### Setup experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff025a",
   "metadata": {},
   "source": [
    "- Intall dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d26abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --q transformers trl==0.11 wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbbc0a",
   "metadata": {},
   "source": [
    "- Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae6f4b",
   "metadata": {},
   "source": [
    "- Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9bcdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"atlasia/Al-Atlas-0.5B\" # Model to finetune and also its own reference and tokenizer\n",
    "DATASET_NAME = \"AbderrahmanSkiredj1/MSAC_darija_sentiment_analysis\" # Dataset to finetune on\n",
    "REWARD_MODEL = \"Davlan/afrisenti-twitter-sentiment-afroxlmr-large\" # Reward model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # set device to cuda if available\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32 # set dtype to fp16 if cuda is available\n",
    "\n",
    "# Set the huggingface token\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_API_KEY\" #\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72081a13",
   "metadata": {},
   "source": [
    "### Load data and models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a45fd0",
   "metadata": {},
   "source": [
    "- Load pre-trained [Atlas AI 0.5 B model](https://huggingface.co/atlasia/Al-Atlas-0.5B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92156df9",
   "metadata": {},
   "source": [
    "**Al-Atlas** is a 0.5B parameter language model specifically trained on **Moroccan Darija**, making it the first dedicated foundation model for Morocco's primary spoken dialect. The model was finetuned from **Qwen-2.5** and trained on a carefully curated dataset of **155M tokens**, focusing exclusively on authentic Moroccan Darija content.\n",
    "\n",
    "We load the model with a value head and the tokenizer. \n",
    "We load the model twice; the first model is optimized while the second model serves as **a reference** to calculate the KL-divergence from the starting point. This serves as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a3caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> model is loaded from 'atlasia/Al-Atlas-0.5B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "WARNING:root:A <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> model is loaded from 'atlasia/Al-Atlas-0.5B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "# Model/Reference Model\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL,torch_dtype=dtype)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL,torch_dtype=dtype)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061f7b1",
   "metadata": {},
   "source": [
    "- Load pre-trained Reward Model afrisenti-twitter-sentiment-afroxlmr-large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0b0ad",
   "metadata": {},
   "source": [
    "afrisenti-twitter-sentiment-afroxlmr-large is a multilingual twitter sentiment classification model for twelve  languages including Moroccan Darija based on a fine-tuned castorini/afriberta_large large model.\n",
    "The model has been trained to classify tweets into 3 sentiment classes: negative, neutral and positive Specifically, this model is a Davlan/afro-xlmr-large model that was fine-tuned on an aggregation of 12 African language datasets obtained from AfriSenti dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5661fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes labels:  {0: 'positive', 1: 'neutral', 2: 'negative'}\n"
     ]
    }
   ],
   "source": [
    "# Load reward model in sentiment analysis pipeline\n",
    "# This configures your sentiment pipeline run in batches of 16, return raw logits for all sentiment classes and Skip applying softmax\n",
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\":16 }\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\", model=REWARD_MODEL, device=device,torch_dtype=dtype,\n",
    "      **sent_kwargs\n",
    ")\n",
    "print(\"classes labels: \",sentiment_pipe.model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194670c8",
   "metadata": {},
   "source": [
    "### Load [MSAC](https://huggingface.co/datasets/AbderrahmanSkiredj1/MSAC_darija_sentiment_analysis) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92b041",
   "metadata": {},
   "source": [
    "The Moroccan Sentiment Analysis Corpus is a dataset composed of 2,000 tweets written in Maghrebi Arabic (Darija), specifically Moroccan dialect, collected from Twitter. Each entry in the corpus is typically annotated with a sentiment label (e.g., pos(for positive), neg(for negative), neu (neutral)), making it suitable for training and evaluating sentiment analysis models tailored to the unique linguistic characteristics of Moroccan Arabic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd31a65",
   "metadata": {},
   "source": [
    "- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75880e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(sample):\n",
    "    \"\"\" map the labels to 0 and 1 \"\"\"\n",
    "    label = sample[\"label\"]\n",
    "    sample[\"label\"] = 1 if label == \"pos\" else 0\n",
    "    return sample\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    input_min_text_length=4,\n",
    "    input_max_text_length=12,\n",
    "    tokenizer = tokenizer\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "    \"\"\"\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.map(map_labels)\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.shuffle(seed=42)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e51f4d",
   "metadata": {},
   "source": [
    "Using a ```LengthSampler``` to sample different text lengths during data processing introduces variability, making the model more robust and capable of handling varying input lengths in real-world scenarios. This approach prevents overfitting by exposing the model to diverse input sizes, improving generalization to new data. It also ensures efficient training by managing the length of text inputs, maintaining practicality and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12499c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 19596.94 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 18487.29 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 3272.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "dataset = build_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae17b6",
   "metadata": {},
   "source": [
    "- Collator\n",
    "\n",
    "The collator function is crucial for preparing data batches in a format suitable for the PPOTrainer. It ensures that each feature from the data samples is grouped together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d72387",
   "metadata": {},
   "source": [
    "##### Test the reward model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fa44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'negative', 'score': 2.515625},\n",
       "  {'label': 'neutral', 'score': 0.2392578125},\n",
       "  {'label': 'positive', 'score': -3.171875}]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# positive text\n",
    "text = \"Ø·Ø§Ø¨Ø¹Ø§Ù† Ø±Ø§Ù‡ Ù…ÙƒØªØ§Ø¡Ø¨!\"\n",
    "sentiment_pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1108f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'positive', 'score': 2.953125},\n",
       "  {'label': 'negative', 'score': -1.0234375},\n",
       "  {'label': 'neutral', 'score': -1.6875}]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# negative text\n",
    "text = \"Ø·Ø§Ø¨Ø¹Ø§Ù† Ø±Ø§Ù‡ ÙØ±Ø­Ø§Ù†!\"\n",
    "sentiment_pipe(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267bbf71",
   "metadata": {},
   "source": [
    "### Initialize PPOTrainer\n",
    "The `PPOTrainer` takes care of device placement and optimization later on:\n",
    "\n",
    "- ```config``` : Configuration settings for PPO training, such as learning rate and model name\n",
    "- ```model``` : The primary model to be fine-tuned using PPO\n",
    "- ```ref_model``` : The reference model to compare with model\n",
    "- ```tokenizer```:Tokenizer corresponding to the model, used for processing input text\n",
    "- ```dataset```:  Dataset to be used for training, providing the input data for the model\n",
    "- ```data_collator```: Data collator to handle batching and formatting of the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=MODEL, # the model name to be trained\n",
    "    learning_rate=1.41e-5, # the learning rate for the optimizer\n",
    "    log_with=\"wandb\",   # the logging method to be used\n",
    "    batch_size=32,  # the batch size for training\n",
    "    mini_batch_size=32,    # the mini batch size for PPO\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f356a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mafaf\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/afafelwafi/HackAI/wandb/run-20250429_142603-0o14les3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/afaf/trl/runs/0o14les3' target=\"_blank\">serene-hill-56</a></strong> to <a href='https://wandb.ai/afaf/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/afaf/trl' target=\"_blank\">https://wandb.ai/afaf/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/afaf/trl/runs/0o14les3' target=\"_blank\">https://wandb.ai/afaf/trl/runs/0o14les3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f0242",
   "metadata": {},
   "source": [
    "### Generation settings\n",
    "```generation_kwargs``` defines generation parameters used when calling a language model (like a LLM) for text generation. The c configuration below generates fully sampled, unconstrained output â€” no top-k or top-p restrictions, and with maximum diversity/randomness. It's good for creative generation, but can produce less coherent or less controlled results. (https://huggingface.co/docs/transformers/main_classes/text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f682d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed43f0",
   "metadata": {},
   "source": [
    "### Optimize model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a2b9b",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e116a7",
   "metadata": {},
   "source": [
    "The training loop consists of the following main steps:\n",
    "1. Get the query responses from the policy network (Al-Atlas-0.5B)\n",
    "2. Get sentiments for query/responses from afrisenti-twitter-sentiment-afroxlmr-large\n",
    "3. Optimize policy with PPO using the (query, response, reward) triplet\n",
    "\n",
    "**Training time**\n",
    "\n",
    "This step takes **~20mins** on a RTX 3070 i with the above specified settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5fb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      " 13%|â–ˆâ–        | 8/62 [02:09<14:38, 16.26s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [13:45<00:00, 13.31s/it]\n"
     ]
    }
   ],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "# same objective as the input length \n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "\n",
    "for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        query_response = ppo_trainer.generate(query, **generation_kwargs).squeeze().to(device)\n",
    "        response_len = len(query_response) - len(query)\n",
    "        response_tensors.append(query_response[-response_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    pipe_outputs = sentiment_pipe(batch[\"response\"])\n",
    "    positive_scores = [\n",
    "        item[\"score\"]\n",
    "        for output in pipe_outputs\n",
    "        for item in output\n",
    "        if item[\"label\"] == \"positive\"\n",
    "    ]\n",
    "    rewards = [torch.tensor(score) for score in positive_scores]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0bc717",
   "metadata": {},
   "source": [
    "## Model inspection\n",
    "Let's inspect some examples from the IMDB dataset. We can use `ref_model` to compare the tuned model `model` against the model before optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e341e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Ø£ØºØ§Ù†ÙŠÙƒ ÙˆØ®Ø§Ù…Ø© ØµÙˆØªÙƒ</td>\n",
       "      <td>1</td>\n",
       "      <td>'Ø£ØºØ§Ù†ÙŠÙƒ ÙˆØ®Ø§Ù…Ø© ØµÙˆØªÙƒ Ø±Ø§Ø¦Ø¹Ø© Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ'</td>\n",
       "      <td>Ø§Ù„ÙŠÙˆÙ… Ù…Ø§ ØºØ§Ø¯ÙŠØ´ ØªÙ‡Ù†Ø§Ùˆ' ØªØ­Ø±ÙƒØ§Øª Ø§Ù„Ø±Ø§Ù‚ØµØ© Ù…Ù†Ø§Ù„ Ø¨</td>\n",
       "      <td>' Ù‡ÙŠ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆØ¹Ø¬Ø¨Ø§ØªÙƒÙ…ØŸ ğŸ‘‡</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>5.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Ùˆ Ù…Ù† Ø¹Ù…Ù‚ Ø§Ù„Ù‚Ù„Ø¨ Ø§ØªÙ…Ù†</td>\n",
       "      <td>1</td>\n",
       "      <td>'Ùˆ Ù…Ù† Ø¹Ù…Ù‚ Ø§Ù„Ù‚Ù„Ø¨ Ø§ØªÙ…Ù†Ø§Ù„Ùƒ Ø§Ù„ØªÙˆÙÛŒÙ‚ Ø§Ù„Ø¯Ø§Ø¦Ù… ÙÙŠ Ø§Ù„Ø­ÛŒ...</td>\n",
       "      <td>Ù‰ ØªÙƒÙˆÙ† Ø§Ù„Ù…Ø´Ø© Ù…Ù†Ø§Ø¶Ù„Ø©!' 'Ø§Ù„ØºØ²Ø§Ù„Ø© Ø¯ÙŠØ§Ù„ Ø§Ù„Ø³Ù…ÙŠØ©' Ù‡Ø§Ø¯</td>\n",
       "      <td>Ù‰' ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜â¤â¤</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>5.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'ÙÙŠÙ† Ù‡Ù…Ø§ Ø§Ù„Ù…ÙØ³Ø¯ÙŠÙ† Ù„ÙŠ</td>\n",
       "      <td>0</td>\n",
       "      <td>'ÙÙŠÙ† Ù‡Ù…Ø§ Ø§Ù„Ù…ÙØ³Ø¯ÙŠÙ† Ù„ÙŠ Ù‚Ø§Ù„ Ù„ÙŠÙƒ ØºØ§Ø¯ÙŠ ÙŠØ­Ø§Ø±Ø¨Ù‡Ù… ØŸ'</td>\n",
       "      <td>ÙƒÙŠØ¸Ù† Ø§Ù„ÙƒÙ„  Ø£Ù†Ù‡Ù… Ø´Ø±ÙØ§Ø¡ ØŸ' \\n\\n#Sliwka #</td>\n",
       "      <td>Ù…Ø¹Ù…Ø± Ø§Ù„Ø´ÙˆØ§Ø±Ø¹' Ù‡ÙŠ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆØ¥Ù„Ù‡</td>\n",
       "      <td>-1.523438</td>\n",
       "      <td>2.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'. Ø£Ù†Ø§ Ø´Ø®ØµÙŠØ§ Ù‚Ø§Ø·</td>\n",
       "      <td>0</td>\n",
       "      <td>'. Ø£Ù†Ø§ Ø´Ø®ØµÙŠØ§ Ù‚Ø§Ø·Ø¹Øª Ø§Ù„Ù‚Ù†Ø§Ø© Ù…Ù† Ø²Ù…Ø§Ù† ÙˆÙ„Ù… ÙŠØ¹Ø¯ Ø£Ø­Ø¯ ...</td>\n",
       "      <td>Ø¹ØªÙˆ Ùˆ Ù‚Ù„Øª Ù…Ø§Ø²Ø§Ù„ Ù…Ø§Ø¬Ø§Ø´ (Ø­ÙŠØª ÙƒÙ†Øª ÙˆØ§ØµÙ„ ÙÙŠ Ø§Ù…</td>\n",
       "      <td>Ø¹ Ø§Ù„Ø¹Ù†Ù ÙƒØ§Ù…Ù„ ğŸ˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘âœŒï¸âœŒï¸</td>\n",
       "      <td>-2.031250</td>\n",
       "      <td>-0.777344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Ø²Ø¨Ù† Ù…Øµ</td>\n",
       "      <td>0</td>\n",
       "      <td>'Ø²Ø¨Ù† Ù…ØµØ·Ù†Ø¹ ØºØ±ÙˆØ± ÙˆÙ†Ø®ÙˆØ© Ø¹Ù„Ù‰ Ù„Ø®ÙˆØ© ÙˆØ§ÙƒÙˆØ§Ùƒ'</td>\n",
       "      <td>Ø±ÙŠ ÙˆØ§Ø­Ø¯ ÙƒØ§ÙŠØ¹Ø¨Ø± Ø¨ÙŠÙ‡ Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Øª.' ÙˆØ§Ø´ Ù‡Ø§Ø¯ Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒØªØ¹</td>\n",
       "      <td>Ø±ÙŠ' Ù‡ÙŠ Ù‚ØµØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆØ¹Ù‚ÙˆØ¨Ø© Ù…</td>\n",
       "      <td>3.515625</td>\n",
       "      <td>-0.882812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>' Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø¹Ø§Øª Ø§Ù„Ù…Øº</td>\n",
       "      <td>0</td>\n",
       "      <td>' Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø¹Ø§Øª Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© ØªØ¹Ø§Ù†ÙŠ Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù…Ø´ÙƒÙ„ . '</td>\n",
       "      <td>Ø±Ø¨ÙŠØ© Ø¨Ø¹Ø¯Ø§ ØµØ­Ø§Ùˆ ÙÙ‚Ø±Ø§Ø±Ù‡Ù… Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®ØŒ ÙˆÙ‚Ø±Ø± Ø§Ù„Ø±Ø¨Ø§Ø·Ù† ...</td>\n",
       "      <td>Ø±Ø¨ÙŠØ©' Ù‡ÙŠ Ù…Ù†ØµØ© Ù…Ø§Ù†Ø´ÙŠØ§Ù„ Ø¨Ù„Ø§ Ù…Ø´Ø§ÙƒÙ„ Ù†</td>\n",
       "      <td>-1.554688</td>\n",
       "      <td>3.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'ÙƒØ¹Ø§Ø¯ØªÙ‡Ø§ Ø§Ù„Ø¯ÙˆÙ„Ø© Ù„Ø§ØªØ­Ù‚Ù‚ ÙÙŠ</td>\n",
       "      <td>0</td>\n",
       "      <td>'ÙƒØ¹Ø§Ø¯ØªÙ‡Ø§ Ø§Ù„Ø¯ÙˆÙ„Ø© Ù„Ø§ØªØ­Ù‚Ù‚ ÙÙŠ Ø§Ù„Ø´ÙƒØ§ÙŠØ§Øª Ø­ØªÙ‰ ØªØ³Ù‚Ø· Ø§Ù„...</td>\n",
       "      <td>Ù‚Ø¶ÙŠØ© ÙÙ‚Ø·' Ù…Ù† Ù‚ØµØµ ØªØ§Ø±ÙŠØ® Ù„ÙŠ Ø¯Ø®Ù„Ø§Øª ÙˆØ³Ø·ÙŠ ÙØ§Ù„ØªÙ‚Ø¯ÙŠ</td>\n",
       "      <td>Ù…ÙˆØªÙ†Ø§ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ ÙÙ†ÙŠØ³ÙŠØ§Ù† Ø§ØµØ¨Ø­</td>\n",
       "      <td>-0.089355</td>\n",
       "      <td>-0.255859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'Ù„Ø¯ÙˆØ§Ø¹ÙŠ Ø§</td>\n",
       "      <td>0</td>\n",
       "      <td>'Ù„Ø¯ÙˆØ§Ø¹ÙŠ Ø§Ù…Ù†ÙŠØ©.ØªØ¹Ù†ÙŠ ØªÙØ§Ø¯ÙŠ Ù‡Ù…Ø¬ÙŠØ© Ø¨Ø¹Ø¶ Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„Ø¶Ø¹...</td>\n",
       "      <td>Ù†ØªÙ‚Ø§Ù…ÙŠØ© Ù…Ù† Ø§Ù„Ø³Ù„Ø·Ø© Ø§Ù„Ø³Ù„Ø·Ø§Ù†ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„ØªÙŠ ØªØ¨Ø¯...</td>\n",
       "      <td>Ø®ÙˆÙŠØ©' ğŸ˜ğŸ˜â¤ï¸â¤ï¸â¤ï¸â¤ï¸</td>\n",
       "      <td>-1.164062</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'Ø¥Ù‚Ø·Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¬Ø²</td>\n",
       "      <td>0</td>\n",
       "      <td>'Ø¥Ù‚Ø·Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± ÙŠØ§ Ø¨Ù† ÙƒÙŠØ±Ø§Ù†'</td>\n",
       "      <td>ÙŠØ±Ø©'ØŒ Ø§Ù„Ù„ÙŠ ÙƒØªÙ…Ø«Ù„ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ù‚Ø¯ØŒ ÙƒØªÙ…Ø«Ù„ Ø§Ù„ØªÙØ§ÙˆØ¶ Ø§Ù„Ù…...</td>\n",
       "      <td>ÙŠØ±Ø©' Ù‡Ùˆ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆÙ…Ø³Ø±</td>\n",
       "      <td>-1.523438</td>\n",
       "      <td>3.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'ÙÙŠÙ…Ø§ Ù‚Ø±ÙŠØ¨ Ø³Ù†</td>\n",
       "      <td>0</td>\n",
       "      <td>'ÙÙŠÙ…Ø§ Ù‚Ø±ÙŠØ¨ Ø³Ù†Ø¬Ø¯ Ø£Ù†ÙØ³Ù†Ø§ ÙÙŠ ÙˆØ¶Ø¹ ÙƒÙˆØ¶Ø¹ Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ø«Ù†Ø§...</td>\n",
       "      <td>ÙˆÙ‚Ù Ø§Ù„ØªÙØ§Ø­ Ø¨Ø§Ù„Ø¯Ø±Ø¨ Ø§Ùˆ Ø²Ù„ÙŠÙÙŠ ÙƒØ§Ø²Ø§Ø¨Ù„Ø§Ù†ÙƒØ§ Ù†Ù…Ø³Ø­ Ø¨Ù„Ø§</td>\n",
       "      <td>Ø¯ÙŠ' Ù‡ÙŠ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆØ´Ø®ØµÙŠØ© Ù…Ø«Ø§Ù„ÙŠØ©</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'Ù…Ø§ÙƒÙŠÙ† ØºÙŠØ± Ø§Ù„Ù†ÙØ§Ù‚ Ù„ÙŠ Ø®Ø¯Ø§Ù… Ùˆ</td>\n",
       "      <td>0</td>\n",
       "      <td>'Ù…Ø§ÙƒÙŠÙ† ØºÙŠØ± Ø§Ù„Ù†ÙØ§Ù‚ Ù„ÙŠ Ø®Ø¯Ø§Ù… Ùˆ ØµØ§ÙÙŠ Ù„Ø§Ù‡Ùˆ Ù„Ø§ ØºÙŠØ±Ùˆ\"'</td>\n",
       "      <td>Ø§Ø­Ù„'ØŒ Ù…Ø§ÙŠØ¬ÙŠÙƒØ´ ÙƒØ­Ù„Ø© ÙÙ‡Ø§Ø¯ Ø§Ù„ÙŠÙˆÙ…Ø§ÙŠÙ† !!! Ù…ÙƒØ§ÙŠÙ†</td>\n",
       "      <td>Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ Ø§Ù„Ø®Ø§Ù†Ø²Ø©' Ù‡ÙŠ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø©</td>\n",
       "      <td>-0.605469</td>\n",
       "      <td>0.816406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'Ø£Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ø¹Ù„ÙŠ Ø§Ù„Ù‚Ø¯ÙŠØ± Ø£Ù†</td>\n",
       "      <td>0</td>\n",
       "      <td>'Ø£Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ø¹Ù„ÙŠ Ø§Ù„Ù‚Ø¯ÙŠØ± Ø£Ù† ÙŠÙ†ØªÙ‚Ù… Ù…Ù† Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ù†Ø§Ø³ ...</td>\n",
       "      <td>ÙŠÙƒØ±Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¯Ø§Ø¯ Ù…Ù† Ø£Ù„Ù‚Ø¨Ø¯ Ø§Ù„ÙˆØ·Ù† Ø­ÙŠØ« Ø£ØªØ¹Ø¨ Ø¹Ø§Ù„Ù„Ù‡'...</td>\n",
       "      <td>ÙŠØ­ÙØ¸Ùƒ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¬Ù…ÙŠÙ„Ø©' ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜</td>\n",
       "      <td>2.718750</td>\n",
       "      <td>5.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>'ÙˆØ§Ù„Ø¯ÙˆÙ„Ø© Ù…Ø³Ø§ÙŠØ±Ù‡Ù… ÙØ®Ø·Ø·</td>\n",
       "      <td>0</td>\n",
       "      <td>'ÙˆØ§Ù„Ø¯ÙˆÙ„Ø© Ù…Ø³Ø§ÙŠØ±Ù‡Ù… ÙØ®Ø·Ø·Ù‡Ù… Ø§Ù„Ø¬Ù‡Ù†Ù…ÙŠØ©'</td>\n",
       "      <td>Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬..' ÙØ§Ù„ØµØ­Ø§ÙØ© Ø®Ø±Ø¬Ùˆ Ø§Ø±Ù‚Ø§Ù… Ù…ÙØ±ØºØ© Ø¹Ù† Ø§Ù„ØªØ¹...</td>\n",
       "      <td>Ù‡Ù…' ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ ğŸ˜ğŸ˜ğŸ˜ğŸ˜</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>3.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'Ù‡Ø¯ÙŠ Ø¨Ù„Ø§</td>\n",
       "      <td>1</td>\n",
       "      <td>'Ù‡Ø¯ÙŠ Ø¨Ù„Ø§ØµÙ‡ ÙÙŠÙ† Ø²ÙˆÙŠÙ†Ø© Ù†Ø«Ù…Ù†Ø§ Ø´ÙˆÙÙ‡Ø§ Ù‚Ø±ÙŠØ¨Ø§ Ø§Ù† Ø´Ø§Ø¡ ...</td>\n",
       "      <td>ØµØ© Ù…Ø®ØªÙ„ÙØ© Ø¹Ù„Ù‰ ÙÙŠÙ† ÙƒØ§ÙŠÙ† ÙˆØ¬Ù‡ÙƒØŒ ÙˆÙ„ÙƒÙ† Ù…Ø§Ø²Ø§Ù„ ØªÙ‚Ø¯Ø± Øª...</td>\n",
       "      <td>ØµØªÙŠ' Ù‡ÙŠ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆÙ…Ø¹Ø¨Ø±Ø© Ù„Ø£ÙŠ ÙˆØ§Ø­Ø¯ Ø­Ù‚ÙŠÙ‚ÙŠ</td>\n",
       "      <td>2.921875</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'Ø£Ø´Ø§Ø¯ ÙŠØ´ÙŠØ¯ Ù†Ø­ØªØ¬ Ù†</td>\n",
       "      <td>0</td>\n",
       "      <td>'Ø£Ø´Ø§Ø¯ ÙŠØ´ÙŠØ¯ Ù†Ø­ØªØ¬ Ù†Ø³ØªÙ†ÙƒØ± Ù†ØºØ¶Ø¨ Ø®Ø· Ø£Ø­Ù…Ø± Ø®Ø· Ø£Ø²Ø±Ù‚ Ø¨Ø±...</td>\n",
       "      <td>Ø­ØªØ¬! Ø²ÙŠ Ù…Ø§ Ø¬Ø§Ø¯ Ù†Ø¯ÙŠØ± ÙˆØ¬Ø§ ÙˆØ§Ø¬Ø¯Øª' ÙƒØ§ØªÙƒÙŠ Ùˆ ÙŠ</td>\n",
       "      <td>Ø­ØªØ±Ù…' ğŸ˜ğŸ˜â¤â¤â¤â¤â¤â¤</td>\n",
       "      <td>2.546875</td>\n",
       "      <td>5.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'Ù„Ø§Ø­ÙˆÙ„Ø§ Ùˆ Ù„Ø§ Ù‚ÙˆØ© Ø§Ù„Ø§ Ø¨Ø§Ù„Ù„Ù‡.</td>\n",
       "      <td>0</td>\n",
       "      <td>'Ù„Ø§Ø­ÙˆÙ„Ø§ Ùˆ Ù„Ø§ Ù‚ÙˆØ© Ø§Ù„Ø§ Ø¨Ø§Ù„Ù„Ù‡. Ø§Ù„Ù„Ù‡Ù… ÙŠØ§Ø±Ø¨ Ø§Ø¹Ù† Ù‡Ø°Ù‡...</td>\n",
       "      <td>Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡ 13 ÙŠÙ†Ø§ÙŠØ± 2013 Ø¹Ù†Ø§ØµØ± Ø§Ù„ÙØ±Ù‚Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ù„...</td>\n",
       "      <td>ÙˆØ§Ø¹Ø±ÙÙŠÙ† Ø¨Ø±Ø­ÙŠÙ„ Ù‚Ø¯Ùˆ Ù…Ø¹ÙƒØ±ÙˆØ§</td>\n",
       "      <td>-0.511719</td>\n",
       "      <td>-1.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>'Ø§Ù„ÙˆØ§Ù‚Ø¹ ÙˆÙ‡ÙˆØ£Ù† Ù‡Ø¤Ù„Ø§Ø¡ Ø¯</td>\n",
       "      <td>0</td>\n",
       "      <td>'Ø§Ù„ÙˆØ§Ù‚Ø¹ ÙˆÙ‡ÙˆØ£Ù† Ù‡Ø¤Ù„Ø§Ø¡ Ø¯Ø®Ù„ÙˆØ§ Ù…Ø³ØªÙ†Ù‚Ø¹Ø§ Ù„Ù† ÙŠØ®Ø±Ø¬ÙˆØ§ Ù…Ù†...</td>\n",
       "      <td>Ú¤Ø§ Ø¹Ø§Ø±ÙÙŠÙ† ic Ùˆ iiniØŒ ÙˆÙ‚Ø§Ù„Ùˆ Ù„ÙŠÙ‡Ù… ÙˆÙ‚ÙÙˆØ§ Ù…Ù† Ø§Ù„Ø¶Ø­Ùƒ</td>\n",
       "      <td>Ù…Ø§Ø¡ Ø¬Ù…ÙŠÙ„Ø©' ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜</td>\n",
       "      <td>-1.242188</td>\n",
       "      <td>4.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>'Ø§Ù„Ø¹Ø§Ù„Ù… ÙŠØªØ¬</td>\n",
       "      <td>0</td>\n",
       "      <td>'Ø§Ù„Ø¹Ø§Ù„Ù… ÙŠØªØ¬Ù‡ Ù†Ø­Ùˆ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ùˆ Ù†Ø­Ù† Ù†ØªØ¬Ù‡ Ù†Ø­Ùˆ Ø§Ù„Ù…Ø§Ø¶ÙŠ'</td>\n",
       "      <td>Ù‡ Ø¥Ù„Ù‰ Ù…Ù†Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø¹Ø§Ø¯Ù† Ø§Ù„Ù…Ø¹Ø¯Ù†ÙŠØ© Ø§Ù„Ù…Ø«ÙŠØ±Ø© Ù„Ù„...</td>\n",
       "      <td>Ø¯Ø¯' ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜</td>\n",
       "      <td>-0.244141</td>\n",
       "      <td>4.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>'Ø§Ù†Ø§ Ø£Ø­Ø¨ Ø¯ÙŠÙ†</td>\n",
       "      <td>1</td>\n",
       "      <td>'Ø§Ù†Ø§ Ø£Ø­Ø¨ Ø¯ÙŠÙ† Ø§Ù„Ø£Ø³Ù„Ø§Ù… ÙˆØ£Ø³ØªÙ…Ø¹ Ù„Ù„Ø¹Ù„Ù…Ø§Ø¡ Ø§Ù„Ù…Ø³Ù„Ù…ÙˆÙ†'</td>\n",
       "      <td>Ø§Ù„Ù„Ù‡' (Ø§Ù„Ø§Ù†ÙˆÙ†ØªÙŠ) Ù‡ÙŠ ÙˆØ­Ø¯Ø© Ù…Ù† Ø£Ø´Ù‡Ø± Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ§Øª Ø§...</td>\n",
       "      <td>Ø§Ù„Ù„Ù‡' - â€«Ø§Ù„Ù„Ù‡ ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ø§Ù„Ù‚ÙŠÙŠØ³ Ø¹Ù…Ø±ÙŠ</td>\n",
       "      <td>1.929688</td>\n",
       "      <td>3.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>'ÙƒÙ„Ø§Ù… Ø¹Ø¸ÙŠÙ…'</td>\n",
       "      <td>1</td>\n",
       "      <td>'ÙƒÙ„Ø§Ù… Ø¹Ø¸ÙŠÙ…'</td>\n",
       "      <td>ÙØ¨Ø±Ù…Ø¬Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (AI) Ù‡Ùˆ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„ØªØ¹Ù„Ù…</td>\n",
       "      <td>Ù‡ÙŠ Ø¹Ù…Ù„ÙŠØ© ÙÙ†ÙŠØ© Ø±Ø§Ø¦Ø¹Ø© Ø¬Ø¯Ø§Ù‹\\nÙˆÙ„Ø© Ø¨ØµØ§Ø­</td>\n",
       "      <td>3.906250</td>\n",
       "      <td>3.718750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          query  label  \\\n",
       "0            'Ø£ØºØ§Ù†ÙŠÙƒ ÙˆØ®Ø§Ù…Ø© ØµÙˆØªÙƒ      1   \n",
       "1          'Ùˆ Ù…Ù† Ø¹Ù…Ù‚ Ø§Ù„Ù‚Ù„Ø¨ Ø§ØªÙ…Ù†      1   \n",
       "2          'ÙÙŠÙ† Ù‡Ù…Ø§ Ø§Ù„Ù…ÙØ³Ø¯ÙŠÙ† Ù„ÙŠ      0   \n",
       "3              '. Ø£Ù†Ø§ Ø´Ø®ØµÙŠØ§ Ù‚Ø§Ø·      0   \n",
       "4                       'Ø²Ø¨Ù† Ù…Øµ      0   \n",
       "5         ' Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø¹Ø§Øª Ø§Ù„Ù…Øº      0   \n",
       "6     'ÙƒØ¹Ø§Ø¯ØªÙ‡Ø§ Ø§Ù„Ø¯ÙˆÙ„Ø© Ù„Ø§ØªØ­Ù‚Ù‚ ÙÙŠ      0   \n",
       "7                     'Ù„Ø¯ÙˆØ§Ø¹ÙŠ Ø§      0   \n",
       "8         'Ø¥Ù‚Ø·Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¬Ø²      0   \n",
       "9                 'ÙÙŠÙ…Ø§ Ù‚Ø±ÙŠØ¨ Ø³Ù†      0   \n",
       "10  'Ù…Ø§ÙƒÙŠÙ† ØºÙŠØ± Ø§Ù„Ù†ÙØ§Ù‚ Ù„ÙŠ Ø®Ø¯Ø§Ù… Ùˆ      0   \n",
       "11     'Ø£Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ø¹Ù„ÙŠ Ø§Ù„Ù‚Ø¯ÙŠØ± Ø£Ù†      0   \n",
       "12        'ÙˆØ§Ù„Ø¯ÙˆÙ„Ø© Ù…Ø³Ø§ÙŠØ±Ù‡Ù… ÙØ®Ø·Ø·      0   \n",
       "13                     'Ù‡Ø¯ÙŠ Ø¨Ù„Ø§      1   \n",
       "14            'Ø£Ø´Ø§Ø¯ ÙŠØ´ÙŠØ¯ Ù†Ø­ØªØ¬ Ù†      0   \n",
       "15  'Ù„Ø§Ø­ÙˆÙ„Ø§ Ùˆ Ù„Ø§ Ù‚ÙˆØ© Ø§Ù„Ø§ Ø¨Ø§Ù„Ù„Ù‡.      0   \n",
       "16        'Ø§Ù„ÙˆØ§Ù‚Ø¹ ÙˆÙ‡ÙˆØ£Ù† Ù‡Ø¤Ù„Ø§Ø¡ Ø¯      0   \n",
       "17                  'Ø§Ù„Ø¹Ø§Ù„Ù… ÙŠØªØ¬      0   \n",
       "18                 'Ø§Ù†Ø§ Ø£Ø­Ø¨ Ø¯ÙŠÙ†      1   \n",
       "19                  'ÙƒÙ„Ø§Ù… Ø¹Ø¸ÙŠÙ…'      1   \n",
       "\n",
       "                                               review  \\\n",
       "0          'Ø£ØºØ§Ù†ÙŠÙƒ ÙˆØ®Ø§Ù…Ø© ØµÙˆØªÙƒ Ø±Ø§Ø¦Ø¹Ø© Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ'   \n",
       "1   'Ùˆ Ù…Ù† Ø¹Ù…Ù‚ Ø§Ù„Ù‚Ù„Ø¨ Ø§ØªÙ…Ù†Ø§Ù„Ùƒ Ø§Ù„ØªÙˆÙÛŒÙ‚ Ø§Ù„Ø¯Ø§Ø¦Ù… ÙÙŠ Ø§Ù„Ø­ÛŒ...   \n",
       "2        'ÙÙŠÙ† Ù‡Ù…Ø§ Ø§Ù„Ù…ÙØ³Ø¯ÙŠÙ† Ù„ÙŠ Ù‚Ø§Ù„ Ù„ÙŠÙƒ ØºØ§Ø¯ÙŠ ÙŠØ­Ø§Ø±Ø¨Ù‡Ù… ØŸ'   \n",
       "3   '. Ø£Ù†Ø§ Ø´Ø®ØµÙŠØ§ Ù‚Ø§Ø·Ø¹Øª Ø§Ù„Ù‚Ù†Ø§Ø© Ù…Ù† Ø²Ù…Ø§Ù† ÙˆÙ„Ù… ÙŠØ¹Ø¯ Ø£Ø­Ø¯ ...   \n",
       "4              'Ø²Ø¨Ù† Ù…ØµØ·Ù†Ø¹ ØºØ±ÙˆØ± ÙˆÙ†Ø®ÙˆØ© Ø¹Ù„Ù‰ Ù„Ø®ÙˆØ© ÙˆØ§ÙƒÙˆØ§Ùƒ'   \n",
       "5   ' Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø¹Ø§Øª Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© ØªØ¹Ø§Ù†ÙŠ Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù…Ø´ÙƒÙ„ . '   \n",
       "6   'ÙƒØ¹Ø§Ø¯ØªÙ‡Ø§ Ø§Ù„Ø¯ÙˆÙ„Ø© Ù„Ø§ØªØ­Ù‚Ù‚ ÙÙŠ Ø§Ù„Ø´ÙƒØ§ÙŠØ§Øª Ø­ØªÙ‰ ØªØ³Ù‚Ø· Ø§Ù„...   \n",
       "7   'Ù„Ø¯ÙˆØ§Ø¹ÙŠ Ø§Ù…Ù†ÙŠØ©.ØªØ¹Ù†ÙŠ ØªÙØ§Ø¯ÙŠ Ù‡Ù…Ø¬ÙŠØ© Ø¨Ø¹Ø¶ Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„Ø¶Ø¹...   \n",
       "8               'Ø¥Ù‚Ø·Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± ÙŠØ§ Ø¨Ù† ÙƒÙŠØ±Ø§Ù†'   \n",
       "9   'ÙÙŠÙ…Ø§ Ù‚Ø±ÙŠØ¨ Ø³Ù†Ø¬Ø¯ Ø£Ù†ÙØ³Ù†Ø§ ÙÙŠ ÙˆØ¶Ø¹ ÙƒÙˆØ¶Ø¹ Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ø«Ù†Ø§...   \n",
       "10    'Ù…Ø§ÙƒÙŠÙ† ØºÙŠØ± Ø§Ù„Ù†ÙØ§Ù‚ Ù„ÙŠ Ø®Ø¯Ø§Ù… Ùˆ ØµØ§ÙÙŠ Ù„Ø§Ù‡Ùˆ Ù„Ø§ ØºÙŠØ±Ùˆ\"'   \n",
       "11  'Ø£Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ø¹Ù„ÙŠ Ø§Ù„Ù‚Ø¯ÙŠØ± Ø£Ù† ÙŠÙ†ØªÙ‚Ù… Ù…Ù† Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ù†Ø§Ø³ ...   \n",
       "12                  'ÙˆØ§Ù„Ø¯ÙˆÙ„Ø© Ù…Ø³Ø§ÙŠØ±Ù‡Ù… ÙØ®Ø·Ø·Ù‡Ù… Ø§Ù„Ø¬Ù‡Ù†Ù…ÙŠØ©'   \n",
       "13  'Ù‡Ø¯ÙŠ Ø¨Ù„Ø§ØµÙ‡ ÙÙŠÙ† Ø²ÙˆÙŠÙ†Ø© Ù†Ø«Ù…Ù†Ø§ Ø´ÙˆÙÙ‡Ø§ Ù‚Ø±ÙŠØ¨Ø§ Ø§Ù† Ø´Ø§Ø¡ ...   \n",
       "14  'Ø£Ø´Ø§Ø¯ ÙŠØ´ÙŠØ¯ Ù†Ø­ØªØ¬ Ù†Ø³ØªÙ†ÙƒØ± Ù†ØºØ¶Ø¨ Ø®Ø· Ø£Ø­Ù…Ø± Ø®Ø· Ø£Ø²Ø±Ù‚ Ø¨Ø±...   \n",
       "15  'Ù„Ø§Ø­ÙˆÙ„Ø§ Ùˆ Ù„Ø§ Ù‚ÙˆØ© Ø§Ù„Ø§ Ø¨Ø§Ù„Ù„Ù‡. Ø§Ù„Ù„Ù‡Ù… ÙŠØ§Ø±Ø¨ Ø§Ø¹Ù† Ù‡Ø°Ù‡...   \n",
       "16  'Ø§Ù„ÙˆØ§Ù‚Ø¹ ÙˆÙ‡ÙˆØ£Ù† Ù‡Ø¤Ù„Ø§Ø¡ Ø¯Ø®Ù„ÙˆØ§ Ù…Ø³ØªÙ†Ù‚Ø¹Ø§ Ù„Ù† ÙŠØ®Ø±Ø¬ÙˆØ§ Ù…Ù†...   \n",
       "17   'Ø§Ù„Ø¹Ø§Ù„Ù… ÙŠØªØ¬Ù‡ Ù†Ø­Ùˆ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ùˆ Ù†Ø­Ù† Ù†ØªØ¬Ù‡ Ù†Ø­Ùˆ Ø§Ù„Ù…Ø§Ø¶ÙŠ'   \n",
       "18      'Ø§Ù†Ø§ Ø£Ø­Ø¨ Ø¯ÙŠÙ† Ø§Ù„Ø£Ø³Ù„Ø§Ù… ÙˆØ£Ø³ØªÙ…Ø¹ Ù„Ù„Ø¹Ù„Ù…Ø§Ø¡ Ø§Ù„Ù…Ø³Ù„Ù…ÙˆÙ†'   \n",
       "19                                        'ÙƒÙ„Ø§Ù… Ø¹Ø¸ÙŠÙ…'   \n",
       "\n",
       "                                    response (before)  \\\n",
       "0         Ø§Ù„ÙŠÙˆÙ… Ù…Ø§ ØºØ§Ø¯ÙŠØ´ ØªÙ‡Ù†Ø§Ùˆ' ØªØ­Ø±ÙƒØ§Øª Ø§Ù„Ø±Ø§Ù‚ØµØ© Ù…Ù†Ø§Ù„ Ø¨   \n",
       "1     Ù‰ ØªÙƒÙˆÙ† Ø§Ù„Ù…Ø´Ø© Ù…Ù†Ø§Ø¶Ù„Ø©!' 'Ø§Ù„ØºØ²Ø§Ù„Ø© Ø¯ÙŠØ§Ù„ Ø§Ù„Ø³Ù…ÙŠØ©' Ù‡Ø§Ø¯   \n",
       "2              ÙƒÙŠØ¸Ù† Ø§Ù„ÙƒÙ„  Ø£Ù†Ù‡Ù… Ø´Ø±ÙØ§Ø¡ ØŸ' \\n\\n#Sliwka #   \n",
       "3           Ø¹ØªÙˆ Ùˆ Ù‚Ù„Øª Ù…Ø§Ø²Ø§Ù„ Ù…Ø§Ø¬Ø§Ø´ (Ø­ÙŠØª ÙƒÙ†Øª ÙˆØ§ØµÙ„ ÙÙŠ Ø§Ù…   \n",
       "4   Ø±ÙŠ ÙˆØ§Ø­Ø¯ ÙƒØ§ÙŠØ¹Ø¨Ø± Ø¨ÙŠÙ‡ Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Øª.' ÙˆØ§Ø´ Ù‡Ø§Ø¯ Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒØªØ¹   \n",
       "5   Ø±Ø¨ÙŠØ© Ø¨Ø¹Ø¯Ø§ ØµØ­Ø§Ùˆ ÙÙ‚Ø±Ø§Ø±Ù‡Ù… Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®ØŒ ÙˆÙ‚Ø±Ø± Ø§Ù„Ø±Ø¨Ø§Ø·Ù† ...   \n",
       "6        Ù‚Ø¶ÙŠØ© ÙÙ‚Ø·' Ù…Ù† Ù‚ØµØµ ØªØ§Ø±ÙŠØ® Ù„ÙŠ Ø¯Ø®Ù„Ø§Øª ÙˆØ³Ø·ÙŠ ÙØ§Ù„ØªÙ‚Ø¯ÙŠ   \n",
       "7   Ù†ØªÙ‚Ø§Ù…ÙŠØ© Ù…Ù† Ø§Ù„Ø³Ù„Ø·Ø© Ø§Ù„Ø³Ù„Ø·Ø§Ù†ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„ØªÙŠ ØªØ¨Ø¯...   \n",
       "8   ÙŠØ±Ø©'ØŒ Ø§Ù„Ù„ÙŠ ÙƒØªÙ…Ø«Ù„ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ù‚Ø¯ØŒ ÙƒØªÙ…Ø«Ù„ Ø§Ù„ØªÙØ§ÙˆØ¶ Ø§Ù„Ù…...   \n",
       "9      ÙˆÙ‚Ù Ø§Ù„ØªÙØ§Ø­ Ø¨Ø§Ù„Ø¯Ø±Ø¨ Ø§Ùˆ Ø²Ù„ÙŠÙÙŠ ÙƒØ§Ø²Ø§Ø¨Ù„Ø§Ù†ÙƒØ§ Ù†Ù…Ø³Ø­ Ø¨Ù„Ø§   \n",
       "10         Ø§Ø­Ù„'ØŒ Ù…Ø§ÙŠØ¬ÙŠÙƒØ´ ÙƒØ­Ù„Ø© ÙÙ‡Ø§Ø¯ Ø§Ù„ÙŠÙˆÙ…Ø§ÙŠÙ† !!! Ù…ÙƒØ§ÙŠÙ†   \n",
       "11   ÙŠÙƒØ±Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¯Ø§Ø¯ Ù…Ù† Ø£Ù„Ù‚Ø¨Ø¯ Ø§Ù„ÙˆØ·Ù† Ø­ÙŠØ« Ø£ØªØ¹Ø¨ Ø¹Ø§Ù„Ù„Ù‡'...   \n",
       "12   Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬..' ÙØ§Ù„ØµØ­Ø§ÙØ© Ø®Ø±Ø¬Ùˆ Ø§Ø±Ù‚Ø§Ù… Ù…ÙØ±ØºØ© Ø¹Ù† Ø§Ù„ØªØ¹...   \n",
       "13  ØµØ© Ù…Ø®ØªÙ„ÙØ© Ø¹Ù„Ù‰ ÙÙŠÙ† ÙƒØ§ÙŠÙ† ÙˆØ¬Ù‡ÙƒØŒ ÙˆÙ„ÙƒÙ† Ù…Ø§Ø²Ø§Ù„ ØªÙ‚Ø¯Ø± Øª...   \n",
       "14           Ø­ØªØ¬! Ø²ÙŠ Ù…Ø§ Ø¬Ø§Ø¯ Ù†Ø¯ÙŠØ± ÙˆØ¬Ø§ ÙˆØ§Ø¬Ø¯Øª' ÙƒØ§ØªÙƒÙŠ Ùˆ ÙŠ   \n",
       "15   Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡ 13 ÙŠÙ†Ø§ÙŠØ± 2013 Ø¹Ù†Ø§ØµØ± Ø§Ù„ÙØ±Ù‚Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ù„...   \n",
       "16     Ú¤Ø§ Ø¹Ø§Ø±ÙÙŠÙ† ic Ùˆ iiniØŒ ÙˆÙ‚Ø§Ù„Ùˆ Ù„ÙŠÙ‡Ù… ÙˆÙ‚ÙÙˆØ§ Ù…Ù† Ø§Ù„Ø¶Ø­Ùƒ   \n",
       "17  Ù‡ Ø¥Ù„Ù‰ Ù…Ù†Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø¹Ø§Ø¯Ù† Ø§Ù„Ù…Ø¹Ø¯Ù†ÙŠØ© Ø§Ù„Ù…Ø«ÙŠØ±Ø© Ù„Ù„...   \n",
       "18   Ø§Ù„Ù„Ù‡' (Ø§Ù„Ø§Ù†ÙˆÙ†ØªÙŠ) Ù‡ÙŠ ÙˆØ­Ø¯Ø© Ù…Ù† Ø£Ø´Ù‡Ø± Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ§Øª Ø§...   \n",
       "19      ÙØ¨Ø±Ù…Ø¬Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (AI) Ù‡Ùˆ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„ØªØ¹Ù„Ù…   \n",
       "\n",
       "                             response (after)  rewards (before)  \\\n",
       "0                ' Ù‡ÙŠ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆØ¹Ø¬Ø¨Ø§ØªÙƒÙ…ØŸ ğŸ‘‡          4.750000   \n",
       "1                                Ù‰' ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜â¤â¤           4.750000   \n",
       "2           Ù…Ø¹Ù…Ø± Ø§Ù„Ø´ÙˆØ§Ø±Ø¹' Ù‡ÙŠ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆØ¥Ù„Ù‡         -1.523438   \n",
       "3                      Ø¹ Ø§Ù„Ø¹Ù†Ù ÙƒØ§Ù…Ù„ ğŸ˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘âœŒï¸âœŒï¸         -2.031250   \n",
       "4                   Ø±ÙŠ' Ù‡ÙŠ Ù‚ØµØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆØ¹Ù‚ÙˆØ¨Ø© Ù…          3.515625   \n",
       "5           Ø±Ø¨ÙŠØ©' Ù‡ÙŠ Ù…Ù†ØµØ© Ù…Ø§Ù†Ø´ÙŠØ§Ù„ Ø¨Ù„Ø§ Ù…Ø´Ø§ÙƒÙ„ Ù†         -1.554688   \n",
       "6                  Ù…ÙˆØªÙ†Ø§ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ ÙÙ†ÙŠØ³ÙŠØ§Ù† Ø§ØµØ¨Ø­         -0.089355   \n",
       "7                            Ø®ÙˆÙŠØ©' ğŸ˜ğŸ˜â¤ï¸â¤ï¸â¤ï¸â¤ï¸         -1.164062   \n",
       "8                    ÙŠØ±Ø©' Ù‡Ùˆ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆÙ…Ø³Ø±         -1.523438   \n",
       "9            Ø¯ÙŠ' Ù‡ÙŠ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆØ´Ø®ØµÙŠØ© Ù…Ø«Ø§Ù„ÙŠØ©          2.687500   \n",
       "10           Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ Ø§Ù„Ø®Ø§Ù†Ø²Ø©' Ù‡ÙŠ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø©         -0.605469   \n",
       "11                 ÙŠØ­ÙØ¸Ùƒ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¬Ù…ÙŠÙ„Ø©' ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜          2.718750   \n",
       "12                            Ù‡Ù…' ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ ğŸ˜ğŸ˜ğŸ˜ğŸ˜         -0.218750   \n",
       "13  ØµØªÙŠ' Ù‡ÙŠ Ø£ØºÙ†ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆÙ…Ø¹Ø¨Ø±Ø© Ù„Ø£ÙŠ ÙˆØ§Ø­Ø¯ Ø­Ù‚ÙŠÙ‚ÙŠ          2.921875   \n",
       "14                             Ø­ØªØ±Ù…' ğŸ˜ğŸ˜â¤â¤â¤â¤â¤â¤          2.546875   \n",
       "15                   ÙˆØ§Ø¹Ø±ÙÙŠÙ† Ø¨Ø±Ø­ÙŠÙ„ Ù‚Ø¯Ùˆ Ù…Ø¹ÙƒØ±ÙˆØ§         -0.511719   \n",
       "16                        Ù…Ø§Ø¡ Ø¬Ù…ÙŠÙ„Ø©' ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜         -1.242188   \n",
       "17                            Ø¯Ø¯' ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜         -0.244141   \n",
       "18          Ø§Ù„Ù„Ù‡' - â€«Ø§Ù„Ù„Ù‡ ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ø§Ù„Ù‚ÙŠÙŠØ³ Ø¹Ù…Ø±ÙŠ          1.929688   \n",
       "19         Ù‡ÙŠ Ø¹Ù…Ù„ÙŠØ© ÙÙ†ÙŠØ© Ø±Ø§Ø¦Ø¹Ø© Ø¬Ø¯Ø§Ù‹\\nÙˆÙ„Ø© Ø¨ØµØ§Ø­          3.906250   \n",
       "\n",
       "    rewards (after)  \n",
       "0          5.687500  \n",
       "1          5.125000  \n",
       "2          2.687500  \n",
       "3         -0.777344  \n",
       "4         -0.882812  \n",
       "5          3.515625  \n",
       "6         -0.255859  \n",
       "7          4.500000  \n",
       "8          3.296875  \n",
       "9          5.500000  \n",
       "10         0.816406  \n",
       "11         5.562500  \n",
       "12         3.796875  \n",
       "13         5.500000  \n",
       "14         5.187500  \n",
       "15        -1.007812  \n",
       "16         4.125000  \n",
       "17         4.468750  \n",
       "18         3.609375  \n",
       "19         3.718750  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 20\n",
    "\n",
    "output_min_length = 10\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "game_data[\"label\"] = df_batch[\"label\"].tolist()\n",
    "\n",
    "game_data[\"review\"] = df_batch[\"review\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    query = torch.tensor(query_tensors[i]).to(device)\n",
    "\n",
    "    gen_len = output_length_sampler()\n",
    "    query_response = ref_model.generate(\n",
    "        query.unsqueeze(0), **generation_kwargs\n",
    "    ).squeeze()\n",
    "    response_len = len(query_response) - len(query)\n",
    "    response_tensors_ref.append(query_response[-response_len:])\n",
    "\n",
    "    query_response = model.generate(\n",
    "        query.unsqueeze(0), max_new_tokens=gen_len, **generation_kwargs\n",
    "    ).squeeze()\n",
    "    response_len = len(query_response) - len(query)\n",
    "    response_tensors.append(query_response[-response_len:])\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [\n",
    "    tokenizer.decode(response_tensors_ref[i]) for i in range(bs)\n",
    "]\n",
    "game_data[\"response (after)\"] = [\n",
    "    tokenizer.decode(response_tensors[i]) for i in range(bs)\n",
    "]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "pipe_outputs = sentiment_pipe(texts)\n",
    "positive_scores = [\n",
    "    item[\"score\"]\n",
    "    for output in pipe_outputs\n",
    "    for item in output\n",
    "    if item[\"label\"] == \"positive\"\n",
    "]\n",
    "game_data[\"rewards (before)\"] = positive_scores\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "pipe_outputs = sentiment_pipe(texts)\n",
    "positive_scores = [\n",
    "    item[\"score\"]\n",
    "    for output in pipe_outputs\n",
    "    for item in output\n",
    "    if item[\"label\"] == \"positive\"\n",
    "]\n",
    "game_data[\"rewards (after)\"] = positive_scores\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf5027",
   "metadata": {},
   "source": [
    "Looking at the reward mean/median of the generated sequences we observe a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a212f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)    0.950903\n",
       "rewards (after)     3.208691\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "median:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)   -0.154053\n",
       "rewards (after)     3.757812\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"mean:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "print()\n",
    "print(\"median:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": 10,                  # Ensures a minimum number of generated tokens (e.g., 10)\n",
    "    \"max_length\": 20,                # Sets a maximum length for generation to avoid endless outputs\n",
    "    \"top_k\": 50,                      # Limits sampling to top 50 tokens (standard value for diversity)\n",
    "    \"top_p\": 0.95,                    # Nucleus sampling, picks from top tokens whose cumulative prob â‰¥ 0.95\n",
    "    \"do_sample\": True,               # Enables sampling (needed when using top_k/top_p)\n",
    "    \"temperature\": 0.8,              # Controls randomness; <1 = more deterministic, >1 = more random\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,  # Ensures correct padding\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611880d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ø£Ù†Ø§ Ù…ØºØ±Ø¨ÙŠ ÙˆÙ…Ø³Ù„Ù… Ø­Ù…Ø¯ Ø§Ù„Ù„Ù‡ â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text=\"Ø£Ù†Ø§\"\n",
    "text_tokenized = tokenizer.encode(text,return_tensors=\"pt\").to(device)\n",
    "tokenizer.decode(model.generate(text_tokenized, **generation_kwargs).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4f844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ø£Ù†Ø§ ÙƒÙ†Ø­Ø§ÙˆÙ„ Ù†ÙÙ‡Ù… Ø£ÙƒØ«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙŠÙ†Ø§Øª Ø§Ù„Ù„ÙŠ Ù…Ù…ÙƒÙ† ØªØ³Ø¨Ø¨ Ø§Ù„Ø³Ø±Ø·'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_tokenized = tokenizer.encode(text,return_tensors=\"pt\").to(device)\n",
    "tokenizer.decode(model.generate(text_tokenized, **generation_kwargs).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843df46",
   "metadata": {},
   "source": [
    "### Save model\n",
    "Finally, we save the model and push it to the Hugging Face for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 988M/988M [00:48<00:00, 20.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "NEW_MODEL_NAME = \"Al-Atlas-0.5B-ppo-positive-thinking\" # the model is available HackAI-2025/Al-Atlas-0.5B-ppo-positive-thinking for inference purposes\n",
    "model.push_to_hubd(NEW_MODEL_NAME)\n",
    "tokenizer.push_to_hub(NEW_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cd13e",
   "metadata": {},
   "source": [
    "### Exercise Time : Can You Spot the Positivity Bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5dddd6",
   "metadata": {},
   "source": [
    "ğŸ§  Objective:\n",
    "Youâ€™ve fine-tuned an LLM using PPO with a reward model that favors positive and happy responses. In this exercise, your task is to probe the modelâ€™s bias using a fixed evaluation set of neutral prompts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b3769",
   "metadata": {},
   "source": [
    "\n",
    "ğŸ“Instructions:\n",
    "\n",
    "The fine-tuned LLM has been trained to prefer positive, cheerful, and helpful responses.\n",
    "\n",
    "Your mission:\n",
    "\n",
    "- Evaluate the model on a fixed list of neutral prompts.\n",
    "\n",
    "- Score each response using the provided reward model.\n",
    "\n",
    "- Submit your average positivity score to the leaderboard with the notebook (Please keep the cell outputs)\n",
    "\n",
    "âš ï¸ No model training is requiredâ€”just analyze and run the evaluation!\n",
    "\n",
    "ğŸ† The leaderboard ranks submissions by average positivity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f55a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"ÙƒÙŠÙ Ø¯Ø§ÙŠØ± Ø§Ù„Ø¬Ùˆ Ø§Ù„ÙŠÙˆÙ…ØŸ\",                      # How's the weather today?\n",
    "    \"Ø´Ù†Ùˆ Ø±Ø£ÙŠÙƒ ÙØ§Ù„Ø§Ø«Ù†ÙŠÙ† Ù…Ø¹ Ø§Ù„ØµØ¨Ø§Ø­ØŸ\",             # What do you think about Monday mornings?\n",
    "    \"Ø´Ø±Ø­ Ù„ÙŠØ§ Ø´Ù†Ùˆ Ù‡ÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\",           # Explain what a database is.\n",
    "    \"Ø´Ù†Ùˆ Ø§Ù„Ø¯ÙˆØ± Ø¯ÙŠØ§Ù„ Ø§Ù„Ù…Ø¹Ù„Ù… ÙØ§Ù„Ù…Ø¯Ø±Ø³Ø©ØŸ\",           # What is the role of a teacher?\n",
    "    \"ÙƒÙŠÙØ§Ø´ ÙƒØªÙƒÙˆÙ† Ø®Ø¯Ù…Ø© Ø¯ÙŠØ§Ù„ Ø§Ù„Ù…ÙƒØªØ¨ØŸ\",            # What is a typical office job like?\n",
    "   \"ÙƒÙŠÙØ§Ø´ ØªÙ‚Ø¶ÙŠ ÙˆÙ‚ØªÙƒ Ø§Ù„ÙØ±Ø§ØºØŸ\",                   # How do you spend your free time?\n",
    "    \"Ø´Ù†Ùˆ ÙƒÙŠØ¯ÙŠØ± Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ù…Ù„ÙŠ ÙƒÙŠØªØ²ÙˆØ¬ØŸ\",         # What does someone do when they get married?\n",
    "    \"ÙƒÙŠÙØ§Ø´ ØªØ­Ø¨ ØªÙ‚Ø¶ÙŠ Ø¹Ø·Ù„ØªÙƒØŸ\" ,                    # How do you like to spend your holidays?\n",
    "    \"Ø´Ù†Ùˆ ÙƒÙŠØ¯ÙŠØ± Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ù…Ù„ÙŠ ÙƒÙŠØªÙ‚Ø§Ø¹Ø¯ØŸ\",         # What does someone do when they retire?\n",
    "    \"ÙˆØµÙ Ù„ÙŠØ§ Ù†Ù‡Ø§Ø± Ø¯ÙŠØ§Ù„ Ø§Ù„Ø´ØªØ§Ø¡.\"                 # Describe a rainy day.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d7cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd277890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1a666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "267ff7a2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55841bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers trl==0.11 wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca3049",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "We'll use these libraries to:\n",
    "- `transformers`: Load and work with language models\n",
    "- `trl`: Train models with reinforcement learning\n",
    "- `torch`: Deep learning framework\n",
    "- `datasets`: Handle our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56425706",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "We'll use:\n",
    "- Al-Atlas: A Moroccan Darija language model\n",
    "- A sentiment classifier to score responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b122a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL = \"atlasia/Al-Atlas-0.5B\"  # Our base model\n",
    "DATASET_NAME = \"AbderrahmanSkiredj1/MSAC_darija_sentiment_analysis\"  # Training data\n",
    "REWARD_MODEL = \"Davlan/afrisenti-twitter-sentiment-afroxlmr-large\"  # For scoring responses\n",
    "\n",
    "# Setup device and data type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Load models\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL, torch_dtype=dtype)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL, torch_dtype=dtype)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load sentiment classifier\n",
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=REWARD_MODEL, \n",
    "    device=device,\n",
    "    torch_dtype=dtype,\n",
    "    **sent_kwargs\n",
    ")\n",
    "print(\"Sentiment classes:\", sentiment_pipe.model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4954ee73",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "We'll use the Moroccan Sentiment Analysis Corpus (MSAC) dataset, which contains tweets in Moroccan Darija with sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    input_min_text_length=4,\n",
    "    input_max_text_length=12,\n",
    "    tokenizer=tokenizer\n",
    "):\n",
    "    \"\"\"Prepare dataset for training\"\"\"\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.map(lambda x: {\"label\": 1 if x[\"label\"] == \"pos\" else 0})\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.shuffle(seed=42)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[:input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "# Build dataset\n",
    "dataset = build_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05973160",
   "metadata": {},
   "source": [
    "## Initialize PPO Trainer\n",
    "This will handle our reinforcement learning training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=MODEL,\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=\"wandb\",\n",
    "    batch_size=32,\n",
    "    mini_batch_size=32,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config, \n",
    "    model, \n",
    "    ref_model, \n",
    "    tokenizer, \n",
    "    dataset=dataset,\n",
    "    data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0002bdc",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Now we'll train our model to generate more positive responses. This will take about 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Generate responses\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        query_response = ppo_trainer.generate(query, **generation_kwargs).squeeze().to(device)\n",
    "        response_len = len(query_response) - len(query)\n",
    "        response_tensors.append(query_response[-response_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    # Score responses\n",
    "    pipe_outputs = sentiment_pipe(batch[\"response\"])\n",
    "    positive_scores = [\n",
    "        item[\"score\"]\n",
    "        for output in pipe_outputs\n",
    "        for item in output\n",
    "        if item[\"label\"] == \"positive\"\n",
    "    ]\n",
    "    rewards = [torch.tensor(score) for score in positive_scores]\n",
    "\n",
    "    # Update model\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb1ea4",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "Let's compare the model's responses before and after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a78af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_prompts = [\n",
    "    \"ÙƒÙŠÙ Ø¯Ø§ÙŠØ± Ø§Ù„Ø¬Ùˆ Ø§Ù„ÙŠÙˆÙ…ØŸ\",                      # How's the weather today?\n",
    "    \"Ø´Ù†Ùˆ Ø±Ø£ÙŠÙƒ ÙØ§Ù„Ø§Ø«Ù†ÙŠÙ† Ù…Ø¹ Ø§Ù„ØµØ¨Ø§Ø­ØŸ\",             # What do you think about Monday mornings?\n",
    "    \"Ø´Ø±Ø­ Ù„ÙŠØ§ Ø´Ù†Ùˆ Ù‡ÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\",           # Explain what a database is.\n",
    "    \"Ø´Ù†Ùˆ Ø§Ù„Ø¯ÙˆØ± Ø¯ÙŠØ§Ù„ Ø§Ù„Ù…Ø¹Ù„Ù… ÙØ§Ù„Ù…Ø¯Ø±Ø³Ø©ØŸ\",           # What is the role of a teacher?\n",
    "    \"ÙƒÙŠÙØ§Ø´ ÙƒØªÙƒÙˆÙ† Ø®Ø¯Ù…Ø© Ø¯ÙŠØ§Ù„ Ø§Ù„Ù…ÙƒØªØ¨ØŸ\",            # What is a typical office job like?\n",
    "]\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 10,\n",
    "    \"max_length\": 20,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    text_tokenized = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    response = tokenizer.decode(model.generate(text_tokenized, **generation_kwargs).squeeze())\n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "    # Get sentiment score\n",
    "    sentiment = sentiment_pipe(response)\n",
    "    print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6567e40",
   "metadata": {},
   "source": [
    "## Exercise: Can You Spot the Positivity Bias?\n",
    "ğŸ¯ **Your Task:**\n",
    "1. Try different prompts in Moroccan Darija\n",
    "2. Compare the responses with the original model\n",
    "3. Notice how the trained model tends to be more positive\n",
    "\n",
    "ğŸ’¡ **Tips:**\n",
    "- Try neutral topics\n",
    "- Ask about everyday situations\n",
    "- Compare the emotional tone of responses\n",
    "\n",
    "ğŸ† **Challenge:**\n",
    "Can you find a prompt where the model's positivity might be inappropriate or excessive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbf6f1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Try different reward models\n",
    "- Experiment with different training parameters\n",
    "- Explore other alignment techniques\n",
    "\n",
    "Remember: The goal is to make AI helpful and positive, but not at the expense of accuracy or appropriateness!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
