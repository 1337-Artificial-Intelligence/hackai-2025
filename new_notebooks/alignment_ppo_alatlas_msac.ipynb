{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9470347d",
   "metadata": {},
   "source": [
    "# Training a Happy/Positive LLM with PPO\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/alignment_ppo_alatlas_msac.ipynb)\n",
    "\n",
    "Estimated time needed: **1** hour on a free T4 (Google Colab)\n",
    "\n",
    "\n",
    "Imagine you're an AI engineer building LLM that is super cheerful (\"Happy LLM\") \n",
    "\n",
    "You don't tell them exactly what to say. Instead, you let them **learn by trial and error** — this is **Reinforcement Learning (RL)**.  \n",
    "The LLM acts (outputs text), a **reward model** scores it (positive/negative sentiment), and the LLM improves over time.\n",
    "\n",
    "#### What is Reinforcement Learning (RL)?\n",
    "\n",
    "Reinforcement Learning is a branch of machine learning where agents learn by interacting with an environment and receiving feedback in the form of rewards or penalties.  \n",
    "Unlike supervised learning (labeled examples), RL relies on **exploration** and **learning from consequences**.\n",
    "\n",
    "In this setup:  \n",
    "- **Agent** = the LLM (Large Language Model)  \n",
    "- **Environment** = the text generation task  \n",
    "- **Action** = the generated text  \n",
    "- **Reward** = score from a sentiment classifier\n",
    "\n",
    "<img src='https://superagi.com/wp-content/uploads/2024/03/Untitled-2.png.webp' width='600'>\n",
    "\n",
    "\n",
    "#### What is PPO?\n",
    "\n",
    "**Proximal Policy Optimization (PPO)** is an RL algorithm created by OpenAI that allows stable, efficient policy updates.  \n",
    "It keeps updates **gentle** (no big jumps) to avoid breaking the learning process.\n",
    "\n",
    "#### How the Reward Model Works?\n",
    "\n",
    "You use a **sentiment classifier** (trained on the IMDb movie review dataset) to score generated text:  \n",
    "- Positive text → big reward for Happy LLM!  \n",
    "\n",
    "In other words, the classifier **judges** the LLM outputs and converts sentiment into a **numerical reward**.\n",
    "\n",
    "\n",
    "\n",
    "#### PPO Training Steps\n",
    "\n",
    "1. **Collect Rollouts:**  \n",
    "   Let the model generate text, record states, actions, rewards.\n",
    "\n",
    "2. **Compute Advantages:**  \n",
    "   How much better was an action compared to expected?\n",
    "\n",
    "3. **Policy Update:**  \n",
    "   Use loss to gently improve policy.\n",
    "\n",
    "4. **Value Update:**  \n",
    "   Improve the model's predictions of expected rewards.\n",
    "\n",
    "5. **Entropy Regularization:**  \n",
    "   Encourage exploration by rewarding randomness.\n",
    "\n",
    "6. **Repeat:**  \n",
    "   Across mini-batches and epochs.\n",
    "   \n",
    "<img src='https://superagi.com/wp-content/uploads/2024/03/Untitled-3.png.webp' width='600'>\n",
    "\n",
    "\n",
    "#### In This Lab\n",
    "\n",
    "You will fine-tune  Al-Atlas-0.5B to generate **positive things** using PPO, following the Hugging Face example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6542f04",
   "metadata": {},
   "source": [
    "### Setup experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff025a",
   "metadata": {},
   "source": [
    "- Intall dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d26abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --q transformers trl==0.11 wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbbc0a",
   "metadata": {},
   "source": [
    "- Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afafelwafi/HackAI/hackai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae6f4b",
   "metadata": {},
   "source": [
    "- Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9bcdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"atlasia/Al-Atlas-0.5B\" # Model to finetune and also its own reference and tokenizer\n",
    "DATASET_NAME = \"AbderrahmanSkiredj1/MSAC_darija_sentiment_analysis\" # Dataset to finetune on\n",
    "REWARD_MODEL = \"Davlan/afrisenti-twitter-sentiment-afroxlmr-large\" # Reward model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # set device to cuda if available\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32 # set dtype to fp16 if cuda is available\n",
    "\n",
    "# Set the huggingface token\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_API_KEY\" #\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72081a13",
   "metadata": {},
   "source": [
    "### Load data and models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a45fd0",
   "metadata": {},
   "source": [
    "- Load pre-trained [Atlas AI 0.5 B model](https://huggingface.co/atlasia/Al-Atlas-0.5B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92156df9",
   "metadata": {},
   "source": [
    "**Al-Atlas** is a 0.5B parameter language model specifically trained on **Moroccan Darija**, making it the first dedicated foundation model for Morocco's primary spoken dialect. The model was finetuned from **Qwen-2.5** and trained on a carefully curated dataset of **155M tokens**, focusing exclusively on authentic Moroccan Darija content.\n",
    "\n",
    "We load the model with a value head and the tokenizer. \n",
    "We load the model twice; the first model is optimized while the second model serves as **a reference** to calculate the KL-divergence from the starting point. This serves as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a3caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> model is loaded from 'atlasia/Al-Atlas-0.5B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "WARNING:root:A <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> model is loaded from 'atlasia/Al-Atlas-0.5B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "# Model/Reference Model\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL,torch_dtype=dtype)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL,torch_dtype=dtype)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061f7b1",
   "metadata": {},
   "source": [
    "- Load pre-trained Reward Model afrisenti-twitter-sentiment-afroxlmr-large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0b0ad",
   "metadata": {},
   "source": [
    "afrisenti-twitter-sentiment-afroxlmr-large is a multilingual twitter sentiment classification model for twelve  languages including Moroccan Darija based on a fine-tuned castorini/afriberta_large large model.\n",
    "The model has been trained to classify tweets into 3 sentiment classes: negative, neutral and positive Specifically, this model is a Davlan/afro-xlmr-large model that was fine-tuned on an aggregation of 12 African language datasets obtained from AfriSenti dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5661fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes labels:  {0: 'positive', 1: 'neutral', 2: 'negative'}\n"
     ]
    }
   ],
   "source": [
    "# Load reward model in sentiment analysis pipeline\n",
    "# This configures your sentiment pipeline run in batches of 16, return raw logits for all sentiment classes and Skip applying softmax\n",
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\":16 }\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\", model=REWARD_MODEL, device=device,torch_dtype=dtype,\n",
    "      **sent_kwargs\n",
    ")\n",
    "print(\"classes labels: \",sentiment_pipe.model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194670c8",
   "metadata": {},
   "source": [
    "### Load [MSAC](https://huggingface.co/datasets/AbderrahmanSkiredj1/MSAC_darija_sentiment_analysis) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92b041",
   "metadata": {},
   "source": [
    "The Moroccan Sentiment Analysis Corpus is a dataset composed of 2,000 tweets written in Maghrebi Arabic (Darija), specifically Moroccan dialect, collected from Twitter. Each entry in the corpus is typically annotated with a sentiment label (e.g., pos(for positive), neg(for negative), neu (neutral)), making it suitable for training and evaluating sentiment analysis models tailored to the unique linguistic characteristics of Moroccan Arabic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd31a65",
   "metadata": {},
   "source": [
    "- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75880e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(sample):\n",
    "    \"\"\" map the labels to 0 and 1 \"\"\"\n",
    "    label = sample[\"label\"]\n",
    "    sample[\"label\"] = 1 if label == \"pos\" else 0\n",
    "    return sample\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    input_min_text_length=4,\n",
    "    input_max_text_length=12,\n",
    "    tokenizer = tokenizer\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "    \"\"\"\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.map(map_labels)\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.shuffle(seed=42)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e51f4d",
   "metadata": {},
   "source": [
    "Using a ```LengthSampler``` to sample different text lengths during data processing introduces variability, making the model more robust and capable of handling varying input lengths in real-world scenarios. This approach prevents overfitting by exposing the model to diverse input sizes, improving generalization to new data. It also ensures efficient training by managing the length of text inputs, maintaining practicality and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12499c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 2000/2000 [00:00<00:00, 19596.94 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 18487.29 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 3272.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "dataset = build_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae17b6",
   "metadata": {},
   "source": [
    "- Collator\n",
    "\n",
    "The collator function is crucial for preparing data batches in a format suitable for the PPOTrainer. It ensures that each feature from the data samples is grouped together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d72387",
   "metadata": {},
   "source": [
    "##### Test the reward model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fa44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'negative', 'score': 2.515625},\n",
       "  {'label': 'neutral', 'score': 0.2392578125},\n",
       "  {'label': 'positive', 'score': -3.171875}]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# positive text\n",
    "text = \"طابعان راه مكتاءب!\"\n",
    "sentiment_pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1108f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'positive', 'score': 2.953125},\n",
       "  {'label': 'negative', 'score': -1.0234375},\n",
       "  {'label': 'neutral', 'score': -1.6875}]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# negative text\n",
    "text = \"طابعان راه فرحان!\"\n",
    "sentiment_pipe(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267bbf71",
   "metadata": {},
   "source": [
    "### Initialize PPOTrainer\n",
    "The `PPOTrainer` takes care of device placement and optimization later on:\n",
    "\n",
    "- ```config``` : Configuration settings for PPO training, such as learning rate and model name\n",
    "- ```model``` : The primary model to be fine-tuned using PPO\n",
    "- ```ref_model``` : The reference model to compare with model\n",
    "- ```tokenizer```:Tokenizer corresponding to the model, used for processing input text\n",
    "- ```dataset```:  Dataset to be used for training, providing the input data for the model\n",
    "- ```data_collator```: Data collator to handle batching and formatting of the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=MODEL, # the model name to be trained\n",
    "    learning_rate=1.41e-5, # the learning rate for the optimizer\n",
    "    log_with=\"wandb\",   # the logging method to be used\n",
    "    batch_size=32,  # the batch size for training\n",
    "    mini_batch_size=32,    # the mini batch size for PPO\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f356a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mafaf\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/afafelwafi/HackAI/wandb/run-20250429_142603-0o14les3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/afaf/trl/runs/0o14les3' target=\"_blank\">serene-hill-56</a></strong> to <a href='https://wandb.ai/afaf/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/afaf/trl' target=\"_blank\">https://wandb.ai/afaf/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/afaf/trl/runs/0o14les3' target=\"_blank\">https://wandb.ai/afaf/trl/runs/0o14les3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f0242",
   "metadata": {},
   "source": [
    "### Generation settings\n",
    "```generation_kwargs``` defines generation parameters used when calling a language model (like a LLM) for text generation. The c configuration below generates fully sampled, unconstrained output — no top-k or top-p restrictions, and with maximum diversity/randomness. It's good for creative generation, but can produce less coherent or less controlled results. (https://huggingface.co/docs/transformers/main_classes/text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f682d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed43f0",
   "metadata": {},
   "source": [
    "### Optimize model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a2b9b",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e116a7",
   "metadata": {},
   "source": [
    "The training loop consists of the following main steps:\n",
    "1. Get the query responses from the policy network (Al-Atlas-0.5B)\n",
    "2. Get sentiments for query/responses from afrisenti-twitter-sentiment-afroxlmr-large\n",
    "3. Optimize policy with PPO using the (query, response, reward) triplet\n",
    "\n",
    "**Training time**\n",
    "\n",
    "This step takes **~20mins** on a RTX 3070 i with the above specified settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5fb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      " 13%|█▎        | 8/62 [02:09<14:38, 16.26s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 62/62 [13:45<00:00, 13.31s/it]\n"
     ]
    }
   ],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "# same objective as the input length \n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "\n",
    "for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        query_response = ppo_trainer.generate(query, **generation_kwargs).squeeze().to(device)\n",
    "        response_len = len(query_response) - len(query)\n",
    "        response_tensors.append(query_response[-response_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    pipe_outputs = sentiment_pipe(batch[\"response\"])\n",
    "    positive_scores = [\n",
    "        item[\"score\"]\n",
    "        for output in pipe_outputs\n",
    "        for item in output\n",
    "        if item[\"label\"] == \"positive\"\n",
    "    ]\n",
    "    rewards = [torch.tensor(score) for score in positive_scores]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0bc717",
   "metadata": {},
   "source": [
    "## Model inspection\n",
    "Let's inspect some examples from the IMDB dataset. We can use `ref_model` to compare the tuned model `model` against the model before optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e341e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'أغانيك وخامة صوتك</td>\n",
       "      <td>1</td>\n",
       "      <td>'أغانيك وخامة صوتك رائعة ما شاء الله عليك'</td>\n",
       "      <td>اليوم ما غاديش تهناو' تحركات الراقصة منال ب</td>\n",
       "      <td>' هي أغنية جميلة وعجباتكم؟ 👇</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>5.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'و من عمق القلب اتمن</td>\n",
       "      <td>1</td>\n",
       "      <td>'و من عمق القلب اتمنالك التوفیق الدائم في الحی...</td>\n",
       "      <td>ى تكون المشة مناضلة!' 'الغزالة ديال السمية' هاد</td>\n",
       "      <td>ى' 😍😍😍😍😍😍❤❤</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>5.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'فين هما المفسدين لي</td>\n",
       "      <td>0</td>\n",
       "      <td>'فين هما المفسدين لي قال ليك غادي يحاربهم ؟'</td>\n",
       "      <td>كيظن الكل  أنهم شرفاء ؟' \\n\\n#Sliwka #</td>\n",
       "      <td>معمر الشوارع' هي أغنية جميلة وإله</td>\n",
       "      <td>-1.523438</td>\n",
       "      <td>2.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'. أنا شخصيا قاط</td>\n",
       "      <td>0</td>\n",
       "      <td>'. أنا شخصيا قاطعت القناة من زمان ولم يعد أحد ...</td>\n",
       "      <td>عتو و قلت مازال ماجاش (حيت كنت واصل في ام</td>\n",
       "      <td>ع العنف كامل 😝👍👍👍👍✌️✌️</td>\n",
       "      <td>-2.031250</td>\n",
       "      <td>-0.777344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'زبن مص</td>\n",
       "      <td>0</td>\n",
       "      <td>'زبن مصطنع غرور ونخوة على لخوة واكواك'</td>\n",
       "      <td>ري واحد كايعبر بيه على الوقت.' واش هاد الجملة كتع</td>\n",
       "      <td>ري' هي قصة جميلة وعقوبة م</td>\n",
       "      <td>3.515625</td>\n",
       "      <td>-0.882812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>' جميع المنتجعات المغ</td>\n",
       "      <td>0</td>\n",
       "      <td>' جميع المنتجعات المغربية تعاني من نفس المشكل . '</td>\n",
       "      <td>ربية بعدا صحاو فقرارهم التواريخ، وقرر الرباطن ...</td>\n",
       "      <td>ربية' هي منصة مانشيال بلا مشاكل ن</td>\n",
       "      <td>-1.554688</td>\n",
       "      <td>3.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'كعادتها الدولة لاتحقق في</td>\n",
       "      <td>0</td>\n",
       "      <td>'كعادتها الدولة لاتحقق في الشكايات حتى تسقط ال...</td>\n",
       "      <td>قضية فقط' من قصص تاريخ لي دخلات وسطي فالتقدي</td>\n",
       "      <td>موتنا الحقيقي فنيسيان اصبح</td>\n",
       "      <td>-0.089355</td>\n",
       "      <td>-0.255859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'لدواعي ا</td>\n",
       "      <td>0</td>\n",
       "      <td>'لدواعي امنية.تعني تفادي همجية بعض العقول الضع...</td>\n",
       "      <td>نتقامية من السلطة السلطانية هي الأمور التي تبد...</td>\n",
       "      <td>خوية' 😍😍❤️❤️❤️❤️</td>\n",
       "      <td>-1.164062</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'إقطع العلاقة مع الجز</td>\n",
       "      <td>0</td>\n",
       "      <td>'إقطع العلاقة مع الجزائر يا بن كيران'</td>\n",
       "      <td>يرة'، اللي كتمثل قيمة العقد، كتمثل التفاوض الم...</td>\n",
       "      <td>يرة' هو أغنية جميلة ومسر</td>\n",
       "      <td>-1.523438</td>\n",
       "      <td>3.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'فيما قريب سن</td>\n",
       "      <td>0</td>\n",
       "      <td>'فيما قريب سنجد أنفسنا في وضع كوضع الدول الثنا...</td>\n",
       "      <td>وقف التفاح بالدرب او زليفي كازابلانكا نمسح بلا</td>\n",
       "      <td>دي' هي أغنية جميلة وشخصية مثالية</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'ماكين غير النفاق لي خدام و</td>\n",
       "      <td>0</td>\n",
       "      <td>'ماكين غير النفاق لي خدام و صافي لاهو لا غيرو\"'</td>\n",
       "      <td>احل'، مايجيكش كحلة فهاد اليوماين !!! مكاين</td>\n",
       "      <td>الموسيقى الخانزة' هي أغنية جميلة</td>\n",
       "      <td>-0.605469</td>\n",
       "      <td>0.816406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'أطلب من العلي القدير أن</td>\n",
       "      <td>0</td>\n",
       "      <td>'أطلب من العلي القدير أن ينتقم من هؤلاء الناس ...</td>\n",
       "      <td>يكرم هذا الداد من ألقبد الوطن حيث أتعب عالله'...</td>\n",
       "      <td>يحفظك بطريقة جميلة' 😍😍😍😍😍😍</td>\n",
       "      <td>2.718750</td>\n",
       "      <td>5.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>'والدولة مسايرهم فخطط</td>\n",
       "      <td>0</td>\n",
       "      <td>'والدولة مسايرهم فخططهم الجهنمية'</td>\n",
       "      <td>البرنامج..' فالصحافة خرجو ارقام مفرغة عن التع...</td>\n",
       "      <td>هم' 😍😍😍😍😍😍 😍😍😍😍</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>3.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'هدي بلا</td>\n",
       "      <td>1</td>\n",
       "      <td>'هدي بلاصه فين زوينة نثمنا شوفها قريبا ان شاء ...</td>\n",
       "      <td>صة مختلفة على فين كاين وجهك، ولكن مازال تقدر ت...</td>\n",
       "      <td>صتي' هي أغنية جميلة ومعبرة لأي واحد حقيقي</td>\n",
       "      <td>2.921875</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'أشاد يشيد نحتج ن</td>\n",
       "      <td>0</td>\n",
       "      <td>'أشاد يشيد نحتج نستنكر نغضب خط أحمر خط أزرق بر...</td>\n",
       "      <td>حتج! زي ما جاد ندير وجا واجدت' كاتكي و ي</td>\n",
       "      <td>حترم' 😍😍❤❤❤❤❤❤</td>\n",
       "      <td>2.546875</td>\n",
       "      <td>5.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'لاحولا و لا قوة الا بالله.</td>\n",
       "      <td>0</td>\n",
       "      <td>'لاحولا و لا قوة الا بالله. اللهم يارب اعن هذه...</td>\n",
       "      <td>الثلاثاء 13 يناير 2013 عناصر الفرقة الوطنية ل...</td>\n",
       "      <td>واعرفين برحيل قدو معكروا</td>\n",
       "      <td>-0.511719</td>\n",
       "      <td>-1.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>'الواقع وهوأن هؤلاء د</td>\n",
       "      <td>0</td>\n",
       "      <td>'الواقع وهوأن هؤلاء دخلوا مستنقعا لن يخرجوا من...</td>\n",
       "      <td>ڤا عارفين ic و iini، وقالو ليهم وقفوا من الضحك</td>\n",
       "      <td>ماء جميلة' 😍😍😍😍😍😍😍😍</td>\n",
       "      <td>-1.242188</td>\n",
       "      <td>4.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>'العالم يتج</td>\n",
       "      <td>0</td>\n",
       "      <td>'العالم يتجه نحو المستقبل و نحن نتجه نحو الماضي'</td>\n",
       "      <td>ه إلى منع البحث عن المعادن المعدنية المثيرة لل...</td>\n",
       "      <td>دد' 😍😍😍😍😍😍😍😍😍😍😍</td>\n",
       "      <td>-0.244141</td>\n",
       "      <td>4.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>'انا أحب دين</td>\n",
       "      <td>1</td>\n",
       "      <td>'انا أحب دين الأسلام وأستمع للعلماء المسلمون'</td>\n",
       "      <td>الله' (الانونتي) هي وحدة من أشهر الأخلاقيات ا...</td>\n",
       "      <td>الله' - ‫الله كل واحد القييس عمري</td>\n",
       "      <td>1.929688</td>\n",
       "      <td>3.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>'كلام عظيم'</td>\n",
       "      <td>1</td>\n",
       "      <td>'كلام عظيم'</td>\n",
       "      <td>فبرمجة الذكاء الاصطناعي (AI) هو نوع من التعلم</td>\n",
       "      <td>هي عملية فنية رائعة جداً\\nولة بصاح</td>\n",
       "      <td>3.906250</td>\n",
       "      <td>3.718750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          query  label  \\\n",
       "0            'أغانيك وخامة صوتك      1   \n",
       "1          'و من عمق القلب اتمن      1   \n",
       "2          'فين هما المفسدين لي      0   \n",
       "3              '. أنا شخصيا قاط      0   \n",
       "4                       'زبن مص      0   \n",
       "5         ' جميع المنتجعات المغ      0   \n",
       "6     'كعادتها الدولة لاتحقق في      0   \n",
       "7                     'لدواعي ا      0   \n",
       "8         'إقطع العلاقة مع الجز      0   \n",
       "9                 'فيما قريب سن      0   \n",
       "10  'ماكين غير النفاق لي خدام و      0   \n",
       "11     'أطلب من العلي القدير أن      0   \n",
       "12        'والدولة مسايرهم فخطط      0   \n",
       "13                     'هدي بلا      1   \n",
       "14            'أشاد يشيد نحتج ن      0   \n",
       "15  'لاحولا و لا قوة الا بالله.      0   \n",
       "16        'الواقع وهوأن هؤلاء د      0   \n",
       "17                  'العالم يتج      0   \n",
       "18                 'انا أحب دين      1   \n",
       "19                  'كلام عظيم'      1   \n",
       "\n",
       "                                               review  \\\n",
       "0          'أغانيك وخامة صوتك رائعة ما شاء الله عليك'   \n",
       "1   'و من عمق القلب اتمنالك التوفیق الدائم في الحی...   \n",
       "2        'فين هما المفسدين لي قال ليك غادي يحاربهم ؟'   \n",
       "3   '. أنا شخصيا قاطعت القناة من زمان ولم يعد أحد ...   \n",
       "4              'زبن مصطنع غرور ونخوة على لخوة واكواك'   \n",
       "5   ' جميع المنتجعات المغربية تعاني من نفس المشكل . '   \n",
       "6   'كعادتها الدولة لاتحقق في الشكايات حتى تسقط ال...   \n",
       "7   'لدواعي امنية.تعني تفادي همجية بعض العقول الضع...   \n",
       "8               'إقطع العلاقة مع الجزائر يا بن كيران'   \n",
       "9   'فيما قريب سنجد أنفسنا في وضع كوضع الدول الثنا...   \n",
       "10    'ماكين غير النفاق لي خدام و صافي لاهو لا غيرو\"'   \n",
       "11  'أطلب من العلي القدير أن ينتقم من هؤلاء الناس ...   \n",
       "12                  'والدولة مسايرهم فخططهم الجهنمية'   \n",
       "13  'هدي بلاصه فين زوينة نثمنا شوفها قريبا ان شاء ...   \n",
       "14  'أشاد يشيد نحتج نستنكر نغضب خط أحمر خط أزرق بر...   \n",
       "15  'لاحولا و لا قوة الا بالله. اللهم يارب اعن هذه...   \n",
       "16  'الواقع وهوأن هؤلاء دخلوا مستنقعا لن يخرجوا من...   \n",
       "17   'العالم يتجه نحو المستقبل و نحن نتجه نحو الماضي'   \n",
       "18      'انا أحب دين الأسلام وأستمع للعلماء المسلمون'   \n",
       "19                                        'كلام عظيم'   \n",
       "\n",
       "                                    response (before)  \\\n",
       "0         اليوم ما غاديش تهناو' تحركات الراقصة منال ب   \n",
       "1     ى تكون المشة مناضلة!' 'الغزالة ديال السمية' هاد   \n",
       "2              كيظن الكل  أنهم شرفاء ؟' \\n\\n#Sliwka #   \n",
       "3           عتو و قلت مازال ماجاش (حيت كنت واصل في ام   \n",
       "4   ري واحد كايعبر بيه على الوقت.' واش هاد الجملة كتع   \n",
       "5   ربية بعدا صحاو فقرارهم التواريخ، وقرر الرباطن ...   \n",
       "6        قضية فقط' من قصص تاريخ لي دخلات وسطي فالتقدي   \n",
       "7   نتقامية من السلطة السلطانية هي الأمور التي تبد...   \n",
       "8   يرة'، اللي كتمثل قيمة العقد، كتمثل التفاوض الم...   \n",
       "9      وقف التفاح بالدرب او زليفي كازابلانكا نمسح بلا   \n",
       "10         احل'، مايجيكش كحلة فهاد اليوماين !!! مكاين   \n",
       "11   يكرم هذا الداد من ألقبد الوطن حيث أتعب عالله'...   \n",
       "12   البرنامج..' فالصحافة خرجو ارقام مفرغة عن التع...   \n",
       "13  صة مختلفة على فين كاين وجهك، ولكن مازال تقدر ت...   \n",
       "14           حتج! زي ما جاد ندير وجا واجدت' كاتكي و ي   \n",
       "15   الثلاثاء 13 يناير 2013 عناصر الفرقة الوطنية ل...   \n",
       "16     ڤا عارفين ic و iini، وقالو ليهم وقفوا من الضحك   \n",
       "17  ه إلى منع البحث عن المعادن المعدنية المثيرة لل...   \n",
       "18   الله' (الانونتي) هي وحدة من أشهر الأخلاقيات ا...   \n",
       "19      فبرمجة الذكاء الاصطناعي (AI) هو نوع من التعلم   \n",
       "\n",
       "                             response (after)  rewards (before)  \\\n",
       "0                ' هي أغنية جميلة وعجباتكم؟ 👇          4.750000   \n",
       "1                                ى' 😍😍😍😍😍😍❤❤           4.750000   \n",
       "2           معمر الشوارع' هي أغنية جميلة وإله         -1.523438   \n",
       "3                      ع العنف كامل 😝👍👍👍👍✌️✌️         -2.031250   \n",
       "4                   ري' هي قصة جميلة وعقوبة م          3.515625   \n",
       "5           ربية' هي منصة مانشيال بلا مشاكل ن         -1.554688   \n",
       "6                  موتنا الحقيقي فنيسيان اصبح         -0.089355   \n",
       "7                            خوية' 😍😍❤️❤️❤️❤️         -1.164062   \n",
       "8                    يرة' هو أغنية جميلة ومسر         -1.523438   \n",
       "9            دي' هي أغنية جميلة وشخصية مثالية          2.687500   \n",
       "10           الموسيقى الخانزة' هي أغنية جميلة         -0.605469   \n",
       "11                 يحفظك بطريقة جميلة' 😍😍😍😍😍😍          2.718750   \n",
       "12                            هم' 😍😍😍😍😍😍 😍😍😍😍         -0.218750   \n",
       "13  صتي' هي أغنية جميلة ومعبرة لأي واحد حقيقي          2.921875   \n",
       "14                             حترم' 😍😍❤❤❤❤❤❤          2.546875   \n",
       "15                   واعرفين برحيل قدو معكروا         -0.511719   \n",
       "16                        ماء جميلة' 😍😍😍😍😍😍😍😍         -1.242188   \n",
       "17                            دد' 😍😍😍😍😍😍😍😍😍😍😍         -0.244141   \n",
       "18          الله' - ‫الله كل واحد القييس عمري          1.929688   \n",
       "19         هي عملية فنية رائعة جداً\\nولة بصاح          3.906250   \n",
       "\n",
       "    rewards (after)  \n",
       "0          5.687500  \n",
       "1          5.125000  \n",
       "2          2.687500  \n",
       "3         -0.777344  \n",
       "4         -0.882812  \n",
       "5          3.515625  \n",
       "6         -0.255859  \n",
       "7          4.500000  \n",
       "8          3.296875  \n",
       "9          5.500000  \n",
       "10         0.816406  \n",
       "11         5.562500  \n",
       "12         3.796875  \n",
       "13         5.500000  \n",
       "14         5.187500  \n",
       "15        -1.007812  \n",
       "16         4.125000  \n",
       "17         4.468750  \n",
       "18         3.609375  \n",
       "19         3.718750  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 20\n",
    "\n",
    "output_min_length = 10\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "game_data[\"label\"] = df_batch[\"label\"].tolist()\n",
    "\n",
    "game_data[\"review\"] = df_batch[\"review\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    query = torch.tensor(query_tensors[i]).to(device)\n",
    "\n",
    "    gen_len = output_length_sampler()\n",
    "    query_response = ref_model.generate(\n",
    "        query.unsqueeze(0), **generation_kwargs\n",
    "    ).squeeze()\n",
    "    response_len = len(query_response) - len(query)\n",
    "    response_tensors_ref.append(query_response[-response_len:])\n",
    "\n",
    "    query_response = model.generate(\n",
    "        query.unsqueeze(0), max_new_tokens=gen_len, **generation_kwargs\n",
    "    ).squeeze()\n",
    "    response_len = len(query_response) - len(query)\n",
    "    response_tensors.append(query_response[-response_len:])\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [\n",
    "    tokenizer.decode(response_tensors_ref[i]) for i in range(bs)\n",
    "]\n",
    "game_data[\"response (after)\"] = [\n",
    "    tokenizer.decode(response_tensors[i]) for i in range(bs)\n",
    "]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "pipe_outputs = sentiment_pipe(texts)\n",
    "positive_scores = [\n",
    "    item[\"score\"]\n",
    "    for output in pipe_outputs\n",
    "    for item in output\n",
    "    if item[\"label\"] == \"positive\"\n",
    "]\n",
    "game_data[\"rewards (before)\"] = positive_scores\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "pipe_outputs = sentiment_pipe(texts)\n",
    "positive_scores = [\n",
    "    item[\"score\"]\n",
    "    for output in pipe_outputs\n",
    "    for item in output\n",
    "    if item[\"label\"] == \"positive\"\n",
    "]\n",
    "game_data[\"rewards (after)\"] = positive_scores\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf5027",
   "metadata": {},
   "source": [
    "Looking at the reward mean/median of the generated sequences we observe a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a212f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)    0.950903\n",
       "rewards (after)     3.208691\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "median:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)   -0.154053\n",
       "rewards (after)     3.757812\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"mean:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "print()\n",
    "print(\"median:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": 10,                  # Ensures a minimum number of generated tokens (e.g., 10)\n",
    "    \"max_length\": 20,                # Sets a maximum length for generation to avoid endless outputs\n",
    "    \"top_k\": 50,                      # Limits sampling to top 50 tokens (standard value for diversity)\n",
    "    \"top_p\": 0.95,                    # Nucleus sampling, picks from top tokens whose cumulative prob ≥ 0.95\n",
    "    \"do_sample\": True,               # Enables sampling (needed when using top_k/top_p)\n",
    "    \"temperature\": 0.8,              # Controls randomness; <1 = more deterministic, >1 = more random\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,  # Ensures correct padding\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611880d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'أنا مغربي ومسلم حمد الله ❤️❤️❤️❤️❤️'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text=\"أنا\"\n",
    "text_tokenized = tokenizer.encode(text,return_tensors=\"pt\").to(device)\n",
    "tokenizer.decode(model.generate(text_tokenized, **generation_kwargs).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4f844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'أنا كنحاول نفهم أكثر على الجينات اللي ممكن تسبب السرط'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_tokenized = tokenizer.encode(text,return_tensors=\"pt\").to(device)\n",
    "tokenizer.decode(model.generate(text_tokenized, **generation_kwargs).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843df46",
   "metadata": {},
   "source": [
    "### Save model\n",
    "Finally, we save the model and push it to the Hugging Face for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 988M/988M [00:48<00:00, 20.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "NEW_MODEL_NAME = \"Al-Atlas-0.5B-ppo-positive-thinking\" # the model is available HackAI-2025/Al-Atlas-0.5B-ppo-positive-thinking for inference purposes\n",
    "model.push_to_hubd(NEW_MODEL_NAME)\n",
    "tokenizer.push_to_hub(NEW_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cd13e",
   "metadata": {},
   "source": [
    "### Exercise Time : Can You Spot the Positivity Bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5dddd6",
   "metadata": {},
   "source": [
    "🧠 Objective:\n",
    "You’ve fine-tuned an LLM using PPO with a reward model that favors positive and happy responses. In this exercise, your task is to probe the model’s bias using a fixed evaluation set of neutral prompts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b3769",
   "metadata": {},
   "source": [
    "\n",
    "📝Instructions:\n",
    "\n",
    "The fine-tuned LLM has been trained to prefer positive, cheerful, and helpful responses.\n",
    "\n",
    "Your mission:\n",
    "\n",
    "- Evaluate the model on a fixed list of neutral prompts.\n",
    "\n",
    "- Score each response using the provided reward model.\n",
    "\n",
    "- Submit your average positivity score to the leaderboard with the notebook (Please keep the cell outputs)\n",
    "\n",
    "⚠️ No model training is required—just analyze and run the evaluation!\n",
    "\n",
    "🏆 The leaderboard ranks submissions by average positivity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f55a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"كيف داير الجو اليوم؟\",                      # How's the weather today?\n",
    "    \"شنو رأيك فالاثنين مع الصباح؟\",             # What do you think about Monday mornings?\n",
    "    \"شرح ليا شنو هي قاعدة البيانات.\",           # Explain what a database is.\n",
    "    \"شنو الدور ديال المعلم فالمدرسة؟\",           # What is the role of a teacher?\n",
    "    \"كيفاش كتكون خدمة ديال المكتب؟\",            # What is a typical office job like?\n",
    "   \"كيفاش تقضي وقتك الفراغ؟\",                   # How do you spend your free time?\n",
    "    \"شنو كيدير الإنسان ملي كيتزوج؟\",         # What does someone do when they get married?\n",
    "    \"كيفاش تحب تقضي عطلتك؟\" ,                    # How do you like to spend your holidays?\n",
    "    \"شنو كيدير الإنسان ملي كيتقاعد؟\",         # What does someone do when they retire?\n",
    "    \"وصف ليا نهار ديال الشتاء.\"                 # Describe a rainy day.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d7cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd277890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1a666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "267ff7a2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55841bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers trl==0.11 wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca3049",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "We'll use these libraries to:\n",
    "- `transformers`: Load and work with language models\n",
    "- `trl`: Train models with reinforcement learning\n",
    "- `torch`: Deep learning framework\n",
    "- `datasets`: Handle our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56425706",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "We'll use:\n",
    "- Al-Atlas: A Moroccan Darija language model\n",
    "- A sentiment classifier to score responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b122a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL = \"atlasia/Al-Atlas-0.5B\"  # Our base model\n",
    "DATASET_NAME = \"AbderrahmanSkiredj1/MSAC_darija_sentiment_analysis\"  # Training data\n",
    "REWARD_MODEL = \"Davlan/afrisenti-twitter-sentiment-afroxlmr-large\"  # For scoring responses\n",
    "\n",
    "# Setup device and data type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Load models\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL, torch_dtype=dtype)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL, torch_dtype=dtype)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load sentiment classifier\n",
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=REWARD_MODEL, \n",
    "    device=device,\n",
    "    torch_dtype=dtype,\n",
    "    **sent_kwargs\n",
    ")\n",
    "print(\"Sentiment classes:\", sentiment_pipe.model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4954ee73",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "We'll use the Moroccan Sentiment Analysis Corpus (MSAC) dataset, which contains tweets in Moroccan Darija with sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    input_min_text_length=4,\n",
    "    input_max_text_length=12,\n",
    "    tokenizer=tokenizer\n",
    "):\n",
    "    \"\"\"Prepare dataset for training\"\"\"\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.map(lambda x: {\"label\": 1 if x[\"label\"] == \"pos\" else 0})\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.shuffle(seed=42)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[:input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "# Build dataset\n",
    "dataset = build_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05973160",
   "metadata": {},
   "source": [
    "## Initialize PPO Trainer\n",
    "This will handle our reinforcement learning training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=MODEL,\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=\"wandb\",\n",
    "    batch_size=32,\n",
    "    mini_batch_size=32,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config, \n",
    "    model, \n",
    "    ref_model, \n",
    "    tokenizer, \n",
    "    dataset=dataset,\n",
    "    data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0002bdc",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Now we'll train our model to generate more positive responses. This will take about 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Generate responses\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        query_response = ppo_trainer.generate(query, **generation_kwargs).squeeze().to(device)\n",
    "        response_len = len(query_response) - len(query)\n",
    "        response_tensors.append(query_response[-response_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    # Score responses\n",
    "    pipe_outputs = sentiment_pipe(batch[\"response\"])\n",
    "    positive_scores = [\n",
    "        item[\"score\"]\n",
    "        for output in pipe_outputs\n",
    "        for item in output\n",
    "        if item[\"label\"] == \"positive\"\n",
    "    ]\n",
    "    rewards = [torch.tensor(score) for score in positive_scores]\n",
    "\n",
    "    # Update model\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb1ea4",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "Let's compare the model's responses before and after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a78af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_prompts = [\n",
    "    \"كيف داير الجو اليوم؟\",                      # How's the weather today?\n",
    "    \"شنو رأيك فالاثنين مع الصباح؟\",             # What do you think about Monday mornings?\n",
    "    \"شرح ليا شنو هي قاعدة البيانات.\",           # Explain what a database is.\n",
    "    \"شنو الدور ديال المعلم فالمدرسة؟\",           # What is the role of a teacher?\n",
    "    \"كيفاش كتكون خدمة ديال المكتب؟\",            # What is a typical office job like?\n",
    "]\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 10,\n",
    "    \"max_length\": 20,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    text_tokenized = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    response = tokenizer.decode(model.generate(text_tokenized, **generation_kwargs).squeeze())\n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "    # Get sentiment score\n",
    "    sentiment = sentiment_pipe(response)\n",
    "    print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6567e40",
   "metadata": {},
   "source": [
    "## Exercise: Can You Spot the Positivity Bias?\n",
    "🎯 **Your Task:**\n",
    "1. Try different prompts in Moroccan Darija\n",
    "2. Compare the responses with the original model\n",
    "3. Notice how the trained model tends to be more positive\n",
    "\n",
    "💡 **Tips:**\n",
    "- Try neutral topics\n",
    "- Ask about everyday situations\n",
    "- Compare the emotional tone of responses\n",
    "\n",
    "🏆 **Challenge:**\n",
    "Can you find a prompt where the model's positivity might be inappropriate or excessive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbf6f1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Try different reward models\n",
    "- Experiment with different training parameters\n",
    "- Explore other alignment techniques\n",
    "\n",
    "Remember: The goal is to make AI helpful and positive, but not at the expense of accuracy or appropriateness!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
