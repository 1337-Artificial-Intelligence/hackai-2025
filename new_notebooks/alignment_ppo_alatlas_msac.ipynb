{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec08abbb",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NouamaneTazi/hackai-challenges/blob/main/new_notebooks/alignment_ppo_alatlas_msac.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a59b5",
   "metadata": {},
   "source": [
    "# Training a Happy/Positive LLM with PPO\n",
    "Estimated time needed: **1** hour on a free T4 (Google Colab)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand the basics of Reinforcement Learning (RL) and how it applies to language models\n",
    "- Learn about Proximal Policy Optimization (PPO) and its role in training LLMs\n",
    "- Fine-tune a language model to generate more positive responses\n",
    "- Evaluate the impact of RL training on model outputs\n",
    "\n",
    "## What is Reinforcement Learning (RL)?\n",
    "RL is like teaching a model through trial and error. Instead of giving it exact instructions, we let it learn from feedback:\n",
    "- The model tries something (generates text)\n",
    "- It gets feedback (positive/negative score)\n",
    "- It learns to do better next time\n",
    "\n",
    "In our case:\n",
    "- **Model** = Al-Atlas (a Moroccan Darija language model)\n",
    "- **Action** = Generating text\n",
    "- **Reward** = How positive the text is\n",
    "\n",
    "<img src='https://superagi.com/wp-content/uploads/2024/03/Untitled-2.png.webp' width='600'>\n",
    "\n",
    "## What is PPO?\n",
    "PPO (Proximal Policy Optimization) is a way to train models that:\n",
    "- Makes small, careful updates\n",
    "- Prevents the model from changing too much at once\n",
    "- Helps maintain stable learning\n",
    "\n",
    "## How We'll Train Our Model\n",
    "1. Start with Al-Atlas (a Moroccan Darija model)\n",
    "2. Use a sentiment classifier to score responses\n",
    "3. Train the model to generate more positive text\n",
    "4. Compare before/after results\n",
    "\n",
    "<img src='https://superagi.com/wp-content/uploads/2024/03/Untitled-3.png.webp' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ac62f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4aa1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers trl==0.11 wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33444299",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "We'll use these libraries to:\n",
    "- `transformers`: Load and work with language models\n",
    "- `trl`: Train models with reinforcement learning\n",
    "- `torch`: Deep learning framework\n",
    "- `datasets`: Handle our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d918d70",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "We'll use:\n",
    "- Al-Atlas: A Moroccan Darija language model\n",
    "- A sentiment classifier to score responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65f8e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL = \"atlasia/Al-Atlas-0.5B\"  # Our base model\n",
    "DATASET_NAME = \"AbderrahmanSkiredj1/MSAC_darija_sentiment_analysis\"  # Training data\n",
    "REWARD_MODEL = \"Davlan/afrisenti-twitter-sentiment-afroxlmr-large\"  # For scoring responses\n",
    "\n",
    "# Setup device and data type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Load models\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL, torch_dtype=dtype)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL, torch_dtype=dtype)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load sentiment classifier\n",
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=REWARD_MODEL, \n",
    "    device=device,\n",
    "    torch_dtype=dtype,\n",
    "    **sent_kwargs\n",
    ")\n",
    "print(\"Sentiment classes:\", sentiment_pipe.model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b59ed8",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "We'll use the Moroccan Sentiment Analysis Corpus (MSAC) dataset, which contains tweets in Moroccan Darija with sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d909ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    input_min_text_length=4,\n",
    "    input_max_text_length=12,\n",
    "    tokenizer=tokenizer\n",
    "):\n",
    "    \"\"\"Prepare dataset for training\"\"\"\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.map(lambda x: {\"label\": 1 if x[\"label\"] == \"pos\" else 0})\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.shuffle(seed=42)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[:input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "# Build dataset\n",
    "dataset = build_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621b469",
   "metadata": {},
   "source": [
    "## Initialize PPO Trainer\n",
    "This will handle our reinforcement learning training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=MODEL,\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=\"wandb\",\n",
    "    batch_size=32,\n",
    "    mini_batch_size=32,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config, \n",
    "    model, \n",
    "    ref_model, \n",
    "    tokenizer, \n",
    "    dataset=dataset,\n",
    "    data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd9d7c",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Now we'll train our model to generate more positive responses. This will take about 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Generate responses\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        query_response = ppo_trainer.generate(query, **generation_kwargs).squeeze().to(device)\n",
    "        response_len = len(query_response) - len(query)\n",
    "        response_tensors.append(query_response[-response_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    # Score responses\n",
    "    pipe_outputs = sentiment_pipe(batch[\"response\"])\n",
    "    positive_scores = [\n",
    "        item[\"score\"]\n",
    "        for output in pipe_outputs\n",
    "        for item in output\n",
    "        if item[\"label\"] == \"positive\"\n",
    "    ]\n",
    "    rewards = [torch.tensor(score) for score in positive_scores]\n",
    "\n",
    "    # Update model\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e921f7",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "Let's compare the model's responses before and after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_prompts = [\n",
    "    \"كيف داير الجو اليوم؟\",                      # How's the weather today?\n",
    "    \"شنو رأيك فالاثنين مع الصباح؟\",             # What do you think about Monday mornings?\n",
    "    \"شرح ليا شنو هي قاعدة البيانات.\",           # Explain what a database is.\n",
    "    \"شنو الدور ديال المعلم فالمدرسة؟\",           # What is the role of a teacher?\n",
    "    \"كيفاش كتكون خدمة ديال المكتب؟\",            # What is a typical office job like?\n",
    "]\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 10,\n",
    "    \"max_length\": 20,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    text_tokenized = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    response = tokenizer.decode(model.generate(text_tokenized, **generation_kwargs).squeeze())\n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "    # Get sentiment score\n",
    "    sentiment = sentiment_pipe(response)\n",
    "    print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfca073",
   "metadata": {},
   "source": [
    "## Exercise: Can You Spot the Positivity Bias?\n",
    "🎯 **Your Task:**\n",
    "1. Try different prompts in Moroccan Darija\n",
    "2. Compare the responses with the original model\n",
    "3. Notice how the trained model tends to be more positive\n",
    "\n",
    "💡 **Tips:**\n",
    "- Try neutral topics\n",
    "- Ask about everyday situations\n",
    "- Compare the emotional tone of responses\n",
    "\n",
    "🏆 **Challenge:**\n",
    "Can you find a prompt where the model's positivity might be inappropriate or excessive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d6903",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Try different reward models\n",
    "- Experiment with different training parameters\n",
    "- Explore other alignment techniques\n",
    "\n",
    "Remember: The goal is to make AI helpful and positive, but not at the expense of accuracy or appropriateness!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
