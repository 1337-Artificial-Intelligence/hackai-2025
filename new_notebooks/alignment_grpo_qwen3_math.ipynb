{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9749de78",
   "metadata": {},
   "source": [
    "# üöÄ Fine-tuning Language Models with GRPO for Math Reasoning\n",
    "\n",
    "This notebook demonstrates how to improve a language model's math reasoning abilities using Group Relative Policy Optimization (GRPO).\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/hackai-challenges/blob/main/py/alignment_grpo_qwen3_math.py)\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "1. How to fine-tune a language model for better math reasoning\n",
    "2. Understanding GRPO and its advantages over other methods\n",
    "3. Implementing reward functions for math problem-solving\n",
    "\n",
    "## üéØ Why GRPO?\n",
    "\n",
    "GRPO (Group Relative Policy Optimization) is a powerful technique that helps language models learn better by:\n",
    "- Grouping similar problems together\n",
    "- Learning from relative performance within groups\n",
    "- Improving reasoning step by step\n",
    "\n",
    "## üîß Setup and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e623c",
   "metadata": {},
   "source": [
    "### 1. Install Required Libraries\n",
    "First, let's install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets trl torch sentence-transformers pypdf math_verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9817ced",
   "metadata": {},
   "source": [
    "### 2. Import Libraries and Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "# Third-party libraries\n",
    "import torch\n",
    "import warnings\n",
    "from datasets import load_dataset, Dataset\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Custom/project-specific libraries\n",
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55cea1",
   "metadata": {},
   "source": [
    "### 3. Basic Configuration\n",
    "Let's set up our basic configuration for the model and training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b3cd3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL = \"unsloth/Qwen3-1.7B\"  # Small, efficient model good for learning\n",
    "max_seq_length = 2048         # Length for input/output\n",
    "NEW_MODEL = \"Qwen3_1.7B-GRPO-math-reasoning\"\n",
    "\n",
    "# Prompt template for consistent responses\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "Show your step-by-step thinking process\n",
    "</reasoning>\n",
    "<answer>\n",
    "Your final answer here\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# Dataset for training\n",
    "DATASET = \"lighteval/MATH-Hard\"  # Math problems dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ebd1b1",
   "metadata": {},
   "source": [
    "### 4. Load and Prepare the Dataset\n",
    "We'll use a dataset of math problems to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe74b21",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_math_questions(split=\"train\") -> Dataset:\n",
    "    \"\"\"Load and prepare math problems dataset.\"\"\"\n",
    "    data = load_dataset(DATASET, 'default')[split]\n",
    "    data = data.map(lambda x: {\n",
    "            'prompt': [\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': x['problem']}\n",
    "            ],\n",
    "            'answer': x['solution'],\n",
    "            'question': x['problem']\n",
    "        }).remove_columns(['problem', 'solution','level','type'])\n",
    "    return data\n",
    "\n",
    "# Load training and test datasets\n",
    "train_dataset = get_math_questions(split=\"train\")\n",
    "test_dataset = get_math_questions(split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a08b3",
   "metadata": {},
   "source": [
    "### 5. Define Reward Functions\n",
    "These functions help the model learn what makes a good math solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07271b3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def accuracy_reward(completions: List[dict], **kwargs) -> List[float]:\n",
    "    \"\"\"Reward function that checks if the answer matches the correct solution.\"\"\"\n",
    "    solutions = kwargs['answer']\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    for content, solution in zip(completion_contents, solutions):\n",
    "        try:\n",
    "            # Parse and verify the solution\n",
    "            gold_parsed = parse(solution, extraction_mode=\"first_match\", \n",
    "                              extraction_config=[LatexExtractionConfig()])\n",
    "            answer_parsed = parse(content, extraction_mode=\"first_match\", \n",
    "                                extraction_config=[LatexExtractionConfig()])\n",
    "            if len(gold_parsed) != 0:\n",
    "                rewards.append(float(verify(answer_parsed, gold_parsed)))\n",
    "            else:\n",
    "                rewards.append(1.0)\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5378171",
   "metadata": {},
   "source": [
    "### 6. Configure Training Parameters\n",
    "Set up the training configuration for GRPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e593bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = GRPOConfig(\n",
    "    lr_scheduler_type=\"cosine\",          # Smooth learning rate adjustment\n",
    "    per_device_train_batch_size=1,       # Small batch size for memory efficiency\n",
    "    gradient_accumulation_steps=1,       # Accumulate gradients for stability\n",
    "    warmup_steps=5,                      # Quick warmup for faster learning\n",
    "    max_steps=50,                        # Number of training steps\n",
    "    learning_rate=2e-4,                  # Learning rate\n",
    "    optim=\"adamw_8bit\",                  # Memory-efficient optimizer\n",
    "    max_grad_norm=0.1,                   # Prevent gradient explosion\n",
    "    max_prompt_length=500,               # Maximum input length\n",
    "    max_completion_length=1024,          # Maximum output length\n",
    "    seed=3407,                           # For reproducibility\n",
    "    output_dir=\"qwen3_1_7B_grpo_math\"    # Save directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d2d71",
   "metadata": {},
   "source": [
    "### 7. Train the Model\n",
    "Now we'll train our model using GRPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e433494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,                    # Our language model\n",
    "    processing_class=tokenizer,     # Text processor\n",
    "    reward_funcs=[accuracy_reward], # Reward function\n",
    "    args=training_args,            # Training configuration\n",
    "    train_dataset=train_dataset    # Training data\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e6e6d",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "You've successfully fine-tuned a language model using GRPO for better math reasoning!\n",
    "\n",
    "## üìù Key Takeaways\n",
    "1. GRPO helps models learn better by comparing performance within groups\n",
    "2. Reward functions guide the model to produce better solutions\n",
    "3. Step-by-step reasoning is crucial for math problem-solving\n",
    "\n",
    "## üîç Next Steps\n",
    "1. Try different reward functions\n",
    "2. Experiment with different model sizes\n",
    "3. Test on more complex math problems\n",
    "\n",
    "## ‚ö†Ô∏è Note\n",
    "This is a simplified version for learning purposes. For production use, you would need:\n",
    "- More training steps\n",
    "- Better reward functions\n",
    "- Proper evaluation metrics\n",
    "- Larger models for better performance"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
