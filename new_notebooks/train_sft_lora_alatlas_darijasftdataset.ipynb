{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd98927f",
   "metadata": {},
   "source": [
    "# Fine-tuning a model with LoRA\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/train_sft_lora_alatlas_darijasftdataset.ipynb)\n",
    "\n",
    "## Before You Start [Learn About LORA]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b66a14",
   "metadata": {},
   "source": [
    "> Large language models are large, and it can be expensive to update all model weights during training due to GPU memory limitations.\n",
    "\n",
    "* **Problem**\n",
    "\n",
    ">For example, suppose we have an LLM with 7B parameters represented in a weight matrix `W`. (In reality, the model parameters are, of course, distributed across different matrices in many layers, but for simplicity, we refer to a single weight matrix here).** During backpropagation**, we learn a `ΔW` matrix, which contains information on how much we want to update the original weights to minimize the loss function during training.\n",
    "* The weight update is then as follows:\n",
    "`W_updated = W + ΔW`\n",
    "* If the weight matrix `W` contains **7B parameters**, then the weight update matrix `ΔW` also contains **7B parameters**, and computing the matrix `ΔW``\n",
    " can be very compute and memory intensive.\n",
    "\n",
    "* **Solution: Low Rank Adaptation (LORA)**\n",
    "\n",
    ">To make understanding LoRA easier, let’s take a sample example:\n",
    "1. suppose we have model parameters represented by a `W (10x10)` matrix.\n",
    "![image](https://i.postimg.cc/7LtmYJ1H/lora1.png)\n",
    "\n",
    ">2. We can come up with two smaller matrices, which when multiplied, reconstruct a 10×10 matrix for example `W(10x10)=A(10,r)*B(r,10)`.\n",
    "![image](https://i.postimg.cc/3Ry9yr9g/lora2.png)\n",
    "\n",
    "> 3.This is a major efficiency win because instead of using **100 weights (10x10)** we now only have **2*(10*r) weights**.\n",
    "\n",
    ">LORA method proposed replaces to decompose the weight changes,`ΔW=A*B`, into a lower-rank representation and make W frozen.\n",
    "![image](https://i.postimg.cc/YqNRLsNy/lora3.png)\n",
    "\n",
    "> the image bellow show the difference between full ft and ft+LORA.\n",
    "![image](https://i.postimg.cc/QtRmcLnv/lora4.png)\n",
    "\n",
    "**How much memory does this save?**\n",
    "\n",
    ">It depends on the rank `r`, which is a **hyperparameter**. For example, if `ΔW` has 10,000 rows and 20,000 columns, it stores `200,000,000` parameters. If we choose A and B with r=8, then A has 10,000 rows and 8 columns, and B has 8 rows and 20,000 columns, that's 10,000×8 + 8×20,000 = `240,000` parameters, which is about **830× less than 200,000,000**.\n",
    "\n",
    "**Are A and B will capture all the information that ΔW could capture?**\n",
    "\n",
    "> Of course, **`A` and `B` can't capture all the information that `ΔW` could capture**, but this is by design. When using LoRA, we hypothesize that the model requires `W` to be a large matrix with full rank to capture all the knowledge in the pretraining dataset. However, when we finetune an LLM, we don't need to update all the weights and capture the core information for the adaptation in a smaller number of weights than ΔW would; hence, we have the low-rank updates via `AB`.\n",
    "\n",
    "**Which parameters we will target with LORA?**\n",
    "\n",
    ">You can target all model architecture layers, in our use case, we will target only the Key and Value weight matrices in each transformers layer to reduce memory requirements.\n",
    "\n",
    "**Scaling Coefficient**\n",
    "\n",
    "\n",
    ">```\n",
    "scaling = alpha / r\n",
    "weight += (lora_B @ lora_A) * scaling\n",
    "```\n",
    "* Choosing **alpha as two times r** is a common rule of thumb when using LoRA for LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a711e1",
   "metadata": {},
   "source": [
    "# Install dependencies 📚\n",
    "\n",
    "We need multiple librairies:\n",
    "\n",
    "- `peft`for LoRA adapters\n",
    "- `Transformers`for loading the model\n",
    "- `datasets`for loading and using the fine-tuning dataset\n",
    "- `trl`for the trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install -U datasets trl -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5547913b",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb1c63a",
   "metadata": {},
   "source": [
    "> We will use [`HackAI-2025/Darija_SFT_Dataset`](https://huggingface.co/datasets/HackAI-2025/Darija_SFT_Dataset) dataset to fine tune [`atlasia/Al-Atlas-0.5B`](https://huggingface.co/atlasia/Al-Atlas-0.5B) or any other model in your choice.\n",
    "\n",
    "* **SFT Dataset Example**\n",
    "\n",
    "1. Instructions\n",
    "\n",
    "<center>\n",
    "\n",
    "![image](https://i.postimg.cc/hvTrrCkv/lora5.png)\n",
    "\n",
    "</center>\n",
    "\n",
    "2. Conversations\n",
    "\n",
    "<center>\n",
    "\n",
    "![image](https://i.postimg.cc/tRx2pTsb/lora6.png\n",
    ")\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ced7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5eb45ddc65b4ffc82c630d125350118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hf login\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87df0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0570fe4e49524488a5cb192108b44334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-b55ed4afe86252b4.parquet:   0%|          | 0.00/86.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b2069e6b394dfd900e3285517b4be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'conversation', 'source', 'topic'],\n",
       "    num_rows: 201\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset=load_dataset(\"HackAI-2025/Darija_SFT_Dataset\",split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50e69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conversation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Manually generated\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"topic\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Language\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-37d89395-af75-4b6d-aec5-d310ccd82e62\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation</th>\n",
       "      <th>source</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'content': 'السلام لباس؟', 'role': 'user'}, ...</td>\n",
       "      <td>Manually generated</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'content': 'اهلا شنو سميتك؟', 'role': 'user'...</td>\n",
       "      <td>Manually generated</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'content': 'أهلا شنو سميتك؟', 'role': 'user'...</td>\n",
       "      <td>Manually generated</td>\n",
       "      <td>Chit-chat/Games/Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[{'content': 'عافاك شحال من مدينة كاينة فالمغر...</td>\n",
       "      <td>Manually generated</td>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[{'content': 'عافاك شحال من مدينة كاينة فالمغر...</td>\n",
       "      <td>Manually generated</td>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37d89395-af75-4b6d-aec5-d310ccd82e62')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-37d89395-af75-4b6d-aec5-d310ccd82e62 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-37d89395-af75-4b6d-aec5-d310ccd82e62');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-2b67e793-8fc3-4369-96ce-d087ed8f9ef6\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2b67e793-8fc3-4369-96ce-d087ed8f9ef6')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-2b67e793-8fc3-4369-96ce-d087ed8f9ef6 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   id                                       conversation              source  \\\n",
       "0   0  [{'content': 'السلام لباس؟', 'role': 'user'}, ...  Manually generated   \n",
       "1   1  [{'content': 'اهلا شنو سميتك؟', 'role': 'user'...  Manually generated   \n",
       "2   2  [{'content': 'أهلا شنو سميتك؟', 'role': 'user'...  Manually generated   \n",
       "3   3  [{'content': 'عافاك شحال من مدينة كاينة فالمغر...  Manually generated   \n",
       "4   4  [{'content': 'عافاك شحال من مدينة كاينة فالمغر...  Manually generated   \n",
       "\n",
       "                   topic  \n",
       "0                 Travel  \n",
       "1               Language  \n",
       "2  Chit-chat/Games/Humor  \n",
       "3              Geography  \n",
       "4              Geography  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some examples from ds\n",
    "dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b440c0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 201\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove other columns and rename conversation to messages\n",
    "dataset=dataset.select_columns(\"conversation\").rename_column(\"conversation\",\"messages\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813158ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'السلام لباس؟', 'role': 'user'},\n",
      " {'content': 'لاباس الحمد لله، كاين شي حاجا بغيتي نعاونك فيها؟',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'اه عافاك بغيت نسافر فالمغرب فالصيف ولكن معرفتش فين نمشي. ممكن '\n",
      "             'تعاوني؟',\n",
      "  'role': 'user'},\n",
      " {'content': 'بلان كاين بزاف ديال البلايص اللي تقد تمشي ليهم فالمغرب، انا '\n",
      "             'كنقترح عليك هدو:\\n'\n",
      "             '\\n'\n",
      "             '- شفشاون: هدي مدينة فالجبل، الديور ديالها زرقين او الجو فالمدينة '\n",
      "             'كيجيب الراحة.\\n'\n",
      "             '- الصويرة: هاد المدينة فيها البحر الا فيك ميعوم. البحر ديالها '\n",
      "             'زوين او فيها المدينة القديمة.\\n'\n",
      "             '- الداخلة: الداخلة هي مدينة فالصحرا ديال المغرب، حتاهيا فيها '\n",
      "             'البحر. الناس كيجيو ليه من العالم كامل باش يلعبوا السبور.\\n'\n",
      "             '- مراكش: هاد المدينة عزيزة على السياح لكيجيو من برا. فيها جامع '\n",
      "             'الفنا، المدينة القديمة ولكن فالصيف دايرة بحال الفران.\\n'\n",
      "             '- شلالات أوزود: هاد الشلالات كاينين فالجبل دالأطلس، هادوا اشهر '\n",
      "             'الشلالات فالمغرب سير تمنضر فيهوم معا راسك راه ايعجبوك.\\n'\n",
      "             '\\n'\n",
      "             'كاين بزاف ديال البلايس اخرين فالمغرب ولكن غولي بعدا واش هدوا '\n",
      "             'عجبوك.',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'شكرا بزاف اصاحبي', 'role': 'user'},\n",
      " {'content': 'مرحبا، إلى بغيتي شت حاجة أخرى غولهالي انا هنا باش نعاونك.',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'لالا صافي مبغيت حتى شي حاجة', 'role': 'user'},\n",
      " {'content': 'اوكي اوا نمشي نرتاح شوية على هاد الحساب. ستمتع معا راسك '\n",
      "             'بالتسافيرة.',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# show the first example of messages\n",
    "from pprint import pprint # pprint for pretty print\n",
    "pprint(dataset[\"messages\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cbb62",
   "metadata": {},
   "source": [
    "# Load Model/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b65d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8f2c42d2ca4d93be9b762225357896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e983c8781c4cebb55995299c658f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329a89b1a20547cebbeee8b0ef151c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94f179e31354844a778954b7366007b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a65fb5616c4addbb6ed377d863c683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d974c784a374963bd32c796cca35f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e350484cf4dd45ffa1011988d1806b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbc5acf600c440c857978e4cb0726a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch\n",
    "# select gpu if available\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id=\"atlasia/Al-Atlas-0.5B\"\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "model=AutoModelForCausalLM.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fdd02",
   "metadata": {},
   "source": [
    "## Model Chat Template [Test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc57811",
   "metadata": {},
   "source": [
    "Instruction fine-tuning involves training a model on a dataset where the input-output pairs, like those we extracted from the JSON file, are explicitly provided. There are various methods to format these entries for LLMs.\n",
    "\n",
    "<center>\n",
    "\n",
    "![image](https://i.postimg.cc/J4CKnLXk/lora7.png)\n",
    "</center>\n",
    "\n",
    "* Comparison of prompt styles for instruction fine-tuning in LLMs. The Alpaca style (left) uses a structured format with defined sections for instruction, input, and response, while the Phi-3 style (right) employs\n",
    "a simpler format with designated <|user|> and <|assistant|> tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|im_start|>system\\n'\n",
      " 'You are a helpful assistant.<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'السلام لباس؟<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " 'لاباس الحمد لله، كاين شي حاجا بغيتي نعاونك فيها؟<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'اه عافاك بغيت نسافر فالمغرب فالصيف ولكن معرفتش فين نمشي. ممكن '\n",
      " 'تعاوني؟<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " 'بلان كاين بزاف ديال البلايص اللي تقد تمشي ليهم فالمغرب، انا كنقترح عليك '\n",
      " 'هدو:\\n'\n",
      " '\\n'\n",
      " '- شفشاون: هدي مدينة فالجبل، الديور ديالها زرقين او الجو فالمدينة كيجيب '\n",
      " 'الراحة.\\n'\n",
      " '- الصويرة: هاد المدينة فيها البحر الا فيك ميعوم. البحر ديالها زوين او فيها '\n",
      " 'المدينة القديمة.\\n'\n",
      " '- الداخلة: الداخلة هي مدينة فالصحرا ديال المغرب، حتاهيا فيها البحر. الناس '\n",
      " 'كيجيو ليه من العالم كامل باش يلعبوا السبور.\\n'\n",
      " '- مراكش: هاد المدينة عزيزة على السياح لكيجيو من برا. فيها جامع الفنا، '\n",
      " 'المدينة القديمة ولكن فالصيف دايرة بحال الفران.\\n'\n",
      " '- شلالات أوزود: هاد الشلالات كاينين فالجبل دالأطلس، هادوا اشهر الشلالات '\n",
      " 'فالمغرب سير تمنضر فيهوم معا راسك راه ايعجبوك.\\n'\n",
      " '\\n'\n",
      " 'كاين بزاف ديال البلايس اخرين فالمغرب ولكن غولي بعدا واش هدوا '\n",
      " 'عجبوك.<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'شكرا بزاف اصاحبي<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " 'مرحبا، إلى بغيتي شت حاجة أخرى غولهالي انا هنا باش نعاونك.<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'لالا صافي مبغيت حتى شي حاجة<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " 'اوكي اوا نمشي نرتاح شوية على هاد الحساب. ستمتع معا راسك '\n",
      " 'بالتسافيرة.<|im_end|>\\n')\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer chat template\n",
    "result=tokenizer.apply_chat_template(dataset[\"messages\"][0],tokenize=False)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd1d4b",
   "metadata": {},
   "source": [
    "## Test Model Before SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450b060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "السلام لباس؟\n",
      "السلام عليكم، كيف داير؟ binge\n",
      "الحمد لله، شكرا على السؤال. binge\n",
      "الحمد لله، شكرا على السؤال. binge\n",
      "الحمد لله، شكرا على السؤال. binge\n",
      "الحمد لله، شكرا على السؤال. binge\n",
      "الحمد لله، شكرا على السؤال. binge\n",
      "الحمد لله، شكرا على السؤال. binge\n",
      "الحمد لله، شكرا على السؤال. binge\n",
      "الحمد لله، شكرا على السؤال. binge\n",
      "الحمد لله\n"
     ]
    }
   ],
   "source": [
    "# Generate with before ft\n",
    "prompt=\"السلام لباس؟\"\n",
    "messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    "formatted_prompt=tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "ids=tokenizer(formatted_prompt,return_tensors=\"pt\").to(device)\n",
    "output_ids=model.generate(**ids,max_new_tokens=120)\n",
    "output=tokenizer.decode(output_ids[0],skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb826af",
   "metadata": {},
   "source": [
    "# SFT + LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029c503",
   "metadata": {},
   "source": [
    "## Show Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee5741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show model architecture to select which layer we will apply lora\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f322c",
   "metadata": {},
   "source": [
    "## set LORA Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea0d665",
   "metadata": {},
   "source": [
    "* `r`:  This is the rank of the compressed matrices, Increasing this value will also increase the sizes of compressed matrices leading to less compression and thereby improved representative power. Values typically range between 4 and 64.\n",
    "\n",
    "* `lora_alpha`: Controls the amount of change that is added to the original weights. In essence, it balances the knowledge of the original model with that of the new task. A rule of thumb is to choose a value twice the size of r.\n",
    "\n",
    "* `target_modules`: Controls which layers to target. The LoRA procedure can choose to ignore specific layers, like specific projection layers. This can speed up training but reduce performance and vice versa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f00d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig\n",
    "lora_config=LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # apply lora only on q_proj and v_proj\n",
    "    bias=\"none\",\n",
    ")\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69804c63",
   "metadata": {},
   "source": [
    "# TODO @nouamane: add qlora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97d507",
   "metadata": {},
   "source": [
    "## Set Training Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75824d",
   "metadata": {},
   "source": [
    "* **What is gradient accumulation?**\n",
    "\n",
    "> Gradient accumulation is a way to virtually increase the batch size during training, which is very useful when the available GPU memory is insufficient to accommodate the desired batch size. In gradient accumulation, gradients are computed for smaller batches and accumulated (usually summed or averaged) over multiple iterations instead of updating the model weights after every batch. Once the accumulated gradients reach the target “virtual” batch size, the model weights are updated with the accumulated gradients.\n",
    "\n",
    "<center>\n",
    "\n",
    "![image](https://i.postimg.cc/x10Rv3tj/lora10.png\n",
    ")\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644cb9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=TrainingArguments(\n",
    "    output_dir=\"alatlas_instruct_lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=4,\n",
    "    bf16=True,\n",
    "    save_total_limit=2,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    hub_token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdbc6b",
   "metadata": {},
   "source": [
    "## SFT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67256870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09cea7c456044539e280175458e3c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455eb1e1fba3480589fadfbe5c0401de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08535290a1a4387b887510535059f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f03f847460f4ff6a904e0a4c44550cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig,SFTTrainer\n",
    "sft_trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df3e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540672"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_trainer.get_num_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb21a5d",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd0795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mth3elma2\u001b[0m (\u001b[33mth3elma2-enset-mohammedia\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250513_120703-uvnxr44v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/th3elma2-enset-mohammedia/huggingface/runs/uvnxr44v' target=\"_blank\">alatlas_instruct_lora</a></strong> to <a href='https://wandb.ai/th3elma2-enset-mohammedia/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/th3elma2-enset-mohammedia/huggingface' target=\"_blank\">https://wandb.ai/th3elma2-enset-mohammedia/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/th3elma2-enset-mohammedia/huggingface/runs/uvnxr44v' target=\"_blank\">https://wandb.ai/th3elma2-enset-mohammedia/huggingface/runs/uvnxr44v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:45, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.958900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.619700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.214900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.138900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.5668185424804686, metrics={'train_runtime': 316.2648, 'train_samples_per_second': 2.542, 'train_steps_per_second': 0.316, 'total_flos': 552914764402176.0, 'train_loss': 2.5668185424804686})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3378c",
   "metadata": {},
   "source": [
    "## Test Model After SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdad639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "السلام لباس\n",
      "ﭺassistant\n",
      "مرحبا، أنا هنا باش نعاونك فبزاف ديال الحوايج بحال اللغات و المهام اليومية.\n",
      "\n",
      "مثلا إلى كنتي كتقلب على شي معلومة فالإنترنت ولا عندنا مشكل مع شي حاجة خاصها تدار؟ غادي يبان ليك هادشي بالتفصيل.\n",
      "ولا يمكن تكون سولتيني أسئلة قبل من دابا وكنت عارف ش\n"
     ]
    }
   ],
   "source": [
    "# Generate with after ft\n",
    "prompt=\"السلام لباس\"\n",
    "messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    "formatted_prompt=tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "ids=tokenizer(formatted_prompt,return_tensors=\"pt\").to(device)\n",
    "output_ids=model.generate(**ids,max_new_tokens=100,\n",
    "                          repetition_penalty=1.2)\n",
    "output=tokenizer.decode(output_ids[0],skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3c7c8",
   "metadata": {},
   "source": [
    "## Push To THe HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f14ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f4c3eaa51c4e88921323fc3964736b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95e972a6e164631b825ada1e768aa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eac2d8f1c7e42b39a3ca3be0dda01f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702325f1788f43f8ab2be3f0e610c1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/abdeljalilELmajjodi/alatlas_instruct_lora/commit/b1786bda85f7332e4585f499d4bb7074e18b1ad9', commit_message='abdeljalilELmajjodi/alatlas-sft-lora-gra', commit_description='', oid='b1786bda85f7332e4585f499d4bb7074e18b1ad9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/abdeljalilELmajjodi/alatlas_instruct_lora', endpoint='https://huggingface.co', repo_type='model', repo_id='abdeljalilELmajjodi/alatlas_instruct_lora'), pr_revision=None, pr_num=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_trainer.push_to_hub(\"abdeljalilELmajjodi/alatlas-sft-lora-gra\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
