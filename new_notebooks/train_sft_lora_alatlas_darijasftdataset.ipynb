{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0e9b74",
   "metadata": {},
   "source": [
    "# Fine-tuning Al-Atlas with LoRA for Moroccan Darija\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/train_sft_lora_alatlas_darijasftdataset.ipynb)\n",
    "\n",
    "In this notebook, we'll learn how to fine-tune Al-Atlas, a Moroccan Darija language model, using LoRA (Low-Rank Adaptation). This is a memory-efficient way to adapt large language models to specific tasks.\n",
    "\n",
    "## What you'll learn:\n",
    "- What is LoRA and why we use it\n",
    "- How to fine-tune a language model on Moroccan Darija conversations\n",
    "- How to test the model before and after fine-tuning\n",
    "\n",
    "## Quick Concepts:\n",
    "- **LoRA**: A technique that makes fine-tuning large models more efficient by only updating a small number of parameters\n",
    "- **Fine-tuning**: Adapting a pre-trained model to a specific task or style\n",
    "- **Moroccan Darija**: The Moroccan Arabic dialect we're working with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236ba36",
   "metadata": {},
   "source": [
    "## What is LoRA?\n",
    "\n",
    "When fine-tuning large language models, we need to update many parameters (weights). LoRA makes this more efficient by:\n",
    "\n",
    "1. Breaking down the weight updates into smaller matrices\n",
    "2. Only updating these smaller matrices during training\n",
    "3. This saves memory and makes training faster\n",
    "\n",
    "![image](https://i.postimg.cc/7LtmYJ1H/lora1.png)\n",
    "\n",
    "Instead of updating all weights (left), we only update a small number of parameters (right)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50347b95",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00293f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q datasets trl transformers peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae17054",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We'll use the Darija SFT Dataset, which contains conversations in Moroccan Darija:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14077ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face (you'll need to get your token from huggingface.co)\n",
    "login()  # You'll be prompted to enter your token\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"HackAI-2025/Darija_SFT_Dataset\", split=\"train\")\n",
    "print(\"Dataset loaded:\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcbfa1c",
   "metadata": {},
   "source": [
    "Let's look at an example conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28986cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset[\"conversation\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa4d4e",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "We'll use Al-Atlas, a 0.5B parameter model trained on Moroccan Darija:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Select GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"atlasia/Al-Atlas-0.5B\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94c3ab",
   "metadata": {},
   "source": [
    "## Test Model Before Fine-tuning\n",
    "\n",
    "Let's see how the model responds before we fine-tune it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08adb7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"السلام لباس؟\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "ids = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(**ids, max_new_tokens=120)\n",
    "output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e32f7a",
   "metadata": {},
   "source": [
    "## Setup LoRA Configuration\n",
    "\n",
    "We'll configure LoRA to only update the key and value projection layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bedbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the update matrices\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    lora_dropout=0.05,  # Dropout probability\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Only update these layers\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206556a",
   "metadata": {},
   "source": [
    "## Setup Training Arguments\n",
    "\n",
    "We'll use gradient accumulation to handle larger effective batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"alatlas_instruct_lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=4,\n",
    "    bf16=True,\n",
    "    save_total_limit=2,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b44b57",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Now we'll start the fine-tuning process. This might take a while, so we've provided a pre-trained checkpoint you can use instead.\n",
    "\n",
    "To use the pre-trained checkpoint, skip the training cell and load the model from:\n",
    "```\n",
    "model_id = \"abdeljalilELmajjodi/alatlas-sft-lora-gra\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7040f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = dataset.select_columns(\"conversation\").rename_column(\"conversation\", \"messages\")\n",
    "\n",
    "# Initialize trainer\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=args\n",
    ")\n",
    "\n",
    "# Start training\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c0ede",
   "metadata": {},
   "source": [
    "## Test Model After Fine-tuning\n",
    "\n",
    "Let's see how the model responds after fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"السلام لباس\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "ids = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(**ids, max_new_tokens=100, repetition_penalty=1.2)\n",
    "output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ab398",
   "metadata": {},
   "source": [
    "## Save and Share Your Model\n",
    "\n",
    "You can save your fine-tuned model to Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae447a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save your model\n",
    "# sft_trainer.push_to_hub(\"your-username/your-model-name\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
