{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e07c61e",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/embedding_process_pdf.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148b8b8",
   "metadata": {},
   "source": [
    "# Building a Simple RAG System with PDFs\n",
    "\n",
    "In this notebook, you'll learn how to build a Retrieval Augmented Generation (RAG) system that can answer questions about PDF documents. This is a fundamental skill in GenAI that combines document processing, information retrieval, and language models.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what RAG is and why it's useful\n",
    "- Learn how to process PDF documents for AI\n",
    "- Build a simple question-answering system using RAG\n",
    "\n",
    "## What is RAG?\n",
    "RAG (Retrieval Augmented Generation) is a technique that combines:\n",
    "1. **Retrieval**: Finding relevant information from documents\n",
    "2. **Generation**: Using an AI model to generate answers based on that information\n",
    "\n",
    "This helps AI models provide more accurate and up-to-date answers by using specific information from your documents.\n",
    "\n",
    "Let's start by installing our required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9950bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q docling rapidocr_onnxruntime ollama scikit-learn python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96fd769",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment\n",
    "We'll need to set up our environment variables and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import ollama  # For running our language model locally\n",
    "\n",
    "# --- Docling Imports ---\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    RapidOcrOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "# --- Scikit-learn Imports ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55083e1a",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Let's set up our basic configuration. We'll use a simple PDF file for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89fd80",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Basic configuration\n",
    "PDF_PATH = Path(\"sample.pdf\")  # You'll need to upload your PDF\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# LLM Configuration\n",
    "MODEL = \"gemma:2b\"  # A smaller model that works well for our needs\n",
    "TEMPERATURE = 0.0  # Lower temperature for more focused answers\n",
    "TOP_K = 64\n",
    "TOP_P = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe43e1e",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Let's create some helper functions that we'll use in our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326416ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def call_model(prompt: str) -> str:\n",
    "    \"\"\"Calls the Ollama model with the specified prompt.\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"top_k\": TOP_K,\n",
    "                \"top_p\": TOP_P,\n",
    "            }\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling model: {e}\")\n",
    "        return f\"Error: Could not get response from model. {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e923e",
   "metadata": {},
   "source": [
    "## Building Our RAG System\n",
    "Now let's create our simple RAG system. We'll break this into steps:\n",
    "1. Process the PDF\n",
    "2. Split it into chunks\n",
    "3. Create a retriever\n",
    "4. Build our question-answering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789f83d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimpleRetriever:\n",
    "    \"\"\"A simple TF-IDF based retriever for finding relevant text chunks.\"\"\"\n",
    "    def __init__(self, texts: list[str]):\n",
    "        if not texts:\n",
    "            raise ValueError(\"Cannot initialize retriever with empty text list.\")\n",
    "        self.texts = texts\n",
    "        print(f\"Initializing retriever with {len(texts)} text chunks.\")\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        try:\n",
    "            self.text_vectors = self.vectorizer.fit_transform(self.texts)\n",
    "            print(\"TF-IDF vectors created successfully.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error during vectorization: {e}\")\n",
    "            self.text_vectors = None\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> tuple[list[str], list[float]]:\n",
    "        \"\"\"Finds the most relevant text chunks for a query.\"\"\"\n",
    "        if self.text_vectors is None:\n",
    "            return [], []\n",
    "        \n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_vector, self.text_vectors)[0]\n",
    "        \n",
    "        # Get top k most similar chunks\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        return [self.texts[i] for i in top_k_indices], [similarities[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf86a42",
   "metadata": {},
   "source": [
    "## Processing the PDF\n",
    "Let's set up our PDF processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf362e31",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configure PDF processing\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    generate_page_images=False,\n",
    "    do_ocr=True,\n",
    "    do_picture_description=False,  # Simplified for this example\n",
    "    ocr_options=RapidOcrOptions(),\n",
    ")\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73260a85",
   "metadata": {},
   "source": [
    "## Main RAG Pipeline\n",
    "Now let's put everything together to create our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73db891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_and_setup_rag(pdf_path: Path) -> SimpleRetriever:\n",
    "    \"\"\"Processes a PDF and sets up a RAG system.\"\"\"\n",
    "    print(f\"Processing PDF: {pdf_path}...\")\n",
    "    \n",
    "    # Convert PDF to text\n",
    "    result = converter.convert(pdf_path)\n",
    "    doc = result.document\n",
    "    \n",
    "    # Export to markdown\n",
    "    markdown_text = doc.export_to_markdown()\n",
    "    \n",
    "    # Simple chunking by paragraphs\n",
    "    chunks = [chunk.strip() for chunk in markdown_text.split('\\n\\n') if chunk.strip()]\n",
    "    \n",
    "    # Create retriever\n",
    "    retriever = SimpleRetriever(chunks)\n",
    "    return retriever\n",
    "\n",
    "def ask_question(query: str, retriever: SimpleRetriever, k: int = 3) -> str:\n",
    "    \"\"\"Asks a question using our RAG system.\"\"\"\n",
    "    print(f\"\\nQuestion: {query}\")\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    retrieved_texts, scores = retriever.retrieve(query, k=k)\n",
    "    \n",
    "    if not retrieved_texts:\n",
    "        return \"Could not find relevant information.\"\n",
    "    \n",
    "    # Create prompt with context\n",
    "    context = \"\\n\\n\".join([f\"Context {i+1}:\\n{text}\" for i, text in enumerate(retrieved_texts)])\n",
    "    prompt = f\"\"\"Use the following context to answer the question. If you can't answer based on the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Get answer from model\n",
    "    answer = call_model(prompt)\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(textwrap.fill(answer, width=80))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa147ca",
   "metadata": {},
   "source": [
    "## Let's Try It Out!\n",
    "Now we can use our RAG system to answer questions about the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a140c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Upload your PDF file first\n",
    "# retriever = process_pdf_and_setup_rag(PDF_PATH)\n",
    "\n",
    "# Example questions (uncomment after uploading PDF):\n",
    "# ask_question(\"What is the main topic of this document?\", retriever)\n",
    "# ask_question(\"What are the key points discussed?\", retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f33f343",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Try different chunking strategies\n",
    "- Experiment with different retrieval methods\n",
    "- Add more sophisticated context processing\n",
    "- Evaluate the system's performance"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
