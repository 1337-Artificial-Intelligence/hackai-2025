{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75473b06",
   "metadata": {},
   "source": [
    "# Web Scraping with BeautifulSoup4 üï∑Ô∏è\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NouamaneTazi/hackai-challenges/blob/main/new_notebooks/data_scraping_bs4_goudma.ipynb)\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "- Extract data from websites using BeautifulSoup4\n",
    "- Parse HTML content\n",
    "- Save the scraped data to a structured format\n",
    "- Share your dataset on HuggingFace ü§ó\n",
    "\n",
    "Time to complete: ~45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936d9f1",
   "metadata": {},
   "source": [
    "## What is Web Scraping? üåê\n",
    "\n",
    "Web scraping is like having a robot that can read websites and collect information for you. It's useful for:\n",
    "- Gathering data for analysis\n",
    "- Creating datasets for AI training\n",
    "- Monitoring website changes\n",
    "- Automating data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18913975",
   "metadata": {},
   "source": [
    "## Setup üõ†Ô∏è\n",
    "\n",
    "First, let's install the required packages:\n",
    "- `beautifulsoup4`: Helps us parse and navigate HTML\n",
    "- `requests`: Lets us download web pages\n",
    "- `pandas`: For organizing our data\n",
    "- `datasets`: For sharing on HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install beautifulsoup4 requests pandas datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b533ec",
   "metadata": {},
   "source": [
    "## Import Libraries üìö\n",
    "\n",
    "Let's import the tools we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a48ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  # For parsing HTML\n",
    "import requests  # For downloading web pages\n",
    "import pandas as pd  # For data organization\n",
    "from tqdm import tqdm  # For progress bars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1789a41f",
   "metadata": {},
   "source": [
    "## Our Target üéØ\n",
    "\n",
    "We'll scrape news articles from [Goud.ma](https://www.goud.ma), a Moroccan news website. We'll collect:\n",
    "- Article titles\n",
    "- Article content\n",
    "- Article images\n",
    "\n",
    "Before beginning our scraping, we need to analyze the HTML of our target website to understand its structure.\n",
    "As shown in the image below, we'll target:\n",
    "1. The `article` elements with class `card`\n",
    "2. Inside each article, we'll find the `a` tag with class `stretched-link` to get the article URL\n",
    "3. Then we'll extract the content from each article page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL we want to scrape\n",
    "target_url = \"https://www.goud.ma/topics/%d8%a7%d9%84%d8%b1%d8%a6%d9%8a%d8%b3%d9%8a%d8%a9/\"\n",
    "\n",
    "# Send a request to the website\n",
    "# The User-Agent header helps identify our request\n",
    "response = requests.get(target_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"‚úÖ Successfully connected to the website!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to the website\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23224989",
   "metadata": {},
   "source": [
    "## Parsing the HTML üß©\n",
    "\n",
    "Now that we have the webpage, let's parse it with BeautifulSoup. We'll find all article elements with class \"card\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cce3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BeautifulSoup object to parse the HTML\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all article elements with class \"card\"\n",
    "# We'll get the first 6 articles\n",
    "articles = soup.find_all(\"article\", class_=\"card\")[:6]\n",
    "print(f\"Found {len(articles)} articles!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f268dab",
   "metadata": {},
   "source": [
    "## Extracting Article Links üîó\n",
    "\n",
    "As shown in the image, we need to:\n",
    "1. Find the `a` tag with class `stretched-link` inside each article\n",
    "2. Extract the `href` attribute to get the article URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a34821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract links from articles\n",
    "article_links = [\n",
    "    article.find(\"a\", class_=\"stretched-link\").get(\"href\")\n",
    "    for article in articles\n",
    "]\n",
    "\n",
    "print(\"Article links:\")\n",
    "for i, link in enumerate(article_links, 1):\n",
    "    print(f\"{i}. {link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc80b1",
   "metadata": {},
   "source": [
    "## Scraping Article Content üìù\n",
    "\n",
    "Now let's scrape the content from each article. As shown in the image, we'll:\n",
    "1. Visit each article page\n",
    "2. Find the title in the `h1` tag with class `entry-title`\n",
    "3. Find the content in the `div` with class `post-content`\n",
    "4. Find the image in the `img` tag with class `img-fluid wp-post-image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store our data\n",
    "data = {\n",
    "    \"titles\": [],\n",
    "    \"content\": [],\n",
    "    \"images\": []\n",
    "}\n",
    "\n",
    "# Scrape each article\n",
    "for link in tqdm(article_links, desc=\"Scraping articles\"):\n",
    "    # Get the article page\n",
    "    article_response = requests.get(link, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract data\n",
    "    title = article_soup.find(\"h1\", class_=\"entry-title\").text\n",
    "    content = article_soup.find(\"div\", class_=\"post-content\").text.strip()\n",
    "    image = article_soup.find(\"img\", class_=\"img-fluid wp-post-image\").get(\"src\")\n",
    "    \n",
    "    # Save data\n",
    "    data[\"titles\"].append(title)\n",
    "    data[\"content\"].append(content)\n",
    "    data[\"images\"].append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c1b48",
   "metadata": {},
   "source": [
    "## Organizing the Data üìä\n",
    "\n",
    "Let's put our data in a pandas DataFrame for better organization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4a41f",
   "metadata": {},
   "source": [
    "## Saving to HuggingFace ü§ó\n",
    "\n",
    "Finally, let's share our dataset on HuggingFace. This makes it easy to:\n",
    "- Share your data with others\n",
    "- Use it in other AI projects\n",
    "- Track changes to your dataset\n",
    "\n",
    "To use this part, you'll need to:\n",
    "1. Create a HuggingFace account\n",
    "2. Get your write token from https://huggingface.co/settings/tokens\n",
    "3. Replace `HF_WRITE_TOKEN` with your token\n",
    "4. Change `HF_DATASET_REPO` to your username/dataset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert pandas DataFrame to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Uncomment and fill these to push to HuggingFace\n",
    "# HF_WRITE_TOKEN = \"your_token_here\"  # Get from https://huggingface.co/settings/tokens\n",
    "# HF_DATASET_REPO = \"your_username/dataset_name\"\n",
    "# dataset.push_to_hub(HF_DATASET_REPO, token=HF_WRITE_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a593be2",
   "metadata": {},
   "source": [
    "## Congratulations! üéâ\n",
    "\n",
    "You've successfully:\n",
    "- Scraped a website using BeautifulSoup4\n",
    "- Extracted structured data\n",
    "- Organized it in a pandas DataFrame\n",
    "- Prepared it for sharing on HuggingFace\n",
    "\n",
    "## Next Steps üöÄ\n",
    "- Try scraping a different website\n",
    "- Add more data fields (like dates, authors, etc.)\n",
    "- Clean the text data (remove extra spaces, special characters)\n",
    "- Create visualizations of your data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
