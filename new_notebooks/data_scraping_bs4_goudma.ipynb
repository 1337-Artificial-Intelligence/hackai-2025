{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee41b418",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Web Scraping with BeautifulSoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad948ac",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/data_scraping_bs4_goudma.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9fd6b",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand what web scraping is and why it's useful\n",
    "- Use BeautifulSoup4 to parse and extract data from websites\n",
    "- Create a simple dataset from scraped web content\n",
    "- Save your scraped data to a structured format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ecfef",
   "metadata": {},
   "source": [
    "## What is Web Scraping?\n",
    "Web scraping is the process of automatically extracting data from websites. It's like having a robot that can read web pages and collect information for you. This is useful for:\n",
    "- Collecting data for analysis\n",
    "- Monitoring prices or news\n",
    "- Creating datasets for machine learning\n",
    "- Automating data collection tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cbba1b",
   "metadata": {},
   "source": [
    "## What is BeautifulSoup4?\n",
    "BeautifulSoup4 is a Python library that helps us parse (read and understand) HTML and XML documents. Think of it as a tool that can:\n",
    "- Take messy HTML code and make it organized\n",
    "- Help us find specific elements on a webpage\n",
    "- Extract text, links, and other data easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97fb91",
   "metadata": {},
   "source": [
    "## Let's Get Started!\n",
    "First, we need to install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e678a",
   "metadata": {
    "id": "NwvNYma2mYiK"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install beautifulsoup4 requests pandas tqdm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957cfb9",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "We'll use:\n",
    "- `requests`: to download web pages\n",
    "- `BeautifulSoup`: to parse HTML\n",
    "- `pandas`: to organize our data\n",
    "- `tqdm`: to show progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1e511",
   "metadata": {
    "id": "YUacjMzzn4o6"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acfacc4",
   "metadata": {},
   "source": [
    "## Our Goal\n",
    "We'll scrape news articles from goud.ma, a Moroccan news website. We'll collect:\n",
    "- Article titles\n",
    "- Article content\n",
    "- Article images\n",
    "- Article links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aabe1d",
   "metadata": {},
   "source": [
    "TODO: Add screenshot of goud.ma homepage showing the articles we want to scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed1622",
   "metadata": {},
   "source": [
    "## Step 1: Download the Web Page\n",
    "First, we need to get the HTML content of the webpage. We'll use `requests` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9bb93b",
   "metadata": {
    "id": "HoPOQznttz7g"
   },
   "outputs": [],
   "source": [
    "# Send a request to the website\n",
    "target = \"https://www.goud.ma/topics/%d8%a7%d9%84%d8%b1%d8%a6%d9%8a%d8%b3%d9%8a%d8%a9/\"\n",
    "page = requests.get(target, headers={\"User-Agent\": \"XY\"})\n",
    "\n",
    "# Check if the request was successful\n",
    "if page.status_code == 200:\n",
    "    print(\"‚úÖ Successfully downloaded the webpage!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to download the webpage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85cf813",
   "metadata": {},
   "source": [
    "## Step 2: Parse the HTML\n",
    "Now we'll use BeautifulSoup to parse the HTML and make it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a52350",
   "metadata": {
    "id": "L3_MQbeqzJPC"
   },
   "outputs": [],
   "source": [
    "# Parse the HTML content\n",
    "page_soup = BeautifulSoup(page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646b92f",
   "metadata": {},
   "source": [
    "## Step 3: Find Articles\n",
    "We'll look for article elements with the class \"card\". These contain our news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941aef2",
   "metadata": {
    "id": "GHP8O2hOzo4F"
   },
   "outputs": [],
   "source": [
    "# Find all article elements\n",
    "articles = page_soup.find_all(name=\"article\", class_=\"card\")[:6]  # Get first 6 articles\n",
    "print(f\"Found {len(articles)} articles!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777a102",
   "metadata": {},
   "source": [
    "## Step 4: Extract Article Links\n",
    "For each article, we'll get its link to access the full content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183c892",
   "metadata": {
    "id": "1rxW-0dHoPZ1"
   },
   "outputs": [],
   "source": [
    "# Extract links from articles\n",
    "articles_links = [\n",
    "    article.find(\"a\", class_=\"stretched-link\").get(\"href\")\n",
    "    for article in articles\n",
    "]\n",
    "print(\"Article links:\", articles_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c599e",
   "metadata": {},
   "source": [
    "## Step 5: Extract Article Content\n",
    "Now we'll visit each article page and extract:\n",
    "- Title\n",
    "- Content\n",
    "- Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e44ae",
   "metadata": {
    "id": "YQIRlaVG3Slr"
   },
   "outputs": [],
   "source": [
    "# Let's look at one article first\n",
    "link = articles_links[0]\n",
    "article_page = requests.get(link, headers={\"User-Agent\": \"XY\"}).text\n",
    "article_soup = BeautifulSoup(article_page, \"html.parser\")\n",
    "\n",
    "# Extract data from the article\n",
    "article_img = article_soup.find(\"img\", class_=\"img-fluid wp-post-image\").get(\"src\")\n",
    "article_title = article_soup.find(\"h1\", class_=\"entry-title\").text\n",
    "article_content = article_soup.find(\"div\", class_=\"post-content\").text.strip()\n",
    "\n",
    "print(f\"Title: {article_title}\")\n",
    "print(f\"Image URL: {article_img}\")\n",
    "print(f\"Content preview: {article_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892eef25",
   "metadata": {},
   "source": [
    "## Step 6: Scrape All Articles\n",
    "Now let's do this for all articles and save the data in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94129755",
   "metadata": {
    "id": "895z_7DI_ajx"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store our data\n",
    "data = {\"titles\": [], \"content\": [], \"images\": []}\n",
    "\n",
    "# Scrape each article\n",
    "for link in tqdm(articles_links, desc=\"Scraping articles\"):\n",
    "    # Get article page\n",
    "    page_html = requests.get(link, headers={\"User-Agent\": \"XY\"}).text\n",
    "    page_soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "    \n",
    "    # Extract data\n",
    "    img = page_soup.find(\"img\", class_=\"img-fluid wp-post-image\").get(\"src\")\n",
    "    title = page_soup.find(\"h1\", class_=\"entry-title\").text\n",
    "    content = page_soup.find(\"div\", class_=\"post-content\").text.strip()\n",
    "    \n",
    "    # Save data\n",
    "    data[\"titles\"].append(title)\n",
    "    data[\"content\"].append(content)\n",
    "    data[\"images\"].append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17168b4d",
   "metadata": {},
   "source": [
    "## Step 7: Save the Data\n",
    "Let's save our scraped data in a pandas DataFrame for easy viewing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0cbec1",
   "metadata": {
    "id": "ragUnpUMCNYl"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f24a169",
   "metadata": {},
   "source": [
    "## Congratulations! üéâ\n",
    "You've successfully:\n",
    "1. Scraped a website using BeautifulSoup4\n",
    "2. Extracted structured data from web pages\n",
    "3. Created a dataset from web content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412356f9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Try scraping a different website\n",
    "- Add more data fields (like dates, authors)\n",
    "- Save the data to a CSV file\n",
    "- Use the data for analysis or machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343109d",
   "metadata": {},
   "source": [
    "## Optional: Save to HuggingFace\n",
    "If you want to share your dataset, you can upload it to HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8c4cb",
   "metadata": {
    "id": "IEfW3it9BdzA"
   },
   "outputs": [],
   "source": [
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b125853",
   "metadata": {
    "id": "aAKgtv2LCf-j"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "# To upload to HuggingFace, you'll need:\n",
    "# 1. A HuggingFace account\n",
    "# 2. A write token\n",
    "# 3. A dataset name\n",
    "# Uncomment and fill these to upload:\n",
    "# HF_WRITE_TOKEN = \"\"  # Your HuggingFace write token\n",
    "# HF_DATASET_REPO = \"username/datasetname\"  # Your dataset name\n",
    "# ds.push_to_hub(HF_DATASET_REPO, token=HF_WRITE_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
