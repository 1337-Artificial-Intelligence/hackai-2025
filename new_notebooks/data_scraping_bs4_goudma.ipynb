{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b2de12",
   "metadata": {},
   "source": [
    "# Web Scraping with BeautifulSoup4 üï∑Ô∏è\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NouamaneTazi/hackai-challenges/blob/main/new_notebooks/data_scraping_bs4_goudma.ipynb)\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "- Extract data from websites using BeautifulSoup4\n",
    "- Parse HTML content\n",
    "- Save the scraped data to a structured format\n",
    "- Share your dataset on HuggingFace ü§ó\n",
    "\n",
    "Time to complete: ~45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44163e2a",
   "metadata": {},
   "source": [
    "## What is Web Scraping? üåê\n",
    "\n",
    "Web scraping is like having a robot that can read websites and collect information for you. It's useful for:\n",
    "- Gathering data for analysis\n",
    "- Creating datasets for AI training\n",
    "- Monitoring website changes\n",
    "- Automating data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578058f2",
   "metadata": {},
   "source": [
    "## Setup üõ†Ô∏è\n",
    "\n",
    "First, let's install the required packages:\n",
    "- `beautifulsoup4`: Helps us parse and navigate HTML\n",
    "- `requests`: Lets us download web pages\n",
    "- `pandas`: For organizing our data\n",
    "- `datasets`: For sharing on HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install beautifulsoup4 requests pandas datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e41b7a",
   "metadata": {},
   "source": [
    "## Import Libraries üìö\n",
    "\n",
    "Let's import the tools we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84725c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  # For parsing HTML\n",
    "import requests  # For downloading web pages\n",
    "import pandas as pd  # For data organization\n",
    "from tqdm import tqdm  # For progress bars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940a0b0",
   "metadata": {},
   "source": [
    "## Our Target üéØ\n",
    "\n",
    "We'll scrape news articles from [Goud.ma](https://www.goud.ma), a Moroccan news website. We'll collect:\n",
    "- Article titles\n",
    "- Article content\n",
    "- Article images\n",
    "\n",
    "Before beginning our scraping, we need to analyze the HTML of our target website to understand its structure.\n",
    "As shown in the image below, we'll target:\n",
    "1. The `article` elements with class `card`\n",
    "2. Inside each article, we'll find the `a` tag with class `stretched-link` to get the article URL\n",
    "3. Then we'll extract the content from each article page\n",
    "\n",
    "TODO: Add image showing the HTML structure of the main page with arrows pointing to:\n",
    "- article elements with class \"card\"\n",
    "- a tags with class \"stretched-link\"\n",
    "- href attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624629c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL we want to scrape\n",
    "target_url = \"https://www.goud.ma/topics/%d8%a7%d9%84%d8%b1%d8%a6%d9%8a%d8%b3%d9%8a%d8%a9/\"\n",
    "\n",
    "# Send a request to the website\n",
    "# The User-Agent header helps identify our request\n",
    "response = requests.get(target_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"‚úÖ Successfully connected to the website!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to the website\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d08b0",
   "metadata": {},
   "source": [
    "## Parsing the HTML üß©\n",
    "\n",
    "Now that we have the webpage, let's parse it with BeautifulSoup. We'll find all article elements with class \"card\":\n",
    "\n",
    "TODO: Add image showing the BeautifulSoup object structure with:\n",
    "- HTML tree visualization\n",
    "- Highlighted article elements\n",
    "- Class attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BeautifulSoup object to parse the HTML\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all article elements with class \"card\"\n",
    "# We'll get the first 6 articles\n",
    "articles = soup.find_all(\"article\", class_=\"card\")[:6]\n",
    "print(f\"Found {len(articles)} articles!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e3bb48",
   "metadata": {},
   "source": [
    "## Extracting Article Links üîó\n",
    "\n",
    "As shown in the image, we need to:\n",
    "1. Find the `a` tag with class `stretched-link` inside each article\n",
    "2. Extract the `href` attribute to get the article URL\n",
    "\n",
    "TODO: Add image showing:\n",
    "- Article HTML structure\n",
    "- Highlighted a tag with class \"stretched-link\"\n",
    "- Arrow pointing to href attribute\n",
    "- Example of extracted URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract links from articles\n",
    "article_links = [\n",
    "    article.find(\"a\", class_=\"stretched-link\").get(\"href\")\n",
    "    for article in articles\n",
    "]\n",
    "\n",
    "print(\"Article links:\")\n",
    "for i, link in enumerate(article_links, 1):\n",
    "    print(f\"{i}. {link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae34b19",
   "metadata": {},
   "source": [
    "## Scraping Article Content üìù\n",
    "\n",
    "Now let's scrape the content from each article. As shown in the image, we'll:\n",
    "1. Visit each article page\n",
    "2. Find the title in the `h1` tag with class `entry-title`\n",
    "3. Find the content in the `div` with class `post-content`\n",
    "4. Find the image in the `img` tag with class `img-fluid wp-post-image`\n",
    "\n",
    "TODO: Add image showing article page HTML structure with:\n",
    "- Highlighted h1 tag with class \"entry-title\"\n",
    "- Highlighted div with class \"post-content\"\n",
    "- Highlighted img tag with class \"img-fluid wp-post-image\"\n",
    "- Arrows pointing to text content and image source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c1376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store our data\n",
    "data = {\n",
    "    \"titles\": [],\n",
    "    \"content\": [],\n",
    "    \"images\": []\n",
    "}\n",
    "\n",
    "# Scrape each article\n",
    "for link in tqdm(article_links, desc=\"Scraping articles\"):\n",
    "    # Get the article page\n",
    "    article_response = requests.get(link, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract data\n",
    "    title = article_soup.find(\"h1\", class_=\"entry-title\").text\n",
    "    content = article_soup.find(\"div\", class_=\"post-content\").text.strip()\n",
    "    image = article_soup.find(\"img\", class_=\"img-fluid wp-post-image\").get(\"src\")\n",
    "    \n",
    "    # Save data\n",
    "    data[\"titles\"].append(title)\n",
    "    data[\"content\"].append(content)\n",
    "    data[\"images\"].append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5efa19f",
   "metadata": {},
   "source": [
    "## Organizing the Data üìä\n",
    "\n",
    "Let's put our data in a pandas DataFrame for better organization:\n",
    "\n",
    "TODO: Add image showing:\n",
    "- Example of raw scraped data\n",
    "- The resulting pandas DataFrame\n",
    "- Highlighted columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee4c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7c779",
   "metadata": {},
   "source": [
    "## Saving to HuggingFace ü§ó\n",
    "\n",
    "Finally, let's share our dataset on HuggingFace. This makes it easy to:\n",
    "- Share your data with others\n",
    "- Use it in other AI projects\n",
    "- Track changes to your dataset\n",
    "\n",
    "To use this part, you'll need to:\n",
    "1. Create a HuggingFace account\n",
    "2. Get your write token from https://huggingface.co/settings/tokens\n",
    "3. Replace `HF_WRITE_TOKEN` with your token\n",
    "4. Change `HF_DATASET_REPO` to your username/dataset name\n",
    "\n",
    "TODO: Add image showing:\n",
    "- HuggingFace dataset page\n",
    "- Where to find the write token\n",
    "- How to create a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcf41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert pandas DataFrame to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Uncomment and fill these to push to HuggingFace\n",
    "# HF_WRITE_TOKEN = \"your_token_here\"  # Get from https://huggingface.co/settings/tokens\n",
    "# HF_DATASET_REPO = \"your_username/dataset_name\"\n",
    "# dataset.push_to_hub(HF_DATASET_REPO, token=HF_WRITE_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b1a33",
   "metadata": {},
   "source": [
    "## Congratulations! üéâ\n",
    "\n",
    "You've successfully:\n",
    "- Scraped a website using BeautifulSoup4\n",
    "- Extracted structured data\n",
    "- Organized it in a pandas DataFrame\n",
    "- Prepared it for sharing on HuggingFace\n",
    "\n",
    "## Next Steps üöÄ\n",
    "- Try scraping a different website\n",
    "- Add more data fields (like dates, authors, etc.)\n",
    "- Clean the text data (remove extra spaces, special characters)\n",
    "- Create visualizations of your data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
