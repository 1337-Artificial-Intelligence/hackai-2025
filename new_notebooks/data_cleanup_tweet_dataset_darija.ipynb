{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b0fa00",
   "metadata": {},
   "source": [
    "# Data Cleanup - Tweet Dataset (Darija)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NouamaneTazi/hackai-challenges/blob/main/new_notebooks/data_cleanup_tweet_dataset_darija.ipynb)\n",
    "\n",
    "## üìö Quick Glossary\n",
    "- **Dataset**: A collection of data (in our case, tweets)\n",
    "- **Preprocessing**: Cleaning and preparing data for analysis\n",
    "- **Tokenization**: Breaking text into individual words\n",
    "- **Stop Words**: Common words that don't add much meaning (like \"the\", \"and\")\n",
    "- **N-grams**: Groups of n words that appear together\n",
    "\n",
    "## ‚è±Ô∏è Time Estimate\n",
    "- Part 1 (Data Cleaning): 30 minutes\n",
    "- Part 2 (Analysis): 30 minutes\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "1. Learn how to clean text data\n",
    "2. Understand basic text preprocessing steps\n",
    "3. Create visualizations of text data\n",
    "4. Analyze word patterns in Darija tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a907f1f",
   "metadata": {},
   "source": [
    "üìå Challenge Description:\n",
    "\n",
    "In this challenge, you will clean a tweet dataset from Hugging Face (`shmuhammad/AfriSenti-twitter-sentiment`), focusing on the Moroccan Darija subset. The goal is to preprocess the text data by removing emojis, usernames, and applying custom list of Darija stop words. The cleaned data will then be ready for n-gram analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8e0a4",
   "metadata": {},
   "source": [
    "üìä Dataset Summary:\n",
    "AfriSenti is the largest sentiment analysis dataset for under-represented African languages, covering 110,000+ annotated tweets in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yoruba)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c383f",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup\n",
    "First, let's install and import the necessary tools. Run this cell to get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b3101",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install required packages\n",
    "required_packages = [\n",
    "    \"datasets\",\n",
    "    \"pandas\",\n",
    "    \"regex\",\n",
    "    \"nltk\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"wordcloud\",\n",
    "    \"arabic-reshaper\",\n",
    "    \"python-bidi\"\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "# Import libraries\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from tqdm.notebook import tqdm  # For progress bars\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a227dc9a",
   "metadata": {},
   "source": [
    "# Part 1: Data Cleaning (30 minutes)\n",
    "\n",
    "In this section, we'll clean our tweet data by:\n",
    "1. Loading the dataset\n",
    "2. Removing emojis\n",
    "3. Removing usernames\n",
    "4. Removing Latin characters\n",
    "5. Removing punctuation\n",
    "6. Tokenizing the text\n",
    "7. Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009922d8",
   "metadata": {
    "id": "g8FqPxdqHYib"
   },
   "source": [
    "üìå Challenge Description:\n",
    "\n",
    "In this challenge, you will clean a tweet dataset from Hugging Face (`shmuhammad/AfriSenti-twitter-sentiment`), focusing on the Moroccan Darija subset. The goal is to preprocess the text data by removing emojis, usernames, and applying custom list of Darija stop words. The cleaned data will then be ready for n-gram analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b17dd",
   "metadata": {
    "id": "mNThxcvS2O4c"
   },
   "source": [
    "üìä Dataset Summary:\n",
    "AfriSenti is the largest sentiment analysis dataset for under-represented African languages, covering 110,000+ annotated tweets in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yoruba)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a608e",
   "metadata": {
    "id": "vVGZTiRuZWjF"
   },
   "source": [
    "# Part 1: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d2360",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZKBt4Wb2NhP",
    "lines_to_next_cell": 2,
    "outputId": "78af0152-63bf-43ec-99e8-a6cd671b9825"
   },
   "outputs": [],
   "source": [
    "#Installing Necessary Packages\n",
    "!pip install datasets pandas regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc560a",
   "metadata": {
    "id": "6-Nt7JiyZWjH"
   },
   "outputs": [],
   "source": [
    "#Import Necessary Packages\n",
    "from  datasets  import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341b751",
   "metadata": {
    "id": "0Fg2oYRo2rlx"
   },
   "source": [
    "## 1. Loading the Dataset\n",
    "Let's load our Darija tweets dataset from Hugging Face. This dataset contains tweets in Moroccan Arabic (Darija)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491ed7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbIaoq5E2xC2",
    "lines_to_next_cell": 1,
    "outputId": "08070100-8278-4a40-9541-108d1136ae7f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the `shmuhammad/AfriSenti-twitter-sentiment` dataset from the Hugging Face `datasets` library, and specify the \"arq\" subset for Moroccan Darija.\n",
    "\"\"\"\n",
    "print(\"Loading dataset...\")\n",
    "darija_dataset = load_dataset(\"shmuhammad/AfriSenti-twitter-sentiment\", \"arq\")\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation\n",
    "print(\"Converting to DataFrame...\")\n",
    "darija_dataset_df = pd.DataFrame(darija_dataset['train'])\n",
    "\n",
    "# Show the first few tweets\n",
    "print(\"\\nFirst 5 tweets in the dataset:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcb53e3",
   "metadata": {
    "id": "3VEqLcwbZWjJ"
   },
   "source": [
    "## 2. Removing Emojis\n",
    "Emojis don't add much value to our text analysis. Let's remove them using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4c554",
   "metadata": {
    "id": "i-kfuQSjZWjK"
   },
   "outputs": [],
   "source": [
    "def remove_emojis(tweet):\n",
    "    \"\"\"\n",
    "    Remove emojis from text using regular expressions.\n",
    "    Args:\n",
    "        tweet (str): Input text containing emojis\n",
    "    Returns:\n",
    "        str: Text with emojis removed\n",
    "    \"\"\"\n",
    "    emoj = re.compile(\"[\"\n",
    "       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                      u\"\\U0001F923\"\n",
    "                      u\"\\U0001F97A\"\n",
    "                      u\"\\U0001F914\"\"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', tweet)\n",
    "\n",
    "# Remove emojis with progress bar\n",
    "print(\"Removing emojis...\")\n",
    "darija_dataset_df['tweet'] = [remove_emojis(tweet) for tweet in tqdm(darija_dataset_df['tweet'])]\n",
    "\n",
    "# Show example of cleaned tweets\n",
    "print(\"\\nExample of tweets after emoji removal:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede92885",
   "metadata": {
    "id": "k7zlZCxVZWjL"
   },
   "source": [
    "You can use this ressource emojis unicode: https://apps.timwhitlock.info/emoji/tables/unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f68b18",
   "metadata": {
    "id": "lg5hZSRAZWjL"
   },
   "outputs": [],
   "source": [
    "# remove the emojis from the dataset\n",
    "darija_dataset_df['tweet'] = darija_dataset_df['tweet'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde38fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syymGzvvZWjM",
    "outputId": "f0cee374-b1d2-4324-a20e-08beecf69f4d"
   },
   "outputs": [],
   "source": [
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403995cb",
   "metadata": {
    "id": "NsLwBXGKZWjN"
   },
   "source": [
    "## 3. Removing Usernames\n",
    "Twitter usernames (starting with @) should be removed as they don't contribute to the text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd67a7a4",
   "metadata": {
    "id": "K80cxzP3ZWjN"
   },
   "outputs": [],
   "source": [
    "def remove_user(tweet):\n",
    "    \"\"\"\n",
    "    Remove Twitter usernames from text.\n",
    "    Args:\n",
    "        tweet (str): Input text containing usernames\n",
    "    Returns:\n",
    "        str: Text with usernames removed\n",
    "    \"\"\"\n",
    "    user_re = \"@[A-Za-z0-9]+\"\n",
    "    return re.sub(user_re, ' ', tweet)\n",
    "\n",
    "# Remove usernames with progress bar\n",
    "print(\"Removing usernames...\")\n",
    "darija_dataset_df['tweet'] = [remove_user(tweet) for tweet in tqdm(darija_dataset_df['tweet'])]\n",
    "\n",
    "# Show example of cleaned tweets\n",
    "print(\"\\nExample of tweets after username removal:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39171a59",
   "metadata": {
    "id": "FlH4Kx3OZWjP"
   },
   "source": [
    "## 4. Removing Latin Characters\n",
    "Since we're analyzing Darija text, we'll remove Latin characters to focus on Arabic text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12959a",
   "metadata": {
    "id": "5_jyrkVmZWjP"
   },
   "outputs": [],
   "source": [
    "def remove_latin(tweet):\n",
    "    \"\"\"\n",
    "    Remove Latin characters from text.\n",
    "    Args:\n",
    "        tweet (str): Input text containing Latin characters\n",
    "    Returns:\n",
    "        str: Text with Latin characters removed\n",
    "    \"\"\"\n",
    "    latin_re = \"[A-Za-z]+\"\n",
    "    return re.sub(latin_re, ' ', tweet)\n",
    "\n",
    "# Remove Latin characters with progress bar\n",
    "print(\"Removing Latin characters...\")\n",
    "darija_dataset_df['tweet'] = [remove_latin(tweet) for tweet in tqdm(darija_dataset_df['tweet'])]\n",
    "\n",
    "# Show example of cleaned tweets\n",
    "print(\"\\nExample of tweets after Latin character removal:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03cbf46",
   "metadata": {
    "id": "WYHglBD-ZWjS"
   },
   "source": [
    "## Removing Dublicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68589f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "darija_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5d274",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txDycFxH6RMY",
    "lines_to_next_cell": 1,
    "outputId": "b6436162-eab7-48bf-d3a1-cd839077e4d7"
   },
   "outputs": [],
   "source": [
    "darija_dataset_df.drop_duplicates(inplace = True)\n",
    "darija_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2d1f7",
   "metadata": {
    "id": "gUGlJd6pZWjP"
   },
   "source": [
    "## 6. Removing Punctuation\n",
    "Punctuation marks don't add much value to our analysis, so let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef45238",
   "metadata": {
    "id": "OUao2kxzZWjQ"
   },
   "outputs": [],
   "source": [
    "def remove_ponct(tweet):\n",
    "    \"\"\"\n",
    "    Remove punctuation from text.\n",
    "    Args:\n",
    "        tweet (str): Input text containing punctuation\n",
    "    Returns:\n",
    "        str: Text with punctuation removed\n",
    "    \"\"\"\n",
    "    ponct_re = \"[^\\w\\s]+\"\n",
    "    return re.sub(ponct_re, ' ', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5e1ad",
   "metadata": {
    "id": "hvUmWQ6sZWjQ"
   },
   "outputs": [],
   "source": [
    "#remove ponctuation\n",
    "darija_dataset_df['tweet'] = darija_dataset_df['tweet'].apply(remove_ponct)\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4bc02",
   "metadata": {
    "id": "PJMkjpdlZWjT"
   },
   "source": [
    "## 7. Tokenization\n",
    "Now we'll break down the tweets into individual words (tokens) for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9856b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "T6ooyvj0ZWjT",
    "outputId": "e51c9d0e-9539-4a0e-c012-e569d70b45bc"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "def Tokenize(tweet):\n",
    "    return word_tokenize(tweet)\n",
    "\n",
    "darija_dataset_df[\"tweet_token\"] = darija_dataset_df['tweet'].apply(Tokenize)\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399a0ba",
   "metadata": {
    "id": "wJwUQg-6ZWjQ"
   },
   "source": [
    "## 8. Removing Stop Words\n",
    "Stop words are common words that don't add much meaning to our analysis. Let's remove them using a custom Darija stop words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Darija stop words\n",
    "print(\"Loading Darija stop words...\")\n",
    "darija_stop_words_df = pd.read_csv(\"/content/darija_stop_words.csv\")\n",
    "darija_stop_words = darija_stop_words_df['word'].tolist()\n",
    "\n",
    "def remove_stop(all_tokens, stop_lst):\n",
    "    \"\"\"\n",
    "    Remove stop words from a list of tokens.\n",
    "    Args:\n",
    "        all_tokens (list): List of word tokens\n",
    "        stop_lst (list): List of stop words to remove\n",
    "    Returns:\n",
    "        list: List of tokens with stop words removed\n",
    "    \"\"\"\n",
    "    stop_lst = {stp_wrd.strip() for stp_wrd in stop_lst}  # Convert to set for faster lookups\n",
    "    return [token.strip() for token in all_tokens if token.strip() not in stop_lst]\n",
    "\n",
    "# Remove stop words with progress bar\n",
    "print(\"Removing stop words...\")\n",
    "darija_dataset_df['tweet_token'] = [remove_stop(tokens, darija_stop_words) for tokens in tqdm(darija_dataset_df['tweet_token'])]\n",
    "\n",
    "# Show example of cleaned tokens\n",
    "print(\"\\nExample of tweets after stop word removal:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c82b2",
   "metadata": {
    "id": "ImfH7i4xZWjS"
   },
   "source": [
    "# Part 2: Analysis (30 minutes)\n",
    "\n",
    "Now that we've cleaned our data, let's analyze it to find interesting patterns! We'll:\n",
    "1. Count word frequencies\n",
    "2. Create visualizations\n",
    "3. Generate a word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc90ac4",
   "metadata": {},
   "source": [
    "## 1. Word Frequency Analysis\n",
    "Let's count how many times each word appears in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603be0a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Combine all tokens into a single list\n",
    "print(\"Counting word frequencies...\")\n",
    "all_tokens = [token for tokens in darija_dataset_df['tweet_token'] for token in tokens]\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# Show the most common words\n",
    "print(\"\\nTop 10 most common words:\")\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"{word}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c3a9ee",
   "metadata": {},
   "source": [
    "## 2. Visualizing Word Frequencies\n",
    "Let's create a bar plot to visualize the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfca8c4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_top_words(word_counts, n=20, title=\"Most Common Words\"):\n",
    "    \"\"\"\n",
    "    Create a bar plot of the most common words.\n",
    "    Args:\n",
    "        word_counts (Counter): Word frequency counter\n",
    "        n (int): Number of top words to show\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    # Get top n words\n",
    "    top_words = word_counts.most_common(n)\n",
    "    words, counts = zip(*top_words)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Reshape Arabic text for proper display\n",
    "    reshaped_words = [arabic_reshaper.reshape(word) for word in words]\n",
    "    bidi_words = [get_display(word) for word in reshaped_words]\n",
    "    \n",
    "    # Create bar plot\n",
    "    sns.barplot(x=list(counts), y=bidi_words, palette=\"viridis\")\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16, pad=20)\n",
    "    plt.xlabel(\"Frequency\", fontsize=12)\n",
    "    plt.ylabel(\"Words\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(count, i, str(count), va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the visualization\n",
    "print(\"Creating word frequency visualization...\")\n",
    "plot_top_words(word_counts, n=20, title=\"20 Most Common Words in Darija Tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b1596",
   "metadata": {},
   "source": [
    "## 3. Word Cloud\n",
    "A word cloud is a visual representation of text data where the size of each word indicates its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ccf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_cloud(word_counts, title=\"Word Cloud of Darija Tweets\"):\n",
    "    \"\"\"\n",
    "    Create a word cloud from word frequencies.\n",
    "    Args:\n",
    "        word_counts (Counter): Word frequency counter\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='/content/NotoNaskhArabic-VariableFont_wght.ttf',\n",
    "        background_color='white',\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        max_words=100,\n",
    "        colormap='viridis'\n",
    "    ).generate_from_frequencies(word_counts)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate word cloud\n",
    "print(\"Generating word cloud...\")\n",
    "create_word_cloud(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446d8ab",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "You've successfully:\n",
    "1. Cleaned a Darija tweet dataset\n",
    "2. Removed unnecessary elements (emojis, usernames, etc.)\n",
    "3. Analyzed word frequencies\n",
    "4. Created visualizations\n",
    "\n",
    "## üìù Next Steps\n",
    "- Try analyzing different subsets of the data\n",
    "- Experiment with different visualization styles\n",
    "- Explore sentiment analysis of the tweets\n",
    "- Try creating bigrams or trigrams to find common phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a800c1",
   "metadata": {},
   "source": [
    "## üí° Challenge\n",
    "Can you modify the code to:\n",
    "1. Find the most common bigrams (pairs of words)?\n",
    "2. Create a visualization of word frequencies by sentiment?\n",
    "3. Compare word frequencies between different time periods?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
