{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f16ef4",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/1337-Artificial-Intelligence/hackai-2025/blob/main/new_notebooks/data_cleanup_tweet_dataset_darija.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa55d67",
   "metadata": {},
   "source": [
    "# Data Cleanup - Tweet Dataset (Darija)\n",
    "\n",
    "In this notebook, you'll learn how to clean and analyze text data from social media. We'll work with a dataset of Moroccan Darija tweets and learn essential text preprocessing techniques that are commonly used in Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220f963",
   "metadata": {},
   "source": [
    "## What you'll learn:\n",
    "- How to load and explore text data\n",
    "- Basic text cleaning techniques\n",
    "- Tokenization (splitting text into words)\n",
    "- Removing stop words (common words that don't add much meaning)\n",
    "- Basic text analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5c9d2",
   "metadata": {},
   "source": [
    "## Why is this important?\n",
    "Before we can use text data for AI tasks like sentiment analysis or text generation, we need to clean and prepare it. This is called \"text preprocessing\" and it's a crucial first step in any NLP project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441898e4",
   "metadata": {},
   "source": [
    "## Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d0b64",
   "metadata": {},
   "source": [
    "### 1. Install and import necessary packages\n",
    "First, we need to install and import the tools we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install datasets pandas regex nltk matplotlib seaborn wordcloud arabic-reshaper python-bidi\n",
    "\n",
    "# Import the packages\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set pandas to show full text\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa4ce5",
   "metadata": {},
   "source": [
    "### 2. Load the Dataset\n",
    "We'll use a dataset of Moroccan Darija tweets from Hugging Face. This dataset contains tweets with their sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d9656",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "darija_dataset = load_dataset(\"shmuhammad/AfriSenti-twitter-sentiment\", \"arq\")\n",
    "\n",
    "# Convert to pandas DataFrame for easier handling\n",
    "darija_dataset_df = pd.DataFrame(darija_dataset['train'])\n",
    "\n",
    "# Let's look at the first few tweets\n",
    "print(\"First 5 tweets in the dataset:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ade983",
   "metadata": {},
   "source": [
    "### 3. Text Cleaning\n",
    "Social media text often contains emojis, usernames, and other elements we want to remove. Let's clean our tweets step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1017b6",
   "metadata": {},
   "source": [
    "#### 3.1 Remove Emojis\n",
    "Emojis are fun but they can make text analysis more complicated. Let's remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544aa9ce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_emojis(tweet):\n",
    "    emoj = re.compile(\"[\"\n",
    "       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "       u\"\\U0001F923\"\n",
    "       u\"\\U0001F97A\"\n",
    "       u\"\\U0001F914\"\"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', tweet)\n",
    "\n",
    "# Apply the function to our tweets\n",
    "darija_dataset_df['tweet'] = darija_dataset_df['tweet'].apply(remove_emojis)\n",
    "print(\"Tweets after removing emojis:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0b60a",
   "metadata": {},
   "source": [
    "#### 3.2 Remove Usernames\n",
    "Twitter usernames start with @. Let's remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec6e88",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_user(tweet):\n",
    "    user_re = \"@[A-Za-z0-9]+\"\n",
    "    return re.sub(user_re, ' ', tweet)\n",
    "\n",
    "# Apply the function\n",
    "darija_dataset_df['tweet'] = darija_dataset_df['tweet'].apply(remove_user)\n",
    "print(\"Tweets after removing usernames:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d9266",
   "metadata": {},
   "source": [
    "#### 3.3 Remove Latin Letters\n",
    "Since we're working with Darija, let's remove Latin letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6376b39a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_latin(tweet):\n",
    "    latin_re = \"[A-Za-z]+\"\n",
    "    return re.sub(latin_re, ' ', tweet)\n",
    "\n",
    "# Apply the function\n",
    "darija_dataset_df['tweet'] = darija_dataset_df['tweet'].apply(remove_latin)\n",
    "print(\"Tweets after removing Latin letters:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0294de8",
   "metadata": {},
   "source": [
    "#### 3.4 Remove Punctuation\n",
    "Punctuation marks can be distracting for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca8ab5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(tweet):\n",
    "    punct_re = \"[^\\w\\s]+\"\n",
    "    return re.sub(punct_re, ' ', tweet)\n",
    "\n",
    "# Apply the function\n",
    "darija_dataset_df['tweet'] = darija_dataset_df['tweet'].apply(remove_punctuation)\n",
    "print(\"Tweets after removing punctuation:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd5ba7",
   "metadata": {},
   "source": [
    "### 4. Tokenization\n",
    "Tokenization means splitting text into individual words (tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    return word_tokenize(tweet)\n",
    "\n",
    "# Apply tokenization\n",
    "darija_dataset_df[\"tweet_token\"] = darija_dataset_df['tweet'].apply(tokenize)\n",
    "print(\"Tweets after tokenization:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f9d14",
   "metadata": {},
   "source": [
    "### 5. Remove Stop Words\n",
    "Stop words are common words that don't add much meaning (like \"the\", \"and\", etc.). Let's remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1df6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add darija_stop_words.csv file\n",
    "# For now, we'll use a small example list\n",
    "darija_stop_words = [\"و\", \"في\", \"من\", \"على\", \"إلى\", \"عن\", \"مع\", \"هذا\", \"هذه\", \"هؤلاء\"]\n",
    "\n",
    "def remove_stop(all_tokens, stop_lst):\n",
    "    stop_lst = {stp_wrd.strip() for stp_wrd in stop_lst}\n",
    "    return [token.strip() for token in all_tokens if token.strip() not in stop_lst]\n",
    "\n",
    "# Apply stop word removal\n",
    "darija_dataset_df['tweet_token'] = darija_dataset_df['tweet_token'].apply(remove_stop, args=(darija_stop_words,))\n",
    "print(\"Tweets after removing stop words:\")\n",
    "darija_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd58e2a",
   "metadata": {},
   "source": [
    "### 6. Text Analysis\n",
    "Now that our data is clean, let's analyze it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e77cd6",
   "metadata": {},
   "source": [
    "#### 6.1 Count Word Frequencies\n",
    "Let's see which words appear most often:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3af4c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get all tokens\n",
    "all_tokens = [token for list_token in darija_dataset_df['tweet_token'] for token in list_token]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# Show top 20 most common words\n",
    "print(\"Top 20 most common words:\")\n",
    "for word, count in word_counts.most_common(20):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0200b",
   "metadata": {},
   "source": [
    "#### 6.2 Visualize Word Frequencies\n",
    "Let's create a bar plot of the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(word_counts, n=20):\n",
    "    top_n = word_counts.most_common(n)\n",
    "    words, counts = zip(*top_n)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    reshaped_words = [arabic_reshaper.reshape(word) for word in words]\n",
    "    bidi_words = [get_display(word) for word in reshaped_words]\n",
    "    \n",
    "    sns.barplot(x=list(counts), y=bidi_words, palette=\"viridis\")\n",
    "    plt.title(f\"Top {n} Most Common Words\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Word\")\n",
    "    plt.show()\n",
    "\n",
    "# Create the plot\n",
    "plot_top_words(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc5238",
   "metadata": {},
   "source": [
    "#### 6.3 Create a Word Cloud\n",
    "A word cloud is a visual representation of text data where the size of each word indicates its frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add NotoNaskhArabic font file\n",
    "# For now, we'll use a default font\n",
    "cloud = WordCloud(background_color=\"white\").generate_from_frequencies(word_counts)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc370dc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, you learned:\n",
    "1. How to load and explore text data\n",
    "2. Basic text cleaning techniques (removing emojis, usernames, etc.)\n",
    "3. How to tokenize text\n",
    "4. How to remove stop words\n",
    "5. Basic text analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96549a26",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Try different cleaning techniques\n",
    "- Experiment with different visualizations\n",
    "- Use the cleaned data for sentiment analysis or other NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704d1ed",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [Hugging Face Datasets](https://huggingface.co/datasets)\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/)\n",
    "- [Matplotlib Documentation](https://matplotlib.org/)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
